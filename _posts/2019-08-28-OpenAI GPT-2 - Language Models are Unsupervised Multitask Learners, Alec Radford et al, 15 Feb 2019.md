---
layout: post
title: OpenAI GPT-2 - Language Models are Unsupervised Multitask Learners
author: YouWon
categories: [NLP(Natural Language Processing) / RNNs]
tags: [Paper_Review, NLP]
---

---

이 글에서는 2019년 2월 *Alec Radford* 등이 발표한 OpenAI GPT-2: Language Models are Unsupervised Multitask Learners를 살펴보도록 한다.

코드와 논문은 [여기](https://openai.com/blog/better-language-models/)에서 볼 수 있지만, 전체버전은 성능이 너무 강력하다는 이유로 공개되지 않았다.

중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.

---

# OpenAI GPT-2 - Language Models are Unsupervised Multitask Learners

논문 링크: **[OpenAI GPT-2 - Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)**

홈페이지: **[OpenAI](https://openai.com/blog/better-language-models/)**

Tensorflow code: **[Official Code](https://github.com/openai/gpt-2)**

## 초록(Abstract)

자연어이해는 

---

## 1. 서론(Introduction)

원본 

---

## 2. 접근(Approach)

### 2.1. Training Dataset

### 2.2. Input Representation

### 2.3. Model


---

## 3. 실험(Experiments)



### 3.1. Language Modeling



### 3.2. Children's Boot Test



### 3.3. LAMBADA



### 3.4. Winograd Schema Challenge



### 3.5. Reading Comprehension



### 3.6. Summarization



### 3.7. Translation



### 3.8. Question Answering


---

## 4. 일반화 vs 암기(Generalization vs Memorization)


---

## 5. 관련 연구(Related Work)



---

## 6. 토의(Discussion)



---

## 7. 결론(Conclusion)




**Acknowledgements**

---

## Refenrences

논문 참조. 많은 레퍼런스가 있다.


---

## 8. Appendix A: Samples

### 8.1. Model capacity


### 8.2. Text Memorization


### 8.3. Diversity


### 8.4. Robustness


---