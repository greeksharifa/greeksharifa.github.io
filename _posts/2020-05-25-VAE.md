---
layout: post
title: Variational AutoEncoder 설명
author: Youyoung
categories: Generative Model
tags: [Machine Learning, Paper_Review]
---

본 글의 주제는  2014년에 발표된 생성 모델인 Variational AutoEncoder에 대해 설명하고 이를 코드로 구현하는 내용을 담고 있다.  

## 1. Auto-Encoding Variational Bayes 논문 리뷰  
### 1.1. Introduction  
연속형 잠재 변수와 파라미터가 다루기 힘든 사후 분포를 갖는 방향성 확률 모델에 대해 효율적인 근사 추론 및 학습을 수행할 수 있는 방법이 없을까? **Variational Bayesian** 접근법은 다루기 힘든 사후 분포에 대한 근사의 최적화를 내포한다.  

불행히도, 일반적인 평균 필드(mean-field) 접근법은 근사적 사후 분포에 대해 기댓값의 분석적 해결법을 요구하는데 이는 보통 굉장히 다루기 어려운 방법이다. 본 논문은 Variational Lower Bound의 **Reparameterization**이 Lower Bound의 미분 가능한 불편향 estimator를 만드는 방법에 대해 보여줄 것이다. 이 **Stochastic Gradient Variational Bayes: SGVB estimator**는 연속형 잠재변수나 파라미터를 갖고 있는 대부분의 모델에 대해 효율적인 근사 사후 추론을 가능하게 하며, 표준 Stochastic Gradient Ascent 스킬을 사용하여 최적화하기에 굉장히 편리하다.  

IID 데이터셋이고, 데이터포인트 별로 연속형 잠재변수를 갖고 있는 경우에 대해 본 논문은 `Auto-Encoding VB` 알고리즘을 제안한다. 이 알고리즘에서는 **Simple Ancestral Sampling**을 이용하여 근사 사후 추론을 하는 인식 모델을 최적화하기 위해 SGVB estimator를 사용하여 추론과 학습을 효율적으로 해낸다. 이 과정은 MCMC와 같이 데이터포인트 별로 반복적인 추론을 행하여 많은 연산량을 요구하지 않는 장점을 가진다.  

학습된 근사 사후 추론 모델은 recognition, denoising, representation, visualization의 목적으로 활용될 수 있다. 본 알고리즘이 인식(recognition) 모델에 사용될 때, 이를 `Variational Auto-Encoder`라고 부를 것이다.  

### 1.2. Method  
본 섹션에서는 연속형 잠재 변수를 내포하는 다양한 방향성 그래픽 모델에서 Stochastic 목적 함수인 **Lower Bound Estimator**를 끌어내는 과정을 설명할 것이다. 데이터포인트 별 잠재변수는 iid한 상황이라는 가정 하에 본 논문에서는 파라미터에 대해 Maximul Likelihood와 Maximum Posteriori 추론을 수행하고 잠재변수에 대해 **Variational Inference**를 수행할 것이다. 이러한 방법은 온라인 러닝에도 사용될 수 있지만 본 논문에서는 간단히 하기 위해 고정된 데이터셋을 사용할 것이다.  

#### 1.2.1. Problem Scenario  
N개의 Sample을 가진 $X$라는 데이터가 있다고 해보자. 본 논문은 이 데이터가 관측되지 않은 연속형 확률 변수 $z$를 내포하는 어떤 Random Process에 의해 형성되었다고 가정한다.  

이 과정은 2가지 단계로 구성된다.  
1) $z^{i}$라는 값은 어떤 사전 분포 $p_{\theta ^*}(z)$에서 발생한다.  
2) $x^{i}$라는 값은 어떤 조건부 분포 $p_{\theta ^*}(x|z)$에서 발생한다.  

(여기서 $z$는 원인, $x$는 결과라고 보면 이해가 쉬울 것이다.)  

우리는 사전확률 $p_{\theta *}(z)$와 Likelihood $p_{\theta ^*}(x|z)$가 $p_{\theta}(z)$, $p_{\theta}(x|z)$의 parametric families of distributions에서 왔다고 가정하고, 이들의 확률밀도함수는 거의 모든 $\theta, z$에 대해 미분가능하다고 전제한다.  

불행히도, 이러한 과정의 많은 부분은 우리가 직접 확인하기 어렵다. True 파라미터인 $\theta ^*$와 잠재 변수의 값 $z^{i}$은 우리에게 알려져 있지 않다.  

본 논문은 주변 확률이나 사후 확률에 대한 단순화를 위한 일반적인 가정을 취하지 않고 분포가 다루기 힘들고 큰 데이터셋을 마주하였을 경우를 위한 효율적인 알고리즘에 대해 이야기하고자 한다.  

**1) Intractability**(다루기 힘듦)  
(1) marginal likelihood $p_{\theta}(x)$의 적분인 $\int p_{\theta}(x) p_{\theta}(x|z) dz $가 다루기 힘든 경우  

(2) true posterior density $p_{\theta}(z|x) = p_{\theta}(x|z)p_{\theta}(z)/p_{\theta}(x)$가 다루기 힘들어 EM 알고리즘이 사용될 수 없는 경우  

(3) 어떠한 합리적인 평균-필드 VB알고리즘을 위한 적분이 다루기 힘든 경우  

이러한 Intractability는 굉장히 흔하며, 복잡한 우도(likelihood) 함수 $p_{\theta}(x|z)$를 갖는 신경망 네트워크에서 발견할 수 있다.  

**2) A Large Dataset**  
데이터가 너무 크면 배치 최적화는 연산량이 매우 많다. MC-EM과 같은 Sampling Based Solution은 데이터 포인트별로 Sampling Loop를 돌기 때문에 너무 느리다.  

위 시나리오에서 설명한 문제들에 대해 본 논문은 아래와 같은 해결책을 제시한다.  

1) 파라미터 $\theta$에 대한 효율적인 근사 ML/MAP estimation. 이 파라미터들은 숨겨진 랜덤 과정을 흉내내고 실제 데이터를 닮은 인공적인 데이터를 생성할 수 있게 해준다.  
2) 파라미터 $\theta$의 선택에 따라 관측값 $x$이 주어졌을 때 잠재 변수 $z$에 대한 효율적인 근사 사후 추론  
3) 변수 $x$에 대해 효율적인 근사 주변 추론. 이는 $x$에 대한 prior이 필요한 모든 추론 task를 수행할 수 있게 해준다.  

위 문제를 해결하기 위해 인식 모델 $q_{\phi}(z|x)$이 필요하다. 이 모델은 다루기 힘든 True Posterior $p_{\theta}(z|x)$의 근사 버전이라고 할 수 있다. 본 논문에서는 인식 모델 파라미터인 $\phi$와 생성 모델 파라미터인 $\theta$를 동시에 학습하는 방법에 대해 이야기할 것이다.  

코딩 이론의 관점에서 보면, 관측되지 않은 변수 $z$는 잠재 표현 또는 *code*라고 해석될 수 있다. 본 논문에서는 따라서 인식 모델 $q_{\phi}(z|x)$를 **encoder**라고 부를 것인데, 왜냐하면 데이터 포인트 $x$가 주어졌을 때 이 **encoder**가 데이터 포인트 $x$가 발생할 수 code $z$의 가능한 값에 대한 분포를 생산하기 때문이다. 비슷한 맥락에서 우리는 $q_{\theta}(x|z)$를 **확률적 decoder**라고 명명할 것인데, 왜냐하면 code $z$가 주어졌을 때 이 **decoder**가 상응하는 가능한 $x$의 값에 대해 분포를 생산하기 때문이다.  

#### 1.2.2. The Variational Bound  
주변 우도(Marginal Likelihood)는 각 데이터포인트의 주변 우도의 합으로 구성된다. 이를 식으로 표현하면 아래와 같다.  

$$ log p_{\theta}(x^{(1)}, ..., x^{(N)}) = \sum_{i=1}^N log p_{\theta} (x^{(i)}) $$  

그런데 데이터 포인트 하나에 대한 주변 우도는, 아래와 같이 재표현이 가능하다.  

$$ og p_{\theta} (x^{(i)}) = D_{KL} (q_{\phi}(z|x^{(i)}) || p_{\theta} (z|x^{(i)})) + L(\theta, \phi; x^{(i)}) $$  

우변의 첫 번째 항은 true posterior의 근사 **KL Divergence**이다. 이 KL Divergence는 음수가 아니기 때문에, 두 번째 항인 $L(\theta, \phi; x^{(i)})$는 i번째 데이터 포인트의 주변 우도의 **Varitaional Lower Bound**라고 한다.  

부등식으로 나타내면 아래와 같다.  

$$ log p_{\theta} (x^{(i)}) \geq L(\theta, \phi; x^{(i)}) = E_{q_{\phi} (z|x)} [-logq_{\phi}(z|x) + log p_{\theta}(x, z)] $$  

이 식은 또 아래와 같이 표현할 수 있다.  

<center><img src="/public/img/Machine_Learning/2020-05-25-VAE/01.JPG" width="60%"></center>  

우리는 Lower Bound $L(\theta, \phi; x^{(i)})$를 Variational 파라미터와 생성 파라미터인 $\phi, \theta$에 대하여 미분하고 최적화하고 싶은데, 이 Lower Bound의 $\phi$에 대한 Gradient는 다소 복잡하다.  

이러한 문제에 대한 일반적인 Monte Carlo Gradient Estimator는 아래와 같다.  

<center><img src="/public/img/Machine_Learning/2020-05-25-VAE/02.JPG" width="60%"></center>  

여기서 $z^{(l)}$ ~ $q_{\phi} (z|x^{(i)})$이다.  

이 Gradient Estimator는 굉장히 큰 분산을 갖고 있어서 우리의 목적에 적합하지 않다.  

#### 1.2.3. The SGVB estimator and AEVB algorithm  





---
## 2. 이론에 대한 보충 설명  
### 2.1. 용어 정리  
**1) Variational Inference**  
$q(x)$라는 쉬운 분포를 통해 target 분포 $p(x)$를 근사 추론하는 방법론이다.  

$$ q^* = argmin_{q \in Q} KL(q||p) $$  

**2) KL Divergence**  


**3) s**  



### 2.2.  


---
## Reference  
1) https://ratsgo.github.io/generative%20model/2018/01/27/VAE/  
2) https://www.youtube.com/watch?v=SAfJz_uzaa8  
3) https://taeu.github.io/paper/deeplearning-paper-vae/
4) 
