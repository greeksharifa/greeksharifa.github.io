---
layout: post
title: AGQA 2.0 - An Updated Benchmark for Compositional Spatio-Temporal Reasoning 설명
author: Youyoung
categories: [Paper_Review]
tags: [Machine_Learning, Paper_Review]
---

이번 글에서는 `AGQA 2.0: An Updated Benchmark for Compositional Spatio-Temporal Reasoning` 논문을 정리한다.

- 2022년 4월(Arxiv)
- Madeleine Grunde-McLaughlin, Ranjay Krishna, Maneesh Agrawala
- [홈페이지](https://cs.stanford.edu/people/ranjaykrishna/agqa/)
- [논문 링크](https://arxiv.org/abs/2204.06105)  
- [Github](https://github.com/madeleinegrunde/AGQA_baselines_code)  

---

## Abstract

- [AGQA 1.0](https://greeksharifa.github.io/paper_review/2023/03/03/AGQA1.0/)에서는 언어적 편향의 영향을 줄이기 위해 균형 잡힌 답변 분포로 훈련/테스트 분할을 제공하였다.
- 그러나 여전히 편향성(bias)이 일부 남아 있다. 
- 2.0 benchmark에서는 몇 가지 개선 사항, 특히 더 엄격한 balancing이 적용되었다.
- 그리고 모든 실험에 대해 업데이트된 benchmark 결과를 report한다.

---

## 1. Action Genome Question Answering

AGQA는 bias를 많이 없앤 좋은 VQA benchmark이지만 부족한 점이 여전히 있었다. 그래서 본 논문에서 2.0 버전으로 업그레이드한 버전을 소개하고자 한다. 언어적 편향성(language bias)을 더욱 제거할 수 있는 더 강력한 balancing 작업을 진행하여 총 96.85M개의 QA 쌍에서 2.27M쌍의 균형잡힌 데이터셋을 만들었다.

Section 2에서는 1.0과 2.0의 차이를, Section 3에서는 기존 모델들의 달라진 성능 체크를 report한다. balanced 버전에서는 어떤 모델도 이지선다형 질문에 51%의 정확도를 넘지 못했다.

---

## 2. Updates to AGQA

2.0에서는 특히 balancing 과정에 신경을 많이 썼다. 이전의 balancing 작업 역시 bias를 많이 없애기는 하였으나 여전히 남아 있는 bias를 모델들이 활용하였다. 

- balancing algorithm은 (다지선다형에서) 각 정답의 비율이 동일하도록 조정한다. 즉 특정 종류의 질문의 대답이 yes/no라면 yes가 50%, no도 50%의 비율로 나타나도록 한다.
- Temporal localization phrases를 다루는 category definition도 재정의했다. 
- 결과적으로 HCRN의 경우 yes/no 질문에서 1.0에서는 72.12%의 정답률을 보였지만 bias를 제거한 2.0에서는 50.10%밖에 달성하지 못했다.

이외에도 작은 upgrade들이 있었다:

1. relationship type을 명시하지 않고 object의 존재 여부를 물을 때 `contacting`과 `touching` 용어를 `interacts with`로 바꾸었다. 이는 actor가 object를 물리적으로 "touch"하지 않아도 답변이 yes가 될 수 있음을 의미한다. 또한 `doorway` 상호작용은 불명확하므로 제거하였다.
2. 종종 모호할 수 있는 간접적인 표현을 제거하였다. `the thing they were doing to <object>`와 같은 간접 관계 표현은 동치인 직접 관계 표현으로 바꾸기 애매하다. 또한 `the object they held before running`와 같이 temporal localization phrases와 관련한 object에 대한 간접 표현도 제거하였다.
3. temporal localization phrases를 쓰려면 적어도 한 개의 annotated frame이 영상 안에 존재해야 하는 조건을 추가하였다. `What did they do before running?`와 같은 질문을 생성하려면 `running` 정보가 있는 frame이 적어도 하나는 존재해야 한다. (역자: 당연한 거긴 하지만)
4. 간접표현에서 `backward` 용어가 잘못 사용된 것을 `forward`로 수정하였다.
5. GT answer를 일반적인 단어인 `action`으로 대체함으로써 `What action were they doing the longest?`와 같이 최상급 표현으로 된 action에 대한 program을 개선하였다. 
    - 가장 긴 action이 `sitting`인 영상 하나를 생각하자. 
    - `What was the person doing for the longest amount of time?` 프로그램은 
    - `Superlative(max, Filter(video, [actions]), Subtract(Query(end, sitting), Query(start, sitting)))`에서 
    - `Superlative(max, Filter(video, [actions]), Subtract(Query(end, action), Query(start, action)))`으로 바뀌었다.
6. 질문의 추론 단계를 최대 8단계로 제한하였다. 
7. `relTime` 및 `objTime` 유형의 문제는 sequencing 추론 유형으로 labeling했다.
8. 모든 답변은 소문자로 표시한다.
9. `holding a blanket`과 같은 action은 relationship(`holding`)과 object(`blanket`)으로 나눌 수 있다. 하지만 action은 여러 frame과 연관된 반면 object는 하나의 frame과만 연관된다. 그래서 action이 겹치는 경우가 있다면 동일한 질문에 여러 대답이 나올 수 있다. 이를 막기 위해 frame 기반 object-relationship 표현에 우선순위를 두었다. 그러면서 `actExists` 템플릿은 제거되었다.
10. 최상급 표현의 질문에서 답변은 처음 또는 마지막의 것으로 제한된다. 만약 어떤 사람이 들었던 물건들의 순서가 접시, 담요, 의자였다면 중간에 위치한 담요는 답변이 될 수 없다.

결과적으로 96.85M개의 질문-답변 쌍에서 bias를 제거하였더니 2.27M 쌍이 남았다.

<center><img src="/public/img/2023-03-06-AGQA2.0/fig01.png" width="100%"></center>

---

## 3. Results



### 3.1. Performance across question types



### 3.2. Performance without visual data



### 3.3. Model performance by binary and open answer questions



### 3.4. Generalization to novel compositions



### 3.5. Generalization to more compositional steps



### 3.6. Generalization to indirect references



### 3.7. Performance by question complexity



---

## 4. Discussion

