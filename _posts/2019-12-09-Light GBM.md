---
layout: post
title: Light GBM 설명 및 사용법
author: Youyoung
categories: Machine_Learning
tags: [Machine_Learning]
---

## 1. Light GBM: A Highly Efficient Gradient Boosting Decision Tree 논문 리뷰  
### 1.1. Background and Introduction  
다중 분류, 클릭 예측, 순위 학습 등에 주로 사용되는 **Gradient Boosting Decision Tree (GBDT)**는 굉장히 유용한 머신러닝 알고리즘이며, XGBoost나 pGBRT 등 효율적인 기법의 설계를 가능하게 하였다. 이러한 구현은 많은 엔지니어링 최적화를 이룩하였지만 고차원이고 큰 데이터 셋에서는 만족스러운 결과를 내지 못하는 경우도 있었다. 왜냐하면 모든 가능한 분할점에 대해 정보 획득을 평가하기 위해 데이터 개체 전부를 스캔해야 했기 때문이다. 이는 당연하게도, 굉장히 시간 소모적이다.  
  
본 논문은 이 문제를 해결하기 위해 2가지 최신 기술을 도입하였다.  
첫 번째는 **GOSS: Gradient-based One-Side Sampling**이며, 기울기가 큰 데이터 개체가 정보 획득에 있어 더욱 큰 역할을 한다는 아이디어에 입각해 만들어진 테크닉이다. 큰 기울기를 갖는 개체들은 유지되며, 작은 기울기를 갖는 데이터 개체들은 일정 확률에 의해 랜덤하게 제거된다.  
  
두 번째는 **EFB: Exclusive Feature Bundling**으로, 변수 개수를 줄이기 위해 상호배타적인 변수들을 묶는 기법이다. 원핫 인코딩된 변수와 같이 희소한(Sparse) 변수 공간에서는 많은 변수들이 상호 배타적인 경우가 많다. (0이 굉장히 많기 때문에) 본 테크닉은, 최적 묶음 문제를 그래프 색칠 문제로 치환하고 일정 근사 비율을 갖는 Greedy 알고리즘으로 이 문제를 해결한다.  

### 1.2. Preliminaries  
GBDT는 Decision Tree의 앙상블 모델이다. 각각의 반복에서 GBDT는 음의 기울기(잔차 오차)를 적합함으로써 Decision Tree를 학습시킨다. 이 학습 과정에서 가장 시간이 많이 소모되는 과정이 바로 최적의 분할점들을 찾는 것인데, 이를 위한 대표적인 방법에는 **Pre-sorted(사전 정렬) 알고리즘**과 **Histogram-based 알고리즘**이 있다.  
  
**Pre-sorted 알고리즘**의 경우 사전 정렬한 변수 값에 대해 가능한 모든 분할점을 나열함으로써 간단하게 최적의 분할점을 찾을 수 있지만, 효율적이지 못하다는 단점이 있다. **Histogram-based 알고리즘**은 연속적인 변수 값을 이산적인 구간(bin)으로 나누고, 이 구간을 사용하여 학습과정 속에서 피쳐 히스토그램을 구성한다.  
  
학습 데이터의 양을 줄이기 위해 가장 쉽게 생각할 수 있는 방법은 Down Sampling이 될 것이다. 이는 만약 데이터 개체의 중요도(Weight)가 설정한 임계값을 넘지 못할 경우 데이터 개체들이 필터링되는 과정을 말한다. SGB의 경우 약한 학습기를 학습시킬 때 무작위 부분집합을 사용하지만, SGB를 제외한 Down Sampling 방식은 AdaBoost에 기반하였기 때문에 바로 GBDT에 적용시킬 수 없다. 왜냐하면 AdaBoost와 달리 GBDT에는 데이터 개체에 기본 가중치가 존재하지 않기 대문이다.  
  
비슷한 방식으로 피쳐 수를 줄이기 위해서는, 약한(Weak) 피쳐를 필터링하는 것이 자연스러울 것이다. 그러나 이러한 접근법은 변수들 사이에 중대한 중복요소가 있을 것이라는 가정에 의존하는데, 실제로는 이 가정이 옳지 않을 수도 있다.  
  
실제 상황에서 사용되는 대용량 데이터셋은 많은 경우에 희소한(Sparse) 데이터셋일 확률이 높다. Pre-sorted 알고리즘에 기반한 GBDT의 경우 0값을 무시함으로써 학습 비용을 절감할 수 있지만, Histogram-based 알고리즘에 기반한 GBDT에는 효율적인 희소값 최적화 방법이 없다. 그 이유는 Histogram-based 알고리즘은 피쳐 값이 0이든 1이든, 각 데이터 개체마다 피쳐 구간(Bin) 값을 추출해야하기 때문이다. 따라서 Histogram-based 알고리즘에 기반한 GBDT가 희소 변수를 효과적으로 활용할 방안이 요구된다. 이를 해결하기 위한 방법이 바로 앞서 소개한 **GOSS**와 **EFB**인 것이다.  
  
### 1.3. GOSS: Gradient-based One-Sided Sampling  
AdaBoost에서 Sample Weight는 데이터 개체의 중요도를 알려주는 역할을 수행하였다. GBDT에서는 기울기(Gradient)가 이 역할을 수행한다. 각 데이터 개체의 기울기가 작으면 훈련 오차가 작다는 것을 의미하므로, 이는 학습이 잘 되었다는 뜻이다. 이후 이 데이터를 그냥 제거한다면 데이터의 분포가 변화할 것이므로, 다른 접근법(GOSS)이 필요하다.  
  
GOSS의 아이디어는 직관적이다. 큰 Gradient(훈련이 잘 안된)를 갖는 데이터 개체들은 모두 남겨두고, 작은 Gradient를 갖는 데이터 개체들에서는 무작위 샘플링을 진행하는 것이다. 이를 좀 더 상세히 설명하자면 아래와 같다.  

1) 데이터 개체들의 Gradient의 절대값에 따라 데이터 개체들을 정렬함  
2) 상위 a*100% 개의 개체를 추출함  
3) 나머지 개체들 집합에서 b*100% 개의 개체를 무작위로 추출함  
4) 정보 획득을 계산할 때, 위의 2-3 과정을 통해 추출된 Sampled Data를 상수( $ \frac{1-a} {b} $ )를 이용하여 증폭시킴  
  
<center><img src="/public/img/Machine_Learning/2019-12-09-Light GBM/01.JPG" width="90%"></center>  

위 그림에 대하여 추가적으로 부연설명을 하면,  
**topN, randN**은 2, 3 과정에서 뽑는 개수를 의미하며,  
**topSet, randSet** 은 2, 3 과정에서 뽑힌 데이터 개체 집합을 의미한다.  
**w[randSet] x= fact**은 증폭 벡터를 구성하는 과정으로, 증폭 벡터는 randSet에 해당하는 원소는 fact 값을 가지고, 나머지 원소는 1의 값을 가지는 벡터이다.  

마지막으로 **L: Weak Learner**에 저장된 정보는, 훈련데이터, Loss, 증폭된 w 벡터로 정리할 수 있겠다.  

### 1.4. EFB: Exclusive Feature Bundling  



---
## 2. Light GBM 모듈 설명  


> **sns.despine(offset=None, trim=False, top=True, right=True, left=False, bottom=False)**  
> :: Plot의 위, 오른쪽 축을 제거함  
> - *offset* = [integer or dict], 축과 실제 그래프가 얼마나 떨어져 있을지 설정함  

<center><img src="/public/img/Machine_Learning/2019-12-05-Seaborn Module/01.png" width="50%"></center>  

|메서드|기능|종류|
|:--------:|:--------:|:--------:|




---
## Reference
> [LightGBM 공식 문서](https://lightgbm.readthedocs.io/en/latest/index.html)  
> [논문](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)
