---
layout: post
title: DDQN 알고리즘 설명
author: Youyoung
categories: 강화학습
tags: [강화학습, 파이썬]
---

## 1. DDQN 논문 리뷰
Deep Reinforcement Learning with Double Q-learning
[논문 원본 링크](https://arxiv.org/abs/1509.06461)

### 1.1. Abstract  
Q-learning 알고리즘은 특정 조건 하에서 Action Value를 과대평가한다고 알려져있다. 본 논문에서는 먼저 DQN 알고리즘이 일부 게임 상황에서 중대한 과적합문제를 겪고 있다는 것을 밝힐 것이다. 이후 Double Q-learning이 large-scale function approximation에 적용될 수 있다는 것을 보여줄 것이다. 또한 이를 DQN에 적용하면, 과적합 문제를 해결할 수 있을 뿐만 아니라 몇몇 경우에 더 나은 퍼포먼스를 보여준다는 것을 보여줄 것이다.  

강화학습의 중요한 목표는 누적된 Future Reward Signal을 최적화하여 Sequential Decision Problems에 적합한 Policy를 학습하는 것이다. Q-learning은 이 문제에 적합한 알고리즘이지만, 추정된 Action Value 값에 대해 max step을 취함으로써 비정상적으로 높은 Action Value를 학습하여 과적합 문제를 야기한다.  

Overestimation이 Uniform한 분포를 띤다면 큰 문제가 되지 않겠지만, 일반적으로 Uniform하지 않으며 이는 알고리즘의 성능을 저해하는 요인이 된다. 본 논문에서는 **Doulble DQN**이 이 문제를 해결하여 더욱 정확한 추정값을 반환하고 더 나은 성능을 보인다는 것을 증명하고자 한다.  

### 1.2. Background  
Q함수는 state s에서 policy $\pi$에 따른 action a의 True Value로 정의된다.  
  
$$ Q_{\pi}(s, a) = E[R_1 + {\gamma}R_2 + ... | S_0 = s, A_0 = a, {\pi}] $$
  
이 Q함수의 최적값은 아래와 같이 표현된다.  
  
$$ Q^*(s, a) = \max_{\pi} Q_{\pi}(s, a) $$
  
이 최적 Policy는 각 state에서 가장 높은 값을 가지는 Action을 선택하여 derive할 수 있다. 수많은 state와 action 사이의 Q-value를 모두 학습하는 것은 불가능에 가깝기 때문에 우리는 **Parameterized($\theta$) Value Function**을 학습할 것이다. 표준 Q-learning의 업데이트 방식과 Target Y(True Value)는 아래와 같이 정의된다.  

$$ \theta_{t+1} = \theta_t + \alpha(TargetY - Q(S_t, A_t ; \theta_t)) \nabla_{\theta_t} Q(S_t, A_t ; \theta_t) $$

$$ Target Y = Y^Q_t := R_{t+1} + \gamma \max_a Q(S_{t+1}, a; \theta_t) $$

위 업데이트 과정은 Target Value $Y^Q_T$를 향해 현재의 추정값 $Q(S_t, A_t ; \Theta_t)$를 업데이트하게 되는데, 이는 Stocastic Gradient Descent와 닮아 있다.  

#### Deep Q Networks  
본 네트워크는 state space에서 action space로 연결하는 Mapping Function에 해당한다. 타겟 네트워크와 경험 리플레이라는 2가지 방식을 통해 효과적인 학습을 수행한다.  

#### Double Q-learning  
Q-learning과 DQN의 **Max Operator**는 action을 선택하고 평가할 때 동일한 값을 사용한다. 이는 과적합을 유발하게 된다. 이를 해결하기 위해 선택과 과정을 분리할 수 있는데, 이것이 Double Q-learning의 기본적인 아이디어이다.  

초기의 Double Q-learning 알고리즘에서는 2개의 Value Function(Weight Sets: $\theta$ , $\theta$`)은 둘 중 하나만 업데이트하기 위해 각 experience를 랜덤하게 할당하는 방식으로 학습되었다. 각 업데이트에서 한 개의 Weight Set는 Greedy Policy를 결정하기 위해 사용되고 나머지 하나는 그 값을 결정하기 위해 사용되었다. 선택/평가를 분리하여 표현한 Target Y는 아래와 같다.  

$$ Y^Q_t = R_{t+1} + \gamma \max_a Q(S_{t+1}, \argmax_a Q(S_{t+1}, a ; \theta_t); \theta_t) $$
  
Double Q-learning Error는 아래와 같다.  

$$ Y^{DoubleQ}_t = R_{t+1} + \gamma \max_a Q(S_{t+1}, \argmax_a Q(S_{t+1}, a ; \theta_t); \theta_t^`) $$

action을 선택하는 부분(argmax 부분) 에서는 여전히 online weight인 $\theta_t$에 근거한다. 이는 여전히 $\theta_t$에 의해 정의된 현재 값들에 의해 Greedy Policy의 값을 추정한다는 뜻이다.  

그러나 우리는 2번째 weight set인 $\theta_t^`$를 이용하여 Policy를 평가한다. 이 2번째 set에 대해서는 $\theta$와 $\theta`$의 역할을 바꿔가면서 대칭적으로 업데이트가 진행된다.  


### 1.3. Overoptimism due to estimation errors  
Q-learning의 Overestimation은 처음으로 Thrun과 Schwartz에 의해 연구되었는데, 그들은 action value가 [$-\epsilon$, $\epsilon$] 사이의 Uniform Distribution을 갖는 random error를 포함하면, 각 Target은 최대 $ \frac{m-1}{m+1} $ 까지 Overestimate된다고 밝혔다. (m=num of actions) 또한 이들은 이러한 overestimation이 **sub-optimal policy**로 인도할 수 있다고 하였다.  

이후 2010년에 Hasselt가 environment의 noise가 overestimation을 일으킬 수 있다고 하였고, Double Q-learning을 해결책으로 내놓았다.  

본 섹션에서는 우리는 어떠한 종류의 estimation error든(이 error가 environmental noise에서 온 것이든, function approximation에서 온 것이든...) upward bias를 야기할 수 있다는 것을 증명하고자 한다. 이것은 굉장히 중요한 데, 왜냐하면 실제로 이 문제는 어떠한 방식으로든 학습과정에 있어 정확도를 낮출 것이기 때문이다.  

Thrun과 Schwartz는 Overestimation의 Upper/Lower Bound를 구하는 방법을 제시했다. (논문 참조) 본 방법에서 우리는 다른 action에 대한 estimatino error가 독립적이라는 가정을 할 필요는 없다. 이 이론은 value에 대한 추정이 평균적으로 맞다하더라도, 어떤 source로 부터 온 estimation error라도 추정치를 끌어올려 True Optimal Value로 부터 멀어지게 만들 수 있다는 것을 보여준다.  

<center><img src="/public/img/RL/2020-03-15-DDQN/01.JPG" width="100%"></center>  

위 그림을 보면 action의 수가 증가할 수록 Q-learning(빨간색)의 Overestimation은 증가하지만, Double Q-learning(파란색)은 Unbiased함을 알 수 있다.  

이제 Function Approximation으로 돌아와서 각 state 마다 10개의 이산적인 action을 행할 수 있는 연속적인 state space를 생각해보자. 간단히 말해서 이 예시에서 True Optimal action value는 오직 state에만 의존하기 때문에 각 state마다 모든 action은 같은 True Value를 갖게 된다. 

<center><img src="/public/img/RL/2020-03-15-DDQN/02.JPG" width="100%"></center>  

위 그림을 보면, **보라색 그래프**가 위에서 말한 **True Value**를 나타내며 $Q_*(s, a) = sin(s) $(가장 위), $Q_*(s, a) = 2exp(-s^2)$(중간, 밑)과 같이 정의된다.  

**초록색 그래프**는 **State에 대한 함수로서의 single action의 근사값**을 보여준다. 초록색 점으로 된 부분은 추정값이 기반이 되는 sample 값을 의미한다. 추정값은 sample state에서의 true value에 적합한 다항식으로 이루어지는데, 가장 아래 그래프는 9차, 나머지는 6차 방정식으로 구성된다. 각 sample state에서는 추정값이 정확히 True Value와 일치하기 때문에 이러한 sample state에서는 우리는 Ground Truth for action value를 갖고 있다고 판단한다.  

상대적으로 차수가 낮은 위와 중간 그래프를 보면 그래프가 충분히 유연하지 못하여 sampled state에서도 부정확한 것을 알 수 있고, 차수가 높은 가장 아래의 그래프는 sampled state에서는 정확도가 높지만 unsampled state에서는 오히려 부정확한 것을 알 수 있다.  

또한 sampled state들이 본 그래프에서는 더욱 서로 거리를 두고 있는 것을 확인할 수 있는데, 이러한 특성이 더욱 큰 Estimation Error를 발생시키게 되었다. 이렇게 특정 순간에 제한적인 데이터를 보유하게 되는 것은 실제 학습 상황에서 자주 발생하게 된다.  







Example을 살펴보면, Overestimation은 심지어 우리가 특정 state의 true action value에 대한 sample을 갖고 있더라도 발생할 수 있다. 비록 Uniformly Overestimating Value는 Policy의 학습을 방해하지는 않겠지만 실제로 Overesimation Error는 여러 state와 action에 따라 다르다. 



### 1.6. Double DQN  

















---
## Reference  
> 
