---
layout: post
title: YOLO
redirect_from:
  - deep_learning/2018/07/21/YOLO
  - deep_learning/2018/07/21/YOLO/
author: Youyoung
categories: Paper_Review
tags: [Keras, Detection, CNN]

---

## YOLO: You Only Look Once
> 본 포스트는 아래의 논문, 코드와
Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - You Only Look Once: Unified, Real-Time Object Detection (2015)
Joseph Redmon, Ali Farhadi - YOLO9000: Better, Faster, Stronger (2016)
Allan Zelener - YAD2K: Yet Another Darknet 2 Keras
Andrew Ng의 Convolutional Neural Networks 강의의 내용을 토대로 정리한 것임을 밝힌다.

**YOLO** 알고리즘은 Sliding Window를 사용하여 이미지의 픽셀을 Stride 단위로 하나하나  
살펴보는 것이 아니라 이미지를 Grid로 나누어 각각의 Grid Cell에 대해  
Label(그 셀의 정보)를 부여함으로써 한 번에 이미지를 스캔한다.  
이 때문에 YOLO 알고리즘은 빠른 속도라는 강점을 갖고 있다.  

알고리즘에 대해 세세하게 설명하기 전에 코딩을 위한 Setting부터 진행하도록 하겠다.
```python
import os
import numpy as np
import tensorflow as tf
from keras import backend as K
from keras.models import load_model, Model
from yad2k.utils.yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes
from yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body
```

여기서 제일 아래 두 줄에 있는 yad2k 모듈은 터미널에서 바로 다운로드를 받을 수 없다.  
아래 깃헙들을 통해서 다운을 받도록 하자.
[yad2k-첫 번째](https://github.com/allanzelener/YAD2K/blob/master/yad2k/models/keras_yolo.py),
[yad2k-두 번째](https://github.com/tejaslodaya/car-detection-yolo)


첫 번째 깃헙에서는 아래에 보이는 것처럼 font와 yad2k폴더를 저장하면 된다.
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo01.PNG" width="90%"></center>

두 번째 깃헙에서는 아래에 보이는 것처럼 yolo_uilts.py파일만 저장하여  
위 코드에서 알 수 있듯이 yad2k.utils안에 넣어두어 편리하게 사용할 수 있다.
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo02.PNG" width="90%"></center>

하지만 이 사용자 패키지를 그대로 Lib/site-packages에 집어넣는다고 모든 게 해결되지는 않는다.  
폰트 설정 작업을 다시 해주어야 한다.  

첫 번째 깃헙에서 다운로드를 받은 font폴더를 열어보면 아래의 파일을 확인할 수 있다.
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo03.PNG" width="50%"></center>
물론 다른 otf폰트를 사용해서 문제가 없을 것이다만, 나는 이 패키지를 만드신 분의 폰트를 그대로 따랐다.  

여기서 FiraMono-Medium.otf파일을 글꼴 설치해주자. 그러고 나서는 모듈 내 함수에서 2가지만  수정해주면 된다. 모듈 내에 있는 draw_boxes.py와 yolo_utils.py를 열어보자
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo04.PNG" width="80%"></center>
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo05.PNG" width="80%"></center>
모두 draw_boxes란 함수를 포함하고 있는데, 여기서 **font=" "** 부분에 본인이 폰트를 저장한 경로로  
수정을 해주어야만 한다.

*자 그럼 모듈 준비는 끝났다. 본격적으로 YOLO에 대해 탐구해볼까?*

### Bounding Boxes and Endocing Vector
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo06.PNG" width="100%"></center>
YOLO 알고리즘을 사용하기 전에는 본인이 detect하고 싶은 물체를 구분할 수 있는 pre-trained된 CNN기반 모델이 필요하다. 즉, 내가 이미지에서 1개 이상의 차/보행자/오토바이를 구분하고 싶다면,  
pre-trained된 모델은 이미지가 차인지 보행자인지 오토바이인지 하나의 Class로 구분할 수 있는 모델이어야 한다는 것이다.  

**Size가 608 X 608**인 이미지 셋을 이용한다고 해보자.  
그리고 우리는 여러 object를 detect하기 위해 5개 종류의 anchor box를 사용한다고 해보자.  
그렇다면 처음 Input은 (m, 608, 608, 3)의 shape을 가질 것이고(m개의 이미지),  
이 Input은 Deep CNN을 거쳐 (m, 19, 19, 5, 85)의 shape으로 인코딩된다.  

여기서 **19, 19**는 Grid의 Size를 말한다.  
즉 아래의 그림처럼 19*19개의 각각의 Grid Cell이 어떤 object를 detect하고 있는지  
각각의 sign을 남긴다는 뜻이다.  
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo07.PNG" width="80%"></center>  

마지막 85의 길이를 가진 벡터는 아래와 같이 생겼다.
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo08.PNG" width="80%"></center>
이러한 벡터가 각 anchor box별로 하나 씩 있기 때문에 shape의 마지막 부분이 5, 85)가 되는 것이다.

(m, 19, 19, 5, 85)는 필요 이상으로 고차원이기 때문에 계산의 편리함을 위해  
(m, 19, 19, 425)로 Unroll해주도록 한다.  
다시 정리하자면, 위 matrix의 의미는, **19X19**개 각각의 Cell이 자기자신이 어떤 Label인지에  
대한 정보를 425개의 숫자로 표현하고 있다는 것이다.  

마지막 425의 길이를 가진 벡터를 분리하여 다음과 같이 Score를 계산해주도록 한다.
아래 코드에선 이를 **box_scores**라고 명명할 것이다.
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo09.PNG" width="80%"></center>

이제 위에서 구한 box_scores를 바탕으로,  
각 Grid Cell은 각각의 box(여기서는 5개의 anchor box가 있다.)와  
각각의 class(여기서는 80개의 classes가 있다.)에 대해 maximum probability를 찾고  
이를 토대로 자신의 정체성을 확립하게 된다. (나는 어떤 object이다라고 결정!)

Cell의 중심에 앵커의 중심을 놓고 bbox를 그리면, 아래와 같은 그림을 얻을 수 있을 것이다.
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo10.PNG" width="30%"></center>
Box가 너무 많기 때문에 일단 Score면에서 일정 수준 미달인 Box들을 제거해준다.  
Score가 낮다는 것은 Cell이 실제로 이 object를 detect했을 확률이 낮다는 것이다.  
그러고 나서 최고 Score를 받은 Box와 너무 많이 겹치는 Box들을 제거해준다.
(IOU가 높은 박스 제거) 이것이 바로 Non-max Suppression 과정이다.

자 이제 본격적으로 코드로 구현해보자.

### Filtering with a threshold on class scores
앞서 언급한 과정 중 첫 번째 단계를 실현하는 과정이다.  
아래 코드에 등장하는 객체에 대해 설명하자면,  
**box_confidence**: (19*19, 5, 1) - $P_c$를 담고 있다. (타겟 Object가 존재할 확률)  
**boxes**: (19*19, 5, 4) - Bounding Box 좌표 4개를 담고 있다.  
**box_class_probs**: (19*19, 5, 80) - Class 80개에 대한 확률 값을 담고 있다.  

```python
def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold=0.6):
    """
    threshold -- real value, if [highest class probability score < threshold],
    then get rid of the corresponding box

    Returns:
    몇 개의 박스를 선택하는지 모르기 때문에 None을 쓴다. 
    이 개수는 설정된 threshold의 값에 달려 있다.
    scores -- tensor of shape (None,), containing the class probability score for selected boxes
    boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w)
    classes -- tensor of shape (None,), containing the index of the class
    detected by the selected boxes
    """

    # Step 1: Compute box scores
    box_scores = np.multiply(box_confidence, box_class_probs)

    # Step 2: box_scores에서 제일 큰 스코어의 위치와 값을 찾는다.
    box_classes = K.argmax(box_scores, axis=-1)
    box_class_scores = K.max(box_scores, axis=-1)

    # Step 3: 아래 필터링 마스크는 부울렌 tensor로 threshold보다 큰 score를 가지는
    # box_class를 판별한다.
    filtering_mask = K.greater_equal(box_class_scores, threshold)

    # Step 4: Apply the mask to scores, boxes and classes
    scores = tf.boolean_mask(box_class_scores, filtering_mask)
    boxes = tf.boolean_mask(boxes, filtering_mask)
    classes = tf.boolean_mask(box_classes, filtering_mask)

    return scores, boxes, classes
```

위에서 tf.boolean_mask함수는 아래와 같은 형식을 갖는다.
```python
tf.boolean_mask(tensor, mask, name='boolean_mask', axis=None)
```
해당 텐서에 True, False로 구성된 mask를 씌우면 True와 연결된 값만 남고 나머지는 지워진다.  
예를 들어
```python
tensor = [0, 1, 2, 3]
mask = np.array([True, False, True, False])
boolean_mask(tensor, mask)
```
의 결과는 [0, 2]이다.  

### Non-max Suppression
이제 필요 없는 Box들을 제거해보자.  
아래 함수에서 사용할 메서드 중 tf.image.non_max_suppression에 대해 설명하자면,
```python
tf.image.non_max_suppression(boxes, scores, max_output_size,
    iou_threshold=0.5, score_threshold=float('-inf'), name=None)
```
아래와 같은 arguements를 가진다.  
1) **boxes**: A 2-D float Tensor of shape [num_boxes, 4].  
2) **scores**: A 1-D float Tensor of shape [num_boxes] representing a single score corresponding to each box.  
3) **max_output_size**: NMS에 의해 선택될 box의 최대 개수 = num_boxes  
4) **iou_threshold** 

반환하는 객체는  
**Selected_indices**: A 1-D integer Tensor of shape [M] representing the selected indices from the boxes tensor, where M <= max_output_size. 사실상 shape은 (M, 1)


```python
def yolo_non_max_suppression(scores, boxes, classes, max_boxes=10, iou_threshold=0.5):
    """Applies NMS to set of boxes
    scores -- tensor of shape (None,), output of yolo_filter_boxes()
    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes()
    that have been scaled to the image size (see later)
    classes -- tensor of shape (None,), output of yolo_filter_boxes()
    max_boxes -- integer, maximum number of predicted boxes you'd like
    iou_threshold -- real value, "intersection over union" threshold used for NMS

    Returns:
    scores -- tensor of shape (, None), predicted score for each box
    boxes -- tensor of shape (4, None), predicted box coordinates
    classes -- tensor of shape (, None), predicted class for each box

    Note: 이 함수는 scores, boxes, classes의 shape을 편의를 위해 transpose 시킬 것이다.
    """
    # 내가 예측하고 싶은 박스 최대치
    max_boxes_tensor = K.variable(max_boxes, dtype='int32')

    # initialize variable max_boxes_tensor
    K.get_session( ).run(tf.variables_initializer([max_boxes_tensor]))

    # get the list of indices corresponding to boxes you keep
    nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes_tensor, 
    iou_threshold=iou_threshold)

    # Use K.gather() to select only nms_indices from scores, boxes and classes
    scores = K.gather(scores, nms_indices)
    boxes = K.gather(boxes, nms_indices)
    classes = K.gather(classes, nms_indices)

    return scores, boxes, classes
```
첫 줄부터 설명하면
내가 만약 최대 10개의 Box를 선택하고 싶다면, arguement에서 max_boxes=10을 설정하고, 이 숫자를 tensor로 만든 것이 max_boxes_tensor이다.  
다음 줄에서 위 텐서를 집어넣고 세션을 실행시킨다.  
tf.image.non_max_suppression을 통해 nms_indices를 얻게 되는데 이는 선택할 Box의 indices를 의미한다.  

K.gather에 대해 설명하자면,
```python
tf.gather(params, indices, validate_indices=None, name=None, axis=0)
```
여기서 **params**는 최소 axis+1의 rank를 갖는 텐서이다.  
**indices**는 [0, params.shape[axis]) 범위 사이에 있는 텐서이다.

이 함수는 아래와 같이 indices에 따라 결과 값을 직관적으로 반환한다.
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo11.PNG" width="30%"></center>

이제 return되는 scores, boxes, classes에는 NMS를 거쳐 축소된 (max_boxes 이하로 줄어든)  
값이 담겨 있다.

### Wrapping up the filtering
위에서 정의한 두 함수를 바탕으로 종합 함수를 만들어보자.
```python
def yolo_eval(yolo_outputs, image_shape=(720., 1280.), max_boxes=10, 
score_threshold=0.6, iou_threshold=0.5):
    """
    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along 
    with their scores, box coordinates and classes.

    Arguments:
    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)),
    contains 4 tensors:
                    box_confidence: tensor of shape (None, 19, 19, 5, 1)
                    box_xy: tensor of shape (None, 19, 19, 5, 2)
                    box_wh: tensor of shape (None, 19, 19, 5, 2)
                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)
    image_shape -- Input shape을 담은 (2,) 텐서: 여기선 (608., 608.)이고 float32여야 함
    max_boxes -- integer, maximum number of predicted boxes you'd like
    score_threshold -- highest class probability score < threshold이면, 그 Box 제거
    iou_threshold -- IOU threshold used for NMS filtering

    Returns:
    scores -- tensor of shape (None, ), predicted score for each box
    boxes -- tensor of shape (None, 4), predicted box coordinates
    classes -- tensor of shape (None,), predicted class for each box
    """

    # Retrieve outputs of the YOLO model
    box_xy, box_wh, box_confidence, box_class_probs = yolo_outputs

    # Convert boxes to be ready for filtering functions
    boxes = yolo_boxes_to_corners(box_xy, box_wh)

    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, 
    threshold=score_threshold)

    # Scale boxes back to original image shape.
    boxes = scale_boxes(boxes, image_shape)

    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, 
    max_boxes=max_boxes, iou_threshold=iou_threshold)

    return scores, boxes, classes
```
두 번째 줄에 등장한 yolo_boxes_to_corners는 yad2모듈의 사용자 함수로,  
height, width, x_center, y_center로 표기했던 좌표를 편의를 위해 코너 좌표로 바꿔준다.

위에서 정의한 yolo_filter_boxes함수를 통해 수준 미달인 Box들을 제거해주고,  
다음 단계를 위해 yad2k의 scale_boxes 메서드를 통해 scaling을 해준다.  
다음 NMS를 적용하는 yolo_non_max_suppression함수를 통해 겹치는 Box들을 제거해준다.  

### Test
이제 실제 이미지에 적용해보자.
```python
yolo_model = load_model('C:/Users/YY/Documents/Winter Data/NN/Model/yolo.h5')
yolo_model.compile(optimizer='sgd', loss='categorical_crossentropy')
# yolo_model.count_params()
# yolo_model.summary()

sess = K.get_session()
class_names = read_classes("path/coco_classes.txt")
anchors = read_anchors("path/yolo_anchors.txt")
image_shape = (720., 1280.)

yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names))
scores, boxes, classes = yolo_eval(yolo_outputs, image_shape)
```

적용을 위해선 class_names와 anchors가 필요하다. 각 txt파일의 내용은 아래와 같다.  
**coco_classes**: person/bicycle/car/motorbike/aeroplane/bus/train ... 등 80개 class  
**anchor**: 0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828 = 5개의 anchor box 길이  

```python
def predict(sess, image_file):
    """Runs the graph stored in "sess" to predict boxes for "image_file".
    Prints and plots the preditions.

    Returns:
    out_scores -- tensor of shape (None, ), scores of the predicted boxes
    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes
    out_classes -- tensor of shape (None, ), class index of the predicted boxes
    """

    # Preprocess your image
    image, image_data = preprocess_image(image_file, model_image_size=(608, 608))

    # Run the session
    out_scores, out_boxes, out_classes =
    sess.run([scores, boxes, classes],
     feed_dict={yolo_model.input: image_data, K.learning_phase( ): 0})

    # Print predictions info
    print('Found {} boxes for {}'.format(len(out_boxes), image_file))
    # Generate colors for drawing bounding boxes.
    colors = generate_colors(class_names)
    # Draw bounding boxes on the image file
    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)
    # Save the predicted bounding box on the image
    # image.save(os.path.join("out", image_file), quality=90)
    image.save('path/output01.jpg', quality=90)

    return out_scores, out_boxes, out_classes

image_file = ("path/image01.jpg")
out_scores, out_boxes, out_classes = predict(sess, image_file)
```
image_file에 이미지가 있는 path를 입력하고 predict함수를 이용하면 원하는 결과물을 얻을 수 있다.  
  
위 함수들에 보충설명을 하자면, image, image_data라는 output을 반환하는 preprocess_image메서드는 원하는 size에 맞게 이미지를 조정해준다.  
  
image는 이미지 파일 자체를 말하며, image_file은 그 이미지의 RGB 데이터를 저장한다.  
generate_colors를 통해 다양한 색깔을 미리 생성해두면,  
draw_boxes는 찾고자하는 object 둘레에 bounding box를 그려준다.  
quality 설정을 통해 저장하고자 하는 이미지의 화질을 조정할 수 있다.  
image.save를 통해 자동으로 output 이미지를 저장할 수 있다.  


### Check the result
Input:
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo12.jpg" width="50%"></center>

Output:
<center><img src="/public/img/Deep_Learning/2018_07_21_YOLO/yolo13.jpg" width="50%"></center>

