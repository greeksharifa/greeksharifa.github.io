---
layout: post
title: VQ-VAE 논문 설명(Neural Discrete Representation Learning)
author: YouWon
categories: [Discrete Representation]
tags: [Paper_Review, Multimodal, VQVAE]
---

---

이 글에서는 2017년 NIPS에 게재된 Neural Discrete Representation Learning(흔히 VQ-VAE라 부른다) 논문을 살펴보고자 한다.


중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.

---

# Neural Discrete Representation Learning

논문 링크: **[Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)**

## 초록(Abstract)

어떠한 지도 없이 유용한 표현을 학습하는 것은 머신 러닝의 핵심 과제로 남아 있다. 본 논문에서는 이러한 이산 표현을 학습하는 간단하면서도 강력한 생성 모델을 제안한다. 제안하는 **VQ-VAE(Vector Quantised-Variational AutoEncoder)** 모델은 두 가지 면에서 VAE와 다르다:

1. Encoder network는 연속적인 codes를 생성하나 VQ-VAE는 이산 codes를 출력한다. 
2. Prior는 정적(static)이지 않고 대신 학습이 가능하다.

이산 잠재 표현을 학습하기 위해 벡터 양자화(**VQ: Vector Quantisation**)의 아이디어를 통합하였다. VQ 방법을 사용하면 모델이  강력한 autoregressive decoder와 짝을 이룰 때 latent들이 무시되는 **"Posterior Collapse"** 문제(VAE framework에서 곧잘 나타난다)를 피할 수 있다. 

이러한 표현을 autoregressive prior와 짝지으면 모델은 고품질 image, video, audio을 생성할 수 있을 뿐만 아니라 speaker conversion 및 음소의 비지도 학습을 수행하여 학습된 표현이 유용함을 보일 수 있다.


---

## 1. 서론(Introduction)

Image, audio, video를 생성하는 분야에서는 많은 발전이 있었다. 어려운 과제인 few-shot learning, domain adaptation, RL 등은 학습된 표현에 크게 의존하였으나 일반적 표현을 얻는 것은 여전히 한계가 있다. Pixel domain에서 가장 효과적인 모델은 PixelCNN([1](https://arxiv.org/abs/1606.05328), [2](https://arxiv.org/abs/1601.06759))이지만, 연속적인 표현을 사용한다. 이 논문에서는, 연속(continuous)이 아닌, 이산적(discrete)인 표현을 다룬다.

언어는 본질적으로 이산적인 성질을 가지며(단어의 수는 한정적이며, 단어와 단어 사이 중간쯤이 명확히 정의되지 않음을 생각하면 된다), 음성도 비슷한 특성을 가진다. 이미지는 언어로 표현될 수 있다. 이러한 점에서, 이산적인 표현은 이러한 이산적인 domain에 잘 맞을 것이라 생각할 수 있다. 

이 논문에서는 VAE와 이산표현을 결합한 새로운 생성모델(**VQ-VAE**)을 제안한다. **Vector Quantisation(VQ)**를 사용하여 너무 큰 분산으로 생기는 어려움을 피하면서 학습하기 편하고 **Posterior Collapse** 문제를 회피할 수 있다. 그러면서도 연속표현을 사용하는 모델과 비등하면서도 이산표현의 유연함을 제공한다.

VQ-VAE는 데이터 공간을 효과적으로 사용할 수 있게 하여, 큰 차원의 데이터(이미지의 픽셀, 음성의 음소, 텍스트의 메시지 등)에서 *local*이라 부르는 작은 부분, noise에 집중하는 대신 중요한 feature만을 성공적으로 모델링할 수 있게 한다.

또 강력한 prior를 제공할 수 있는데, 단어나 음소에 대한 어떤 사전 지식 없이도 언어의 구조를 파악할 수 있거나, 말하는 내용은 그대로 두면서 목소리만 바꿀 수(speaker conversion)도 있다.

그래서 이 논문이 기여한 바는,

- "posterior collapse" 문제에서 자유롭고 큰 분산 문제가 없는, 이산표현을 사용하는 VQ-VAE 모델을 제안한다.
- 이산표현 모델 VQ-VAE는 연속표현 모델만큼이나 성능이 좋음을 보인다.
- 강력한 prior가 있을 때, sample은 speech/video generation과 같은 다양한 응용 분야에서 뛰어난 결과를 생성한다.
- 어떤 명시적인 지도 없이도 언어의 속성을 파악하고 speaker를 바꿀 수도 있다.



---

## 2. 관련 연구(Related Work)

VAE는 여러 방면에서 연구되어 왔지만, discrete domain을 가지는 분야에서조차 연속표현을 사용해왔다.

사실 이산적인 변수는 backprop이 불가능하여, 이산표현의 특성을 일부 가지면서도 미분이 가능하도록 대체 방안이 연구되었다. [Concrete softmax](https://arxiv.org/abs/1611.00712) 및 [Gumbel-softmax](https://arxiv.org/abs/1611.01144)가 대표적인 예이다. 

또, PixelCNN, Vector Quantisation 등이 연관 분야로 수록되어 있다.


---

## 3. VQ-VAE


VAE가 가장 연관이 있을 분야라고 논문에서는 언급한다(그러나 사실 VAE가 아니라 AE에 더 가깝다고 생각되기도 한다). 

VAE는 input data $x$, prior distribution $p(z)$가 주어질 때, 

- 이산 latent random variable $z$에 대해 posterior distribution $q(z \vert x)$를 parameterise하는 encoder,
- input data에 대한 분포 $p(x \vert z)$를 다루는 decoder

로 구성된다.

VAE에서 내부적으로 취급하는 분포는 대개 Gaussian 분포를 따른다고 가정한다. 확장 버전은 autoregressive prior, posterior model, normalising flow, inverse autoregressive posterior 등을 포함하기도 한다.

VQ-VAE는 여기에 이산 표현을 다루도록 한다. VQ른 사용하면서, posterior과 prior distribution은 categorical하며, 이 분포로부터 생성된 sample은 embedding table을 indexing한다. 이 embeddings는 decoder의 입력으로 들어간다.


### 3.1 Discrete Latent variables


<center><img src="/public/img/2021-11-07-VQVAE/fig01.png" width="100%" alt="HERO"></center>


위 그림에서 embedding $e \in R^{K \times D}$가 이산표현을 나타낸다. 이를 codebook이라 하며, $K$는 이산 표현 공간의 크기($K$-way categorical과 같음), $D$는 각 embedding vector $e_i$의 차원이다. 

즉 $e_i \in R^D, i \in 1, 2, ..., K$이며, embedding vector가 $K$개가 있는 것이다.

모델의 encoder는 입력 $x$를 받아 $z_e(x)$를 출력한다. 이산표현벡터 $z$는 embedding space $e$에서 가장 가까운 embedding vector를 찾는다(look-up).

$$\begin{pmatrix}a & b\\\ c & d\end{pmatrix}$$

$$\begin{eqnarray} a^2 + b^2 &=& c^2 \\ &=& 5  \end{eqnarray}$$

$$\begin{eqnarray} 
y &=& x^4 + 4      \nonumber \\
&=& (x^2+2)^2 -4x^2 \nonumber \\
&\le&(x^2+2)^2    \nonumber
\end{eqnarray}$$


### 3.2 Learning



### 3.3 Prior





## 4. 실험(Experiments)


### 4.1 Comparison with continuous variables




### 4.2 Images




### 4.3 Audio




### 4.4 Video





---

## 5. 결론(Conclusion)




---

## 참고문헌(References)

논문 참조!

--- 

