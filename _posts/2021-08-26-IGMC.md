---
layout: post
title: IGMC (Inductive Graph-based Matrix Completion) 설명
author: Youyoung
categories: [Machine_Learning]
tags: [Machine_Learning, Recommendation System, Paper_Review]
---

이번 글에서는 IGMC란 알고리즘에 대해 다뤄보겠다. 상세한 내용을 원하면 [논문 원본](https://arxiv.org/abs/1904.12058)을 참고하길 바라며, 본 글에서는 핵심적인 부분에 대해 요약 정리하도록 할 것이다.  

[Github](https://github.com/ocasoyy/pytorch-gnn-research)에 관련 코드 또한 정리해서 업데이트할 예정이다.  

---
# Inductive Matrix Completion based on Graph Neural Networks 설명  
## 1. Background  
행렬 분해 알고리즘의 기본에 대해 알고 싶다면 [이 글](https://greeksharifa.github.io/machine_learning/2019/12/20/Matrix-Factorization/)을 참조하길 바란다. Graph Neural Networks을 이용하여 Matrix Completion 알고리즘을 구축한 대표적인 예는 `GC-MC`가 될 것이다. 이에 대해 알고 싶다면 [이 글](https://greeksharifa.github.io/machine_learning/2020/12/06/GCMC/)을 참고하면 좋다. `GC-MC`는 Bipartite Graph에 직접적으로 GNN을 적용하여 user와 item의 잠재 벡터를 추출하였다. 이전의 대부분의 연구와 마찬가지로 `GC-MC` 또한 **Transductive**한 모델로, 학습 셋에서 사용하지 않은 unseen nodes에 대한 대응이 불가능하다는 단점을 지니고 있었다. [GraphSAGE](https://greeksharifa.github.io/machine_learning/2020/12/31/Graph-Sage/), [PinSAGE](https://greeksharifa.github.io/machine_learning/2021/02/21/Pin-Sage/) 등 여러 알고리즘에서 **Inductive**한 방법론을 제시하기는 했지만 이들 방법론을 적용하기 위해서는 node의 feature가 풍부해야 한다.  

node feature에 크게 의존하지 않으면서도 **Inductive**한 학습/예측 환경으로 앞서 기술한 문제점들을 상당 부분 해결한 모델이 `IGMC: Inductive Graph-based Matrix Completion`이다.  

---
## 2. IGMC 알고리즘 설명  
일반적으로 통용되는 방식으로 기호를 정의하고 시작하겠다.  

|기호|설명|
|:------------:|:------------:|
| $\mathbf{G}$ | undirected bipartite graph |
| $\mathbf{R}$ | rating matrix |
| $ u, v $ | 각각 user, item node |
| $ r = \mathbf{R}_{u, v} $ | user $u$ 가 item $v$ 에 부여한 평점 |
| $ \mathcal{N}_r (u)$ | user $u$가 평점 $r$ 을 준 $v$ 의 집합, 즉 edge type $r$ 에 대한 $u$ 의 이웃 집합 |

`IGMC`의 핵심 아이디어는 user $u$ 와 item $i$ 에 관련이 있는 local subgraph를 추출하여 이를 학습에 활용한다는 점이다.  

<center><img src="/public/img/Machine_Learning/2021-08-26-IGMC/01.PNG" width="80%"></center>  

위 그림을 보면 이해가 될 것이다. 진한 초록색 5점의 예시를 보면, $u_2$ 가 $i_7$ 에게 5점을 부여한 것을 알 수 있다. 그렇다면 이 두 node에 대한 **1-hop enclosing subgraph**는 $u_2$ 의 1-hop neighbor인 [ $i_5, i_7, i_8$ ], 그리고 $i_7$ 의 1-hop neighbor인 [ $u_2, u_3, u_4$ ]로 구성되는 것이다. 물론 최종적으로 학습/예측을 할 때는 Target Rating인 5점은 masking될 것이다.  

subgraph를 추출하는 BFS 과정은 아래 표에 나와있다.  

<center><img src="/public/img/Machine_Learning/2021-08-26-IGMC/02.PNG" width="80%"></center>  

다음으로는 **node labeling** 과정이 필요하다. 여기에서의 label은 y값이 아니고, 각 node의 임시 ID를 의미한다. subgraph를 추출하였으면 이 node를 구분할 id가 필요한데, `IGMC`의 경우 global graph를 참조하는 경우는 없고 오직 subgraph만을 이용하여 학습/예측을 수행하기 때문에 기존의 id 방식을 그대로 따를 필요가 없다. `IGMC`의 구조에 맞게 바꿔보자.  

|구분|user id|item id|
|:----------:|:----------:|:----------:|
|target| 0 | 1 |
|1-hop|2|3|
|2-hop|4|5|
|h-hop|2h|2h+1|

위와 같이 subgraph 내에서의 node id를 다시 붙여주면 (node labeling) 각각의 node들은 역할에 맞게 구분된다. 위 label을 통해 0과 1을 추출하여 target node를 구분할 수 있고, 홀수/짝수 구분을 통해 user/item을 구분할 수 있으며, $h$ 의 값을 통해 어떤 계층(h-hop)에 속하는지도 파악할 수 있다. 이러한 node label을 One-hot 인코딩하여 초기 node feature로 활용할 수 있다.  

다음 단계는 **GNN**을 통해 학습을 수행하는 것이다. `IGMC`의 특징이라면 `GC-MC`를 비롯한 여러 알고리즘과 달리 node-level GNN이 아니라 graph-level GNN을 사용한다는 것인데, 논문에서는 이 부분에 대해 장점을 크게 어필하고 단점을 끝에 살짝 언급한 수준에 그쳤는데 상황에 따라 단점이 더 클 수도 있다는 개인적인 의견을 덧붙인다.  

GNN의 기본 구조를 message passing과 pooling(or aggregation)이라고 정의할 때, message passing은 `Relational Graph Convolution Operator: R-GCN` 포맷을 사용하였다.  

$$ x^{l+1} = W_0^l x_i^l + \Sigma_{r \in \mathcal{R}} \Sigma_{j \in \mathcal{N}_r(i)} \frac{1}{\vert \mathcal{N}_r (i) \vert} W_r^l x_j^l $$  

활성화 함수로는 tanh를 사용하게 된다. 1번째 $\Sigma$ 는 각 Rating 별로 따로 파라미터를 둔다는 것을 의미하며, 그 내부에서는 일반적인 GCN이 적용된다. 다만 이 때 이웃 집합의 크기를 나타내는 $\mathcal{N}_r^i$ 가 global graph가 아닌 local subgraph에서 계산된 것이기 때문에 효율적으로 연산이 가능하다는 점은 기억해둘 필요가 있다. 이렇게 쭉 진행해서 $L$ 번째 Layer까지 값을 얻었으면 아래와 같이 최종 hideen representation을 얻는다.  

$$ \mathbf{h}_i = concat(x_i^1, x_i^2, ..., x_i^L) $$  

위와 같은 방식을 적용하면, jumping network의 효과도 있을 것으로 보인다. 이렇게 user, item에 대해 각각의 hidden 벡터를 구한 뒤 이를 다시 하나의 벡터로 결합하면 (sub) graph representation을 얻을 수 있다. 이렇게 graph 표현 벡터를 얻는 것을 graph-level GNN이라고 한다.  

$$ \mathbf{g} = concat(\mathbf{h}_u, \mathbf{h}_v) $$  

위와 같은 pooling 과정은 간단하지만 실제로 적용하였을 때 우수한 성과를 내는 것이 실험으로 증명되었다고 한다. MLP를 적용해서 최종적으로 rating 예측 값을 얻을 수 있다.  

$$ \hat{r} = \mathbf{w}^T \sigma (\mathbf{W} \mathbf{g}) $$  

활성화 함수는 ReLU를 사용하였다.  

---
## 3. Model Training  



---
## 4. 실험 결과와 인사이트 종합  






side info 없이도 가능함 (node feature의 풍부함에 의존 x)
local graph pattern은 user-item 관계를 파악하기에 충분함
long-range dependency는 추천 시스템을 구상할 때 크게 중요하지 않음

unseen nodes에 대해 대응이 가능하지만 아예 아무 interaction이 없으면 안됨


