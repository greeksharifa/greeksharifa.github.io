---
layout: post
title: DCGAN(Deep Convolutional GAN)
author: YouWon
categories: Paper_Review
tags: [GAN, Machine Learning]
---

---

이 글에서는 DCGAN(Deep Convolutional GAN)을 알아보도록 한다.  
DCGAN은 Alee Radford와 Luke Metz가 2015년(2016년 최종 수정) 제안한 GAN의 개선 모델이다.

간단히 GAN은 두 가지 모델을 동시에 학습시키는 구조이다. G(Generator, 생성자)라는 모델은 직접 볼 수 없는 진짜 데이터와 최대한 비슷하게 생긴 가짜 데이터를 만드려고 하고, D(Distriminator, 식별자 또는 감별자)라는 모델은 자신에게 주어진 데이터가 진짜 데이터인지 가짜 데이터인지 최대한 구분하려고 한다.  

논문에서는 설명을 위한 예시로 화폐 위조범(G)와 경찰(D)을 제시하였다. 다만 차이가 있다면,
- 위조범은 진짜를 볼 수 없다는 것(그래서 장님blind라 불린다)
- 경찰은 자신이 판별한 결과를 위조범에게 알려준다
는 것이 있다.

참고로 GAN은 특정한 모델 구조를 가진 것이 아니므로 코드가 특별히 정해진 것은 아니다.

논문을 적절히 번역 및 요약하는 것으로 시작한다. 많은 부분을 생략할 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.

---

# 논문(GAN)

논문 링크: **[Deep Convolutional GAN](https://arxiv.org/abs/1511.06434)**

## 초록(Abstract)

이 논문에서는 

---

## 서론(Introduction)



---

## 관련 연구(Related Works)

궁금하면 읽어보자.


---

## 적대적 망(Adversarial nets)

기호 | 설명
-------- | --------
$x$ | 데이터

<center><img src="/public/img/N/01.PNG" width="100%"></center>



---

## 이론적 결과(Theoretical Results)


---

## 실험(Experiments)


---

---

## 결론 및 추후 연구(Conclusions and future work)

1. conditional generative model로 발전시킬 수 있다(CGAN).
2. Learned approximate inference는 $x$가 주어졌을 때 $z$를 예측하는 보조 네트워크를 학습함으로써 수행될 수 있다. 
3. parameters를 공유하는 조건부 모델을 학습함으로써 다른 조건부 모델을 대략 모델링 할 수 있다. 특히, deterministic MP-DBM의 stochastic extension의 구현에 대부분의 네트워크를 쓸 수 있다.
4. Semi-supervised learning에도 활용 가능하다. classifier의 성능 향상을 꾀할 수 있다.
5. 효율성 개선: G와 D를 조정하는 더 나은 방법이나 학습하는 동안 sample $z$에 대한 더 나은 distributions을 결정하는 등의 방법으로 속도를 높일 수 있다.

--- 

# 학습 방법

GAN은 서로 경쟁하는 두 가지 모델을 학습시킨다. GAN을 쓰려면 다음 방법을 따른다.

1. 우선 다음을 정의한다.
    1. R(Real): 실제 데이터. 논문에선 $x$로 표시
    2. I(Input 또는 Imaginary): G가 가짜 데이터를 생성할 source. 논문에선 $z$로 표시. 
        - $G(z)$는 $G$가 $z$를 입력으로 받아 생성한 가짜 데이터이다.
    3. $G$(generator): 생성자, 위조범
    4. $D$(Distriminator): 감별자 또는 식별자, 경찰
2. 다음 전체 과정을 `num_epochs` 동안 반복한다: 
    1. D를 training하는 과정(`d_steps`만큼 반복): **D와 G를 모두 사용은 하지만 D의 parameter만 업데이트한다.**
        1. $D$에 실제 데이터($x$)와 정답(1)을 입력으로 주고 loss를 계산한다.
        2. $D$에 가짜 데이터($G(z)$)와 정답(0)을 입력으로 주고 loss를 계산한다.
        3. 두 loss를 합친 후 $D$의 parameter를 업데이트한다.
    2. G를 training하는 과정(`g_steps`만큼 반복): **D와 G를 모두 사용은 하지만 G의 parameter만 업데이트한다.**
        1. $D$에 가짜 데이터($G(z)$)와 정답(1)을 입력으로 주고 loss를 계산한다.
        2. 계산한 loss를 이용하여 $G$의 parameter를 업데이트한다.

---

# 튜토리얼

## MNIST 튜토리얼

GAN의 핵심 부분을 제외한 부분은 [여기](https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/)를 참고하면 된다.

---

# 이후 연구들

GAN 이후로 수많은 발전된 GAN이 연구되어 발표되었다. 가장 중요한 것 두 개는 GAN의 학습 불안정성을 많이 개선시킨 DCGAN(Deep Convolutional GAN), 단순 생성이 목적이 아닌 원하는 형태의 이미지를 생성시킬 수 있게 하는 CGAN(Conditional GAN)일 듯 하다.

많은 GAN들(DCGAN, LSGAN, WGAN, WGAN_GP, DRAGAN, CGAN, infoGAN, ACGAN, EBGAN, BEGAN 등)에 대한 설명은 [다음 글]()에서 진행하도록 하겠다.

---
