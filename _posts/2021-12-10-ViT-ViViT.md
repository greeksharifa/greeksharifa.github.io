---
layout: post
title: ViT(Vision Transformer), ViViT(Video ViT) 논문 설명(An Image is Worth 16x16 Words - Transformers for Image Recognition at Scale, ViViT - A Video Vision Transformer)
author: YouWon
categories: [Computer Vision]
tags: [Transformer, ViT, Google Research, Facebook Research]
---

---

이 글에서는 ViT와 ViViT(Video ViT) 논문을 간략하게 정리한다.

---

# ViT(An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale)

논문 링크: **[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)**

Github: [https://github.com/google-research/vision_transformer](https://github.com/google-research/vision_transformer)

- 2020년 10월, ICLR 2021
- Google Research, Brain Team
- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, et al.


이미지 class 분류를 위해 이미지를 여러 patch로 잘라 Transformer에 넣는 방법을 제안한다.


<center><img src="/public/img/2021-12-10-ViT-ViViT/01.png" width="100%"></center>

전체 구조를 순서대로 따라가보자. 왼쪽 아래에서부터 출발한다.

1. 이미지를 16 x 16 크기의 patch로 나눈다. 자연어에서 word token sequence가 주어지는 것처럼, 여기서는 이미지를 16 x 16크기의 token으로 취급한다.
    - 이미지 크기가 224 x 224라면 총 81($=N$)개의 patch가 생긴다.
2. 이 patch들의 linear embedding을 Transformer에 입력으로 주어야 하는데, 그 전에
    - `[CLS]` token embedding이 맨 앞에 추가된다.
    - Linear Embedding을 구하는 것은 Linear Projection $E \in \mathbb{R}^{(P^2C)\times D}$ 를 통해 가능하다.  
        - 입력의 차원 변화는 $P^2C \rightarrow D$이다. 
        - 여기서 $C=3, D=1024$를 사용했다. 
    - 여기까지 보면, word를 embedding으로 바꿔서 Transformer에 입력으로 주는 것과 거의 동일한 과정이다.
    - Patch Embedding에 더해 Position Embedding이란 것을 추가하는 것을 볼 수가 있다. 이는 [Transformer](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/)나 [BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/)에서 위치 정보를 추가해 주는 것과 같은 과정이다. 
        - $E_{pos} \in  \mathbb{R}^{(N+1)\times D}$을 곱한다. 이미지 patch 개수 $N$에 `[CLS]`를 더해 총 $N+1$개에 position embedding을 불인다.
3. Transformer에 이 embedding들을 태우는데 Transformer는 Norm이 맨 앞으로 온 것을 빼면 똑같은 구조를 사용한다.
4. 그리고 input image를 분류하기 위해 MLP에 태운다. 이 과정을 통해 이미지 분류를 수행할 수 있다.
    - MLP는 아래 식을 따른다. 여기서 MSA는 Multi-head Self-Attention이다.


<center><img src="/public/img/2021-12-10-ViT-ViViT/02.png" width="100%"></center>


실험 결과는 다음과 갈다.

<center><img src="/public/img/2021-12-10-ViT-ViViT/03.png" width="100%"></center>


이 논문의 강점은,

- 꽤 많은 일반적인 데이터셋에서 좋은 성능을 보인다.
- Image를 Transformer에 적용시켰다는 점에서 아이디어를 높이 살 수 있다.
- 데이터가 많기만 하다면 spatial locality를 넘어 더 많은 어떤 feature를 잡아낼 수 있다.


단점은,

- 계산량이 매우 많다.
- JFT-300M과 같이 매우 큰 데이터셋에서 학습했을 때에만 잘 동작한다.
- 이미지를 patch로 잘라서 일렬로 집어넣기 때문에 spatial info(inductive bias)를 활용하지 못한다. 작은 데이터셋에서 결과가 잘 나오지 않는 이유이기도 하다.

가장 작은 데이터셋인 ImageNet에서 성능이 가장 좋지 않다는 점을 아래 결과에서 확인할 수 있다.

<center><img src="/public/img/2021-12-10-ViT-ViViT/04.png" width="100%"></center>


---

# ViViT: A Video Vision Transformer

논문 링크: **[ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)**

Github: [https://github.com/google-research/scenic/tree/main/scenic/projects/vivit](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit)

- 2021년 3월, ICCV 2021
- Google Research
- Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid

위의 ViT 논문의 아이디어를 그대로 가져와 Video에 적용한 논문이다.

<center><img src="/public/img/2021-12-10-ViT-ViViT/11.png" width="100%"></center>

1. 비디오의 각 frame을 $n_w \times n_h$ patch로 나누어 각 patch들은 Transformer Encoder에서 contextualize된다.
    - $n_h$: # of rows
    - $n_w$: # of columns
    - $n_t$: # of frames
2. 그러나 attention 계산량이 매우 많다.
    - 총 patch 수가 ($n_t \times n_h \times n_w$)
    - 따라서 계산은 ($n_t^2 \times n_h^2 \times n_w^2$)
3. 따라서 전체 frame 대신 일부만 균등선택하여 계산한다(uniformly frame sampling)
    - 또한 Tubelet embedding을 사용한다. 토큰화 중에 미리 spatio-temporal info를 합친 다음 계산하면 계산량을 줄일 수 있다.

<center><img src="/public/img/2021-12-10-ViT-ViViT/12.png" width="75%"></center>



이 논문에서는 생각할 수 있는 여러 개의 모델을 설명한다. 하나씩 살펴보자.

## Model 1: Spatio-temporal attention

사실상 brute-force 모델에 가깝다. 모든 spatio-temporal token을 forward한다.

## Model 2: Factorised encoder

- 모든 spatio-temporal token에 대해 계산을 진행하는 대신, 일단 spatio token에 대해서만 계산을 진행한다(by ViT).
- 각 frame을 embedding으로 바꾼 뒤 Temporal Transformer Encoder에 태운다.
- 이러면 계산량은 다음과 같다: $(n_h^2n_w^2 + n_t^2)$

<center><img src="/public/img/2021-12-10-ViT-ViViT/13.png" width="75%"></center>

## Model 3: Factorised self-attention

- 모든 token 쌍에 대해 Multi-head Self-attention을 수행하는 대신, 먼저 spatial 부분에 대해 self-attention을 수행한다. 그리고 Temporally하게 self-attention을 수행한다.
- Naive model(Model 1)과 같은 수의 Transformer layer를 갖는다.
- `[CLS]` token은 사용되지 않는다.


<center><img src="/public/img/2021-12-10-ViT-ViViT/14.png" width="75%"></center>

Factorisesd self-attention은 다음과 같이 정의된다.

<center><img src="/public/img/2021-12-10-ViT-ViViT/15.png" width="75%"></center>


## Model 4: Factorised dot-product attention

Transformer가 multi-head 연산을 포함한다는 것을 기억하자.

- 절반의 attention head는 spatial한 부분을 key, value로 다룬다. (Spatial Head)
    - $\mathbf{K}_s, \mathbf{V}_s \in \mathbb{R}^{n_h \cdot n_w \times d}, \quad  \mathbf{Y}_s = \text{Attention}(\mathbf{Q}, \mathbf{K}_s, \mathbf{V}_s)$
- 나머지 절반은 같은 temporal index에서 key, value를 다룬다. (Temporal Head)
    - $\mathbf{K}_t, \mathbf{V}_t \in \mathbb{R}^{n_t \times d}, \quad \mathbf{Y}_t = \text{Attention}(\mathbf{Q}, \mathbf{K}_t, \mathbf{V}_t) $


위의 연산을 수행한 뒤 최종 결과는

$$ \mathbf{Y} = \text{Concat}(\mathbf{Y}_s, \mathbf{Y}_t)\mathbf{W}_O $$

이 된다.

<center><img src="/public/img/2021-12-10-ViT-ViViT/16.png" width="75%"></center>



## Experiments 

ViT가 매우 큰 데이터셋에서만 좋은 결과를 얻을 수 있었기 때문에, ViT를 initialization으로 사용하였다.

위의 모델들을 실험한 결과는 다음과 같다. 또한, Temporal Transformer의 layer 수에 따라서도 Top-1 Accuracy를 측정한 결과도 있다.

<center><img src="/public/img/2021-12-10-ViT-ViViT/17.png" width="80%"></center>

결과가 약간 의외(?)인데, Model 1이 가장 연산이 비싸지만 성능은 제일 좋다. Model 2는 성능이 살짝 낮지만 연산량 대비해서는 꽤 효율적이라 할 수 있다.

---

# TimeSFormer: Is Space-Time Attention All You Need for Video Understanding?


논문 링크: **[Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095)**

Github: https://github.com/facebookresearch/TimeSformer

- 2021년 6월, ICMl 2021
- Facebook Research
- Gedas Bertasius, Heng Wang, Lorenzo Torresani

Facebook에서 만든 논문인데 위의 ViViT와 거의 비슷하다. ViT를 Video에 적용시킨 논문이다.

<center><img src="/public/img/2021-12-10-ViT-ViViT/21.png" width="100%"></center>

<center><img src="/public/img/2021-12-10-ViT-ViViT/22.png" width="100%"></center>
