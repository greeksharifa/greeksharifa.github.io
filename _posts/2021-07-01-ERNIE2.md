---
layout: post
title: ERNIE 논문 설명(ERNIE - Enhanced Language Representation with Informative Entities)
author: YouWon
categories: [NLP(Natural Language Processing) / RNNs]
tags: [Paper_Review, NLP, ERNIE]
---

---

이 글에서는 Baidu에서 만든 모델 시리즈 ERNIE 중 두 번째(ERNIE - Enhanced Language Representation with Informative Entities)를 살펴보고자 한다.

ERNIE 시리즈는 다음과 같다.

- [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223), Yu sun el al., 2019년 4월
- **[ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/abs/1905.07129), Zhengyan Zhang et al., 2019년 5월**
- [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/abs/1907.12412), Yu Sun et al., 2019년 6월
- [ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph](https://arxiv.org/abs/2006.16934), Fei Yu et al., 2019년 6월

중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.

---

# ERNIE: Enhanced Language Representation with Informative Entities

논문 링크: **[ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1905.07129)**

Official Code: [Github](https://github.com/thunlp/ERNIE)

## 초록(Abstract)

대규모 말뭉치에서 사전학습하는 [BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/)같은 자연어표현 모델은 텍스트로부터 충분한 의미적 패턴을 잘 잡아내며 다양한 NLP 문제에서 미세조정을 통해 일관되게 성능이 향상되는 모습을 보인다.  
그러나, 현존하는 자연어 모델은 더 나은 언어 이해를 위한 풍부한 구조적 지식을 제공할 수 있는 지식 그래프(Knowledge Graphs, KGs)를 거의 고려하지 않는다. 우리는 KGs 내의 유익한 entity들이 외부 지식을 통해 언어표현을 향상시킬 수 있다고 주장한다. 본 논문에서, 대규모 텍스트 말뭉치와 KGs를 사용하여 향상된 언어표현 모델 ERNIE를 학습하며 이는 어휘적, 의미적, 지식 정보를 동시에 학습할 수 있다.  
실험 결과는 ERNIE가 다양한 지식기반 문제에서 상당한 발전을 이루었고 또한 다를 NLP 문제에서도 SOTA 모델 BERT에 필적할 만하다. 

<center><img src="/public/img/2021-06-14-ERNIE/01.png" width="50%" alt="ERNIE"></center>

---

## 1. 서론(Introduction)

Feature 기반이나, 미세조정 등을 사용하는 언어표현 사전학습 모델들은 텍스트로부터 풍부한 언어표현을 얻을 수 있고 이는 많은 자연어처리 문제에서 상당한 효과를 거두었다. 최근 제안되었던 [BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/)와 그 응용 모델들은 named entity, 질답, 자연어추론, 텍스트 분류 등 많은 자연어 문제에서 뛰어난 성과를 보여주었다.  

그러나 이러한 언어표현 사전학습 모델은  언어이해를 위해 필요한 연관 지식을 무시한다. 아래 그림에서 보듯이, `Blowin’ in the Wind and Chronicles: Volume One are song and book`라는 지식 없이 `Bob Dylan`의 직업 `songwriter` and `writer`를 식별하는 것은 어려운 일이다. 더욱이, 관계 분류 문제에서 `composer`과 `author`와 같은 fine-grained 관계를 찾는 것은 거의 불가능하다. `UNK wrote UNK in UNK`와 같은 문장은 언어표현 사전학습 모델에게 문법적으로 모호하다. 따라서, 풍부한 지식 정보를 활용하는 것이 언어이해에 도움을 주고 다양한 지식기반 문제에서 이득을 가져다줄 것이다.

<center><img src="/public/img/2021-07-01-ERNIE2/01.png" width="100%" alt="ERNIE"></center>


언어표현 모델에 외부 지식을 잘 포함시키려면 다음 두 가지를 해결해야 한다.

1. 구조화된 지식 Encoding
    - 주어진 텍스트와 관련하여 효율적으로 추출하고 언어표현 모델을 위해 KGs에서 유용한 정보를 찾는 것은 중요한 문제이다.
2. 다차원적 정보 융합
    - 언어표현을 위한 사전학습 절차는 지식표현과는 상당히 다른데, 2개의 독립적인 벡터공간을 다루기 때문이다.

어떻게 특별한 사전학습 목적함수를 어휘적, 문법적, 지식정보와 융합하는 것은 또 다른 도전과제이다.

위와 같은 문제를 극복하기 위해 **ERNIE**(**E**nhanced Language **R**epresentatio**N** with **I**nformative **E**ntities)를 제안한다. 대규모 말뭉치와 KGs를 모두 사용한다.

1. 지식 정보를 추출하고 인코딩하기 위해, 텍스트에서 named entity(명명 객체)를 구분하고 KGs 안의 해당하는 entity를 언급하는 것과 묶는다. KGs의 그래프 기반 사실들을 직접 사용하기보다는 TransE를 사용하여 그래프 구조를 인코딩하고 지식 embedding을 ERNIE의 입력으로 사용한다.  텍스트와 KGs 사이의 일치(alignment)에 기반하여 entity 표현을 통합하고 의미 모듈로 보낸다.
2. BERT와 비슷하게 maksed 언어모델을 적용하여 사전학습 목적함수르 다음 문장 예측을 사용한다. 텍스트와 지식 feature의 융합을 위해 named entity alignment의 일부를 임의로 masking하고 alignment를 완성하도록 모델을 학습시킨다. 오직 지역적 문맥만을 활용하는 기존의 목적함수와는 다르게 본 모델에서는 token과 entity를 모두 예측하도록 문맥과 지식 정보를 모두 사용한다.

실험은 지식기반 자연어처리 문제, entity typing과 관계분류(relation classification)에 대해 진행하였다. 결과는 ERNIE가 어휘적, 문법적, 의미정보를 최대한 사용하여 SOTA인 BERT를 압도하는 것을 보여주었다. 또한 다른 NLP 문제에도 실험하여 충분히 괜찮은 결과를 얻었다.

---

## 2. 관련 연구(Related Work)


단어를 분산표현(distributed representations)으로 변환하는 feature 기반 접근법이 2008~2018년까지 있었다. 텍스트 말뭉치를 갖고 사전학습 목적함수를 통해 학습하였으나 이전에는 단어의 다의성(polysemy)으로 인해 부정확했다. 2018년에는 문맥을 고려하여 이를 어느 정도 해결한 [ELMo](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/20/ELMo-Deep-contextualized-word-representations/)가 발표되었다.

사전학습에 더해 미세조정을 수행하는 방법으로 [BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/), [GPT2](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/28/OpenAI-GPT-2-Language-Models-are-Unsupervised-Multitask-Learners/) 등이 제안되어 다양한 자연어처리 문제에서 뛰어난 성과를 보였다.

이러한 feature 기반 및 미세조정 언어표현 모델은 상당한 성과를 보여주었으나 지식 정보를 활용하지 않는 단점이 있다. 외부 지식정보를 주입하는 방식으로 독해, 기계번역, 자연어추론 등에서 많은 논문들이 발표되었다. 그리고 여기서 우리는 외부 지식정보를 사용하는 것이 기존의 사전학습 모델에 도움이 될 것을 주장한다. 본 논문에서는 말뭉치와 KGs를 모두 활용, BERT에 기반하여 향상된 언어표현을 학습한다.


---

## 3. 방법론(Methodology)

3.1절은 표기법, 3.2절은 전체 구조, 3.4절은 사전학습 task, 3.5절은 미세조정 세부를 다룬다.

<center><img src="/public/img/2021-07-01-ERNIE2/02.png" width="100%" alt="ERNIE"></center>


### 3.1. Notations

$n$은 token sequence의 길이, $m$은 entity sequence의 길이라 할 때 token sequence와 entity sequence는 다음과 같이 나타낸다. 모든 token이 KGs의 entity와 align되는 것은 아니므로 $n \ne m$일 수 있다.

$$ \{w_1, ..., w_n\} , \{e_1, ..., e_m\} $$

모든 token을 포함하는 사전을 $\mathcal{V}$, KGs의 모든 entity를 포함하는 entity list를 $\mathcal{E}$라 한다. 어떤 token $w \in \mathcal{V}$가 해당하는 entity $e \in \mathcal{E}$를 가질 때, alignment는 $f(w) = e$로 정의한다. 그림 2에서 보듯이 entity는 named entity phrase에서 첫 번째 token으로 정한다.


### 3.2. Model Architecture

2개의 모듈을 쌓아 만들었다.

1. 아래쪽에 있는 textual encoder(`T-Encoder`)는 주어진 token에서 기본적인 어휘적, 문맥적 정보를 얻는다.
2. 위쪽의 knowledgeable encoder(`K-Encoder`)는 token에서 유래한 추가적인 지식 정보와 아래 layer의 textual 정보를 통합한다. 즉 token과 entity라는 다차원적 정보를 통합된 feature 공간에 표현할 수 있게 된다.

`T-Encoder` layer의 수를 $N$, `K-Encoder` layer의 수를 $M$이라 한다.

구체적으로, token sequence와 entity sequence가 주어지면, textual encoder는 먼저 각 token에 대해 token/segment/positional embedding을 합하고 어휘적, 문법적 feature를 계산한다.

$$ \{w_1, ..., w_n\} = \text{T-Encoder}(\{w_1, ..., w_n\}) $$

T-Encoder($\cdot$)은 multi-layer 양방향 Transformer encoder이다. T-Encoder($\cdot$)은은 BERT 구현체와 동일하다. 자세한 구조는 [BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/)와 [Transformer](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/) 참조.

feature를 계산하면 ERNIE는 knowledgeable encoder `K-Encoder`를 사용하여 언어표현에 지식정보를 주입한다. 구체적으로, 지식 임베딩 모델 TransE로 사전학습된 entity embedding을 표현하고, $w$와 $e$ 모두를 `K-Encoder`에 입력으로 주어 다차원적 정보를 융합하고 최종 embedding 출력을 뽑아낸다.

$$ \{w_1^o, ..., w_n^o\}, \{e_1^o, ..., e_n^o\} = \text{K-Encoder}(\{w_1, ..., w_n\}, \{e_1, ..., e_m\}) $$

$ \{w_1^o, ..., w_n^o\}, \{e_1^o, ..., e_n^o\} $는 특정 task를 위한 feature로 사용된다. `K-Encoder`의 세부적인 내용은 아래 절 참고.

### 3.3. Knowledgeable Encoder

`K-Encoder`는 aggregator를 쌓아 만든 구조로 token과 entity 모두를 인코딩하여 그 다차원적 feature를 융합하는 것을 목표로 한다. $i$-th aggregator에서, $(i-1)$-th aggregator에서 올라온 입력 token embedding $\{w_1^{(i-1)}, ..., w_n^{(i-1)}\}$와 entity embedding $\{e_1^{(i-1)}, ..., e_m^{(i-1)}\}$을 2개의 multi-head self-attentions([MH-ATTs]((https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/)))에 각각 넣는다.

$$ \{\tilde{w}_1^{(i)}, ..., \tilde{w}_n^{(i)}\} = \text{MH-ATT}(\{w_1^{(i-1)}, ..., w_n^{(i-1)}\}) $$

$$ \{\tilde{e}_1^{(i)}, ..., \tilde{e}_n^{(i)}\} = \text{MH-ATT}(\{e_1^{(i-1)}, ..., e_n^{(i-1)}\}) $$

그리고, $i$-th aggregator는 token과 entity sequence의 상호 통합을 위한  정보융합 layer를 사용, 각 token과 entity에 대한 출력 embedding을 계산한다. Token $w_j$와 상응하는 entity $e_k = f(w_j)$에 대해, 정보융합 과정은 아래와 같다.

$$ h_j = \sigma(\tilde{W}_t^{(i)}\tilde{w}_j^{(i)} + \tilde{W}_e^{(i)}\tilde{e}_k^{(i)} + \tilde{b}^{(i)}) $$

$$w_j^{(i)} = \sigma(W_t^{(i)}h_j + b_t^{(i)}) \qquad \qquad \qquad $$

$$ e_k^{(i)} = \sigma(W_e^{(i)}h_j + b_e^{(i)}) \qquad \qquad \qquad $$


$h_j$는 token과 entity의 정보를 통합하는 inner hidden state이다. $\sigma(\cdot)$은 비선형 활성함수로 GELU를 사용했다. 

상응하는 entity가 없는 token의 경우, 정보융합 layer는 통합 없이 embedding을 계산한다.

$$ h_j = \sigma(\tilde{W}_t^{(i)}\tilde{w}_j^{(i)} + \tilde{b}^{(i)}) $$

$$w_j^{(i)} = \sigma(W_t^{(i)}h_j + b_t^{(i)}) $$


단순히, $i$-th aggregator 연산은 아래와 같이 쓸 수 있다.


$$ \{w_1^{(i)}, ..., w_n^{(i)}\}, \{e_1^{(i)}, ..., e_m^{(i)}\} = \text{Aggregator}(\{w_1^{(i-1)}, ..., w_n^{(i-1)}\}, \{e_1^{(i-1)}, ..., e_m^{(i-1)}\}) $$


가장 위의 aggregator에서 나온 출력은 `K-Encoder`의 최종 output embedding으로 사용된다.



---

## 4. Experiments

ERNIE는 비교를 위해 Bert-base와 같은 모델 크기를 갖는다. 12 Encoder layers, 768 hidden units, 12 attention heads를 포함한다.

### 4.1. Heterogeneous Corpus Pre-training

중국 Wikipedia, Baidu Baike/News/Tieba 말뭉치를 사용하였으며 각 문장 수는 21M~54M이다. 각각 언어모델링의 기초가 되는 백과사전 글과 영화/배우 이름 등에 대한 정보, Reddit과 같은 토론, DLM의 것과 같은 내용을 포함한다. 한자는 번체에서 간체로, 영어는 소문자로 변환하고 17,964개의 공유 유니코드 문자를 포함한다.

### 4.2. DLM

Dialogue Language Model의 약자로 BERT의 token-position-segment embedding과 비슷한 구조를 가지지만, segment embedding이 2개의 문장을 뜻하는 0, 1이 아닌 multi-turn 대화를 나타내도록 되어 있다(Question-Response-Question 등. QRQ, QRR, ...)  
이는 ERNIE가 대화에서 암묵적인 관계를 학습할 수 있게 한다. 

<center><img src="/public/img/2021-06-14-ERNIE/04.png" width="100%" alt="Masking Level"></center>

### 4.3. Experiments on Chinese NLP Tasks

5가지 task에 대해 진행하였다.

1. 자연어추론: XNLI(Cross-lingual NLI) 말뭉치는 모순, 중립, 함의를 포함하며 중국어 포함 14개 언어 쌍이 있다.
2. Semantic Similarity: LCQMC 데이터셋 사용, 두 문장이 같은 내용(intention)을 포함하는지를 판별하는 task.
3. Name Entity Recognition: Microsoft Research Asia에서 배포한 MSRA-NER 데이터셋 사용. 사람/장소/조직 이름 등을 포함하는 entity를 갖고 있으며 sequence labeling task로도 볼 수 있다.
4. Sentiment Analysis: ChnSentiCorp 데이터셋 사용. 호텔, 책, 전자컴퓨터와 같은 여러 domain의 comment를 포함하며 어떤 문장의 긍정/부정을 평가한다.
5. Retrieval Question Answering: NLPCC-DBQA 데이터셋 사용. 질문에 맞는 답을 선택하는 것이다. 평가는 MRR과 F1 score 사용.

### 4.4. Experiment results

ERNIE가 모든 task에서 BERT를 능가한다. 중국어 NLP task에서는 SOTA를 찍고, BERT에 비해서 절대오차 1% 이상으로 우세하다. 이러한 이득은 지식 통합 전략에 따른 것이다.


<center><img src="/public/img/2021-06-14-ERNIE/05.png" width="85%" alt="Masking Level"></center>

### 4.5. Ablation Studies

Knowledge Masking 전략의 효과를 알아보기 위해 전체의 10% 학습 데이터를 뽑고, 각 level(word/phrase/entity)의 masking 전략 중 어느 것을 적용하는지에 따라 성능 차이가 좀 난다. phrase와 entity 수준에서 masking을 하는 것이 성능이 더 좋은 것을 볼 수 있다. 전체 데이터셋을 사용하면 10%일 때에 비해 0.8%의 향상이 있다.

<center><img src="/public/img/2021-06-14-ERNIE/06.png" width="100%" alt="Masking Level"></center>

DLM을 학습에 포함시키는 경우 그렇지 않은 경우에 비해 0.7%/1.0%의 향상이 있다.

<center><img src="/public/img/2021-06-14-ERNIE/07.png" width="100%" alt="Masking Level"></center>

### 4.6. Cloze Test

Name Entity를 단락에서 제거하고 모델이 이를 추론하는 Cloze test를 진행하였다. BERT는 문장에서 복사하려고 한 반면에 ERNIE는 기사에서 언급된 지식 관계를 기억하고 있다거나(Case 1), BERT는 entity 종류는 맞췄지만 제대로 채우는 데에는 실패했다거나(Case 2,5), 빈 칸을 여러 글자로 채웠지만 의미는 맞추지 못했다거나(Case 3, 4, 6) 하는 예시가 있다. 중국어라서 봐도 잘은 모르겠지만..(ERNIE는 Case 4에서 맞추지는 못했지만 semantic type은 맞췄다고 한다.)


<center><img src="/public/img/2021-06-14-ERNIE/08.png" width="100%" alt="Masking Level"></center>



---

## 결론(Conclusion)

지식 통합 전략으로 지식을 사전학습 언어모델에 넣는 새로운 방법을 제시했다. 5가지의 중국어 NLP task에서 BERT보다 우수한 성능을 보였으며 지식통합과 사전학습이 언어표현 학습에 모두 도움이 되는 것을 확인하였다.  
이후에는 다른 종류의 지식을 의미적 표현모델에 넣는 방법(문법적 parsing, 약한 지도학습 등)과 다른 언어에 적용할 방법을 연구한다고 한다.

---

## 참고문헌(References)

논문 참조!

--- 

