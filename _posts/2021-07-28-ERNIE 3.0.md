---
layout: post
title: ERNIE 논문 설명(ERNIE 3.0 - Large-Scale Knowledge Enhanced Pre-Training For Language Understanding And Generation)
author: YouWon
categories: [NLP(Natural Language Processing) / RNNs]
tags: [Paper_Review, NLP, ERNIE]
---

---

이 글에서는 Baidu에서 만든 모델 시리즈 ERNIE 중 다섯 번째(ERNIE 3.0: Large-Scale Knowledge Enhanced Pre-Training For Language Understanding And Generation)를 살펴보고자 한다.

ERNIE 시리즈는 다음과 같다.

- [ERNIE: Enhanced Representation through Knowledge Integration](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2021/06/14/ERNIE/), Yu sun el al., 2019년 4월
- [ERNIE: Enhanced Language Representation with Informative Entities](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2021/07/01/ERNIE2/), Zhengyan Zhang et al., 2019년 5월
- [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2021/07/05/ERNIE-2.0/), Yu Sun et al., 2019년 6월
- [ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2021/07/19/ERNIE-ViL/), Fei Yu et al., 2019년 6월
- **[ERNIE 3.0: Large-Scale Knowledge Enhanced Pre-Training For Language Understanding And Generation](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2021/07/28/ERNIE-3.0/), Yu Sun et al., 2021년 7월**

중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.

---

# ERNIE 3.0: Large-Scale Knowledge Enhanced Pre-Training For Language Understanding And Generation

논문 링크: **[ERNIE 3.0: Large-Scale Knowledge Enhanced Pre-Training For Language Understanding And Generation](https://arxiv.org/abs/2107.02137)**

## 초록(Abstract)


<center><img src="/public/img/2021-06-14-ERNIE/01.png" width="50%" alt="ERNIE"></center>

---

## 1. 서론(Introduction)

[BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/), [GPT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/28/OpenAI-GPT-2-Language-Models-are-Unsupervised-Multitask-Learners/)와 같은 사전학습 모델들은 많은 자연어처리 문제에서 성능을 크게 향상시켰으며 [VQA](https://greeksharifa.github.io/computer%20vision/2019/04/17/Visual-Question-Answering/), VCR과 같은 시각-언어 task를 위한 사전학습의 중요성이 알려졌다.



[ELMo](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/20/ELMo-Deep-contextualized-word-representations/)

[OpenAI GPT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/28/OpenAI-GPT-2-Language-Models-are-Unsupervised-Multitask-Learners/)

[BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/)

[ERNIE 1.0](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2021/07/01/ERNIE2/)




---

## 2. 관련 연구(Related Work)

### 2.1 Large-scale Pre-trained Models



### 2.2 Knowledge Enhanced Models


---

## 3. ERNIE 3.0




### 3.1 Overview of ERNIE 3.0 Framework




#### 3.1.1 Universal Representation Module



#### 3.1.2 Task-specific Representation Module




### 3.2 Pre-training Tasks




#### 3.2.1 Word-aware Pre-training Tasks



#### 3.2.2 Structure-aware Pre-training Tasks




#### 3.2.3 Knowledge-aware Pre-training Tasks



### 3.3 Pre-training Process

#### 3.3.1 Pre-training Algorithm



#### 3.3.2 Pre-training Data



#### 3.3.3 Pre-training Settings





---

## 4. 실험(Experiments)

### 4.1 Evaluation Tasks



#### 4.1.1 Natural Language Understanding Tasks




#### 4.1.2 Natural Language Generation Tasks





### 4.2 Experiments on Fine-tuning Tasks

#### 4.2.1 Fine-tuning on Natural Language Understanding Tasks



#### 4.2.2 Fine-tuning on Natural Language Generation Tasks



#### 4.2.3 LUGE benchmark



### 4.3 Experiments on Zero-shot Learning



#### 4.3.1 Evaluation



#### 4.3.2 Results



#### 4.3.3 Case Study



### 4.4 Experiments on SuperGLUE





---

## 5. 분석(Analysis)


### The Effectiveness of the Task-specific Representation Modules



### Universal Knowledge-Text Prediction



### Progressive Learning to Speed up Convergence


---

## 6. 결론(Conclusion)


---

## 참고문헌(References)

논문 참조!

--- 

