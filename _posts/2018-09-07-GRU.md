---
layout: post
title: GRU
author: Youyoung
categories: Paper_Review
tags: [Sequence_Modeling, RNN, Paper_Review]
---

### Empirical Evaluation of Gated Recurrent NN on Sequence Modeling  
> 본 글은 Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio가 2014년에 publish한 Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling을 리뷰한 것이다.  


**Background**  
RNN은 전통적인 feedforward NN의 확장판으로 이해할 수 있다. 가장 큰 특징 중 하나는 이 네크워크는 variable-length sequence input을 처리할 수 있다는 것이다.(p1)  
본 논문은 LSTM과 GRU의 비교를 main purpose로 두고 있다.  
그 이유는 LSTM이 효과적이기는 하나 상대적으로 복잡한 구조를 갖고 있기도 하고,  
제안된 GRU가 구조가 조금 더 단순함에도 불구하고 일부 경우에서 더 나은 성능을 보였기 때문이다.  

**Long Short-Term Memory Unit**  
> LSTM Unit의 구조는 아래와 같다.  
  
<center><img src="/public/img/Paper_Review/2018-09-07-GRU/G1.PNG" width="50%"></center>  
  
시간 t에 대하여 j번 째 LSTM Unit은 아래와 같은 구조를 가진다.  
  
Candidate $ \tilde{c} $에 대하여
$$ \tilde{c}_t^j = tanh(W_c * x_t + U_c * h_{t-1})^j $$
  
Memory Cell $c$에 대하여  
$$ c_t^j = f_t^j * c_{t-1}^j + i_{t}^j * \tilde{c}_t^j $$  
  
Output Gate $o$에 대하여  
$$ {o}_t^j = \sigma(W_o * x_t + U_o * h_{t-1} + V_o * c_t)^j $$
  
Output $h$에 대하여  
$$ h_t^j = o_t^j * tanh(c_t^j) $$

LSTM 구조의 핵심은 총 3개의 Gate(Input Gate, Forget Gate, Output Gate)를 통해 저장될 정보를 적절히 컨트롤한다는 것이다.  
**Forget Gat**의 경우 $ c_{t-1}^j $ (과거 정보)를 버릴 것인지 말 것인지를 결정하고,  
**Input Gate**의 경우 시간 t의 candidate로 업데이트를 할 것인지 말 것인지를 결정하게 된다.  
최종적으로 **Output Gate** $ o_{t}^j $는 hyperbolic tangent activation function을 통과한 $ c_t^j $와 곱해져 **Output** $ h_t^j $를 형성하게 된다.  

**Gated Recurrent Unit**
> GRU Unit의 구조는 아래와 같다.  
  
<center><img src="/public/img/Paper_Review/2018-09-07-GRU/G2.PNG" width="50%"></center> 
  
시간 t에 대하여 j번 째 GRU Unit은 아래와 같은 구조를 가진다.  
  
Reset Gate $r$에 대하여  
$$ {r}_t^j = \sigma(W_r * x_t + U_r * h_{t-1})^j $$

Candidate $ \tilde{h} $에 대하여
$$ \tilde{h}_t^j = tanh(W * x_t + U * (r_t \odot h_{t-1})^j $$
**Reset Gate**는 과거의 Activation value ($h_{t-1}$)이 새로운 후보 $\tilde{h}_{t}$에 영향을 줄 것인지 말 것인지를 결정하는 관문의 역할을 한다. 만약 위의 Reset Gate이 0에 가까울 경우, 바로 위의 식의 오른쪽 부분이 0이 됨으로써, 
자연스럽게 과거의 값 ($h_{t-1}$)을 잊게 해준다. (Forget) 참고로 식의 오른쪽 편의 $\odot$은 Elementwise Multiplication을 뜻한다.  
자 이제 **Candidate**이 준비되었다.  

Memory Cell $c$에 대하여  
$$ c_t^j = f_t^j * c_{t-1}^j + i_{t}^j * \tilde{c}_t^j $$  
  
Update Gate $z$에 대하여  
$$ {z}_t^j = \sigma(W_z * x_t + U_z * h_{t-1})^j $$
  
이 **Update Gate**는 얼마나 새로운 값으로 업데이트하고 싶은지를 결정하게 되는데, 이 말은 아래 식에서 이 **Update Gate**의 값이 0에 가까울 경우 (sigmoid를 통과하였으므로) **Candidate**의 영향력은 0가 되는 것이고, 이전 Activation $h_{t-1}$이 그대로 살아남는 것을 의미한다.

Output $h$에 대하여  
$$ h_t^j = (1- z_t^j) * h_{t-1}^j + z_t^j * \tilde{h}_t)^j $$





**Discussion**  
LSTM과 GRU가 기존의 traditional한 구조와 가장 차별화되는 포인트는 아래와 같다.  
1) 각각의 Unit은 여러 단계에 걸친 input stream에 있어 구체적인 feature의 존재를 지속적으로 기억하는 데에 있어 좋은 성능을 발휘한다.  
즉, 긴 길이의 Sequence를 Input으로 받는다 하더라도 초기의 정보에 대해 큰 소실 없이 저장이 가능하다는 뜻이다.  
  
2) 일종의 Shortcut path를 만들어 오차가 vanish하지 않고 적절하게 역전파될 수 있도록 한다.  

**Experiments and Results**
> Sequence Modeling은 여러 sequences에 대한 확률 분포를 학습하는 것을 목적으로 한다.  
  
즉, training sequences에 대하여 아래와 같은 model의 log-likelihood를 최대화하는 것을 목적으로 한다.  

$$ \underset{x}{\mathrm{max}} \frac{1}{N} \sum_{n=1}^{N} \sum_{n=1}^{T_n} logp(x_t^n | x_1^n, ..., x_(t-1)^n; \theta ) $$
  
여기서 $\theta$는 model parameters를 뜻한다.  
  
결과만을 요약하자면, GRU와 LSTM이 traditional tanh-RNN를 능가하는 것은 명확하게 확인되었지만, GRU와 LSTM의 비교우위를 판별하기 위해서는 추가적인 연구가 필요하다고 확인되었다.  
(비록 GRU과 하나의 데이터셋을 제외하고는 근소하게 LSTM를 능가했지만)  

*데이터셋은 polyphonic music data와 raw speech signal data를 사용하였다.  






