---
layout: post
title: AGQA - A Benchmark for Compositional Spatio-Temporal Reasoning 설명
author: Youyoung
categories: [Paper_Review]
tags: [Machine_Learning, Paper_Review]
---

이번 글에서는 `AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning` 논문을 정리한다.

- 2021년 3월(Arxiv), CVPR 2021
- Madeleine Grunde-McLaughlin, Ranjay Krishna, Maneesh Agrawala
- [홈페이지](https://cs.stanford.edu/people/ranjaykrishna/agqa/)
- [논문 링크](https://arxiv.org/abs/2103.16002)  
- [Github](https://github.com/madeleinegrunde/AGQA_baselines_code)  

---

## Abstract

- Visual event는 object들과 공간적으로(spatially) 상호작용하는 actor들을 포함하는 temporal action들로 구성된다. 
- 이를 위해 여러 벤치마크 및 모델들이 개발되었지만 단점들이 많았다.
- 본 논문에서는 compositional spatio-temporal reasoning을 위한, 3배 이상 대규모에 balance까지 맞춘 AGQA benchmark를 제안한다.
- 기존 모델은 47.74%의 정확도밖에 달성하지 못한다. 이 AGQA benchmark는 더 다양하고 더 많은 단계를 거치는 추론 능력 등을 좀 더 광범위하게 테스트할 수 있다.

---

## 1. Introduction

<center><img src="/public/img/2023-03-03-AGQA1.0/fig01.png" width="60%"></center>

(기계와 달리) "사람"은 시각적인 이벤트를 표현할 때 'actor가 시간에 따라 주변 물체들과 어떻게 상호작용하는지'를 본다.  

위의 비디오를 예로 들면, 

- 핸드폰을 (바닥에) 내려놓고(`putting a phone down`)
- 병을 잡는다(`holding a bottle`)
    - 이 행동은 다시 병을 돌려 열고(`twisting the bottle`)
    - 왼쪽으로 옮기는 행동으로 나눌 수 있다(`shift to holding it`)

이러한 정보들은 Action Genome으로부터 유래한 Scene Graph에서 얻을 수 있으며, AGQA에서는 이 정보들을 갖고 대답할 수 있는, 여러 단계의 추론을 거쳐야 대답할 수 있는 질문-답변을 포함한다.

AGQA의 질문들은 사람이 직접 "program"을 만들어서 정해진 규칙에 따라 생성되도록 설계되었다.

- Charades dataset의 action annotation과
- Action Genome의 spatio-temporal scene graph를 가지고 만들어졌다.

기존 모델로는 PSAC, HME, HRCN을 가지고 실험하였으며

- 학습 떄 보지 못한 새로운 composition을 잘 일반화할 수 있는지
- object를 간접적으로 표현(ex. `object ther were holding last`)함으로써 간접적인 추론을 할 수 있는지
- 학습 때보다 더 많은 단계의 추론이 필요한 질문에 대답을 잘 하는지

를 테스트하였다.

일반적으로 추론 단계가 많아질수록 성능이 떨어졌다.

---

## 2. Related Work

질문을 생성하기 위해 spatio-temporal scene graphs를 사용하였으며, 새로운 평가 척도를 제안한다.

**Image question answering benchmarks**: 많이 있지만 어느 것도 외부 지식을 요하는 상식추론 그 이상의 temporal 추론을 다루지 않았다.

**Video question answering benchmarks**: 이것도 많지만 기존의 데이터셋에 비해 AGQA는 오직 vision에 입각한, 더 대규모의, 더 복잡한 다단계 추론을 평가한다.

**Scene graphs**: AGQA는 GQA의 pipeline의 일반화 버전이다. spatial relation에 더새 temporal localization까지 활용하였으며 질문을 program에 따라 자동 생성하기 위해 Action Genome의 spatio-temporal scene graph를 사용하였다.

**Compositional reasoning**: 실세계 영상에서의 복잡한 추론을 다루는 벤치마크가 필요했다. (그게 이 논문이다라는 주장)


<center><img src="/public/img/2023-03-03-AGQA1.0/fig02.png" width="60%"></center>


---

## 3. The AGQA benchmark

1. Action Genome의 spatio-temporal scene graphs와 Charades의 action localizations를 symbolic video representation으로 통합하였다.
2. 이를 바탕으로 하여 질문을 자동 생성할 수 있는 규칙(program)을 수동으로(handcraft) 만들었다.
3. 만들어진 데이터에 대해 bias를 제거하고 balance를 맞췄다.
4. 모델이 얼마나 새로운 composition, 간접추론, 더 많은 단계의 추론을 잘 수행하는지 테스트할 수 있는 평가 metric을 만들었다.

<center><img src="/public/img/2023-03-03-AGQA1.0/fig03.png" width="100%"></center>


### 3.1. Augmenting spatio-temporal scene graphs

`carrying a blanket and twisting the blanket`와 같은 문장의 `twisting`을 `holding`이나 `touching` 등으로 바꾸어 augment를 할 수 있다.  
하지만 별 의미없는 문장들, 예를 들면 
- `Were they touching the blanket while carrying the blanket?`와 같이 답을 바로 알 수 있거나
- 이전 행동이 끝나기 전에 다음 행동이 시작해버리는 경우

등은 co-occurrence 등을 체크하여 생성하지 않도록 했다.

최종적으로 36 objects, 44 relationships, and 157 actions, 7787 training / 1814 test set scene graphs를 확보하였다.


### 3.2. Question templates

`“What did they <relationship><time><action>?`와 같은 program을 수동으로 만들어, `What did they tidy after snuggling with a blanket?`나  `What did they carry before putting something on a table?`와 같은 질문을 자동으로 생성할 수 있도록 했다.  
이 질문에 대답하기 위해서는 `put something on a table`에 해당하는 action을, 그 action 이전에 일어난 event를, `carry` 관계가 일어나고 있는 장소를 찾고 그 object이 무엇인지를 찾아야 한다.

결과적으로 269개의 natural language question frames(that can be answered from a set of 28 programs)와, 이 program을 이용하여 192M개의  question-answer 쌍을 만들었다. 이는 45M개의 unique questions, 174개의 unique answers을 포함한다.

### 3.3. Balancing to minimize bias

program대로 만들 수 있는 질문들을 전부 생성해보면 특정 형태의 질문들이 거의 대부분을 차지하게 된다. 이렇게 생기는 bias를 줄이기 위해 질문의 특성에 따라 비율을 강제로 맞춰줌으로써 전체 데이터셋의 balance를 맞추고자 했다.

Question reasoning type, Question Semantics, Question structures 등에 따라 비율을 맞췄고, 각 type에 따른 비율은 아래 그림에 정리되어 있다.

<center><img src="/public/img/2023-03-03-AGQA1.0/fig04.png" width="100%"></center>


### 3.4. New compositional spatio-temporal splits

평가를 위해 새로운 split을 몇 개 제안한다.

- **Novel compositions**를 평가하기 위해 training set에는 존재하지 않고  test set에만 나타나는 concept pair를 따로 선택하였다.
- **Indirect references**을 위해서는 object나 action 등이 간접적인 방식으로 표현되었을 때(ex. `blanket -> the object they threw`) 모델이 잘 대답하는지를 측정하는 metric이 있다.
- **More compositional steps**: $M$개 이하의 추론 단계를 포함하는 질문과 $M$ 이상의 단계를 포함하는 질문을 나누어 모델이 각각에 대해 얼마나 잘 대답하는지를 측정한다.

---

## 4. Experiments and analysis



### 4.1. Human evaluation



### 4.2. Performance across reasoning abilities



### 4.3. Performance across question semantics



### 4.4. Performance across question structures



### 4.5. Generalization to novel compositions



### 4.6. Generalization to indirect references



### 4.7. Generalization to more compositional steps




---

### 5. Discussion and future work



---

### 6. Supplementary
