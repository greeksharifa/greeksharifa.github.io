---
layout: post
title: AGQA - A Benchmark for Compositional Spatio-Temporal Reasoning 설명
author: Youyoung
categories: [Paper_Review]
tags: [Machine_Learning, Paper_Review]
---

이번 글에서는 `AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning` 논문을 정리한다.

- 2021년 3월(Arxiv), CVPR 2021
- Madeleine Grunde-McLaughlin, Ranjay Krishna, Maneesh Agrawala
- [홈페이지](https://cs.stanford.edu/people/ranjaykrishna/agqa/)
- [논문 링크](https://arxiv.org/abs/2103.16002)  
- [Github](https://github.com/madeleinegrunde/AGQA_baselines_code)  

---

## Abstract

- Visual event는 object들과 공간적으로(spatially) 상호작용하는 actor들을 포함하는 temporal action들로 구성된다. 
- 이를 위해 여러 벤치마크 및 모델들이 개발되었지만 단점들이 많았다.
- 본 논문에서는 compositional spatio-temporal reasoning을 위한, 3배 이상 대규모에 balance까지 맞춘 AGQA benchmark를 제안한다.
- 기존 모델은 47.74%의 정확도밖에 달성하지 못한다. 이 AGQA benchmark는 더 다양하고 더 많은 단계를 거치는 추론 능력 등을 좀 더 광범위하게 테스트할 수 있다.

---

## 1. Introduction

<center><img src="/public/img/2023-03-03-AGQA1.0/fig01.png" width="60%"></center>

(기계와 달리) "사람"은 시각적인 이벤트를 표현할 때 'actor가 시간에 따라 주변 물체들과 어떻게 상호작용하는지'를 본다.  

위의 비디오를 예로 들면, 

- 핸드폰을 (바닥에) 내려놓고(`putting a phone down`)
- 병을 잡는다(`holding a bottle`)
    - 이 행동은 다시 병을 돌려 열고(`twisting the bottle`)
    - 왼쪽으로 옮기는 행동으로 나눌 수 있다(`shift to holding it`)

이러한 정보들은 Action Genome으로부터 유래한 Scene Graph에서 얻을 수 있으며, AGQA에서는 이 정보들을 갖고 대답할 수 있는, 여러 단계의 추론을 거쳐야 대답할 수 있는 질문-답변을 포함한다.

AGQA의 질문들은 사람이 직접 "program"을 만들어서 정해진 규칙에 따라 생성되도록 설계되었다.

- Charades dataset의 action annotation과
- Action Genome의 spatio-temporal scene graph를 가지고 만들어졌다.

기존 모델로는 PSAC, HME, HRCN을 가지고 실험하였으며

- 학습 떄 보지 못한 새로운 composition을 잘 일반화할 수 있는지
- object를 간접적으로 표현(ex. `object ther were holding last`)함으로써 간접적인 추론을 할 수 있는지
- 학습 때보다 더 많은 단계의 추론이 필요한 질문에 대답을 잘 하는지

를 테스트하였다.

일반적으로 추론 단계가 많아질수록 성능이 떨어졌다.

---

## 2. Related Work

