---
layout: post
title: RC task using CNN/Daily dataset
author: Youyoung
categories: Paper_Review
tags: [NLP, Paper_Review]
---

### A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task  
> 본 글은 Danqi Chen, Jason Bolton, Christopher D. Manning가 2016년에 Publish한 위 논문을 리뷰한 것이다.  
 
### Introduction  
본 논문은 CNN/Daily Mail 데이터셋의 심도 있는 분석을 목적으로 하며, 어떠한 수준의 자연어가 요구되는지를 파악하는 것을 목적으로 한다.  

### RC Task and systems  
데이터셋은 아래와 같은 3가지의 구성 요소를 지닌다.  
  
Element         | Description
---------       | ---------
passage         | new article
question        | close_style task
answer          | question entity

한 passage의 단어들을 d차원으로 임베딩하여 m개의 token을 형성하였다고 할 때,  
  
$$ p = {p_1, p_2, ..., p_m}, q = {q_1, ..., q_l} $$  
  
q의 경우 오직 하나의 "placeholder" token을 가진다.  
목적은 결국 p와 E(모든 abstract entity markers의 집합)의 교집합에 속하는 a를 찾는 것인데,  
이는 answer(답)가 named entity 리스트에서 선택된다는 것을 의미한다.  

본 논문의 모델은 Hermann의 Attentive Model을 기반으로 하며 일부 수정을 기하였다.  
대략적인 구조는 아래 그림과 같다.  
  
<center><img src="/public/img/Paper_Review/2018-10-29-Self/01.jpg" width="100%"></center>  
  
d차원으로 임베딩 된 $ p_i $, $ q_j $ 벡터들은 (i = 1~m, j = 1~l) 각각 Bidirectional RNN에 인풋으로 투입된다. hidden state들을 수직으로 concat한 것을 $ \tilde{p_i} $라고 한며 이 벡터의 차원은 h이다. q역시 마찬가지로 h차원이다.  
  
기본 구조는 GRU를 사용하였다. (계산 심플)  
  
Attention의 경우 아래와 같이 계산된다.  
  
<center><img src="/public/img/Paper_Review/2018-10-29-Self/02.jpg" width="50%"></center>  
  
attention weight인 $ \alpha_i $ 는 스칼라이며, contextual embedding vector인 $ \tilde{p_i} $ 과 q의 관련 정도를 의미한다. 이 값이 클 수록 질문과 paragraph의 관련성이 강하다는 것을 의미한다.  

**output vector**인 **o**는 (n, 1) 벡터이며 모든 contextual embedding vectors의 가중합으로 생각하면 된다.  

이를 통해 최종적으로 아래와 같이 답을 계산한다. 손실함수는 NLL을 사용한다.  

<center><img src="/public/img/Paper_Review/2018-10-29-Self/03.jpg" width="50%"></center>  
  
Hermann의 모델과의 차이점은 아래와 같다.  
1. Bilinear Term을 사용하여 q와 p사이의 상호작용을 명확히 했다.  
2. output vector를 직접적으로 예측에 사용하였다.  
3. 모든 단어를 후보로 두지 않고 entities 중 passage에 나타나는 (p와 E의 교집합) 단어들만 후보로 두었다.  

### Training  
빈번하게 등장하는 5만개의 단어만을 사용하였고, 임베딩 차원이 d=100인 GloVe를 사용하였다.  
attention과 output 모수들은 균일분포로 초기화되었고, GRU weights은 정규분포로 초기화 되었다.  

hidden size는 128과 256을 사용하였고, SGD 알고리즘을 32 배치사이즈로서 사용하였다.  
0.2의 dropout을 임베딩 layer에 사용하였고, gradient의 norm이 10을 넘을 경우 gradient clipping을 사용하였다.  

