---
layout: post
title: ELMo - Deep contextualized word representations
author: YouWon
categories: [NLP(Natural Language Processing) / RNNs]
tags: [Paper_Review, NLP]
---

이 글에서는 2018년 *Matthew E. Peters* 등 연구자가 발표한 Deep contextualized word representations를 살펴보도록 한다.

참고로 이 논문의 제목에는 ELMo라는 이름이 들어가 있지 않은데, 이 논문에서 제안하는 모델의 이름이 ELMo이다.  
Section 3에서 나오는 이 모델은 **E**mbeddings from **L**anguage **Mo**dels이다.

중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.

---

# ELMo - Deep contextualized word representations

논문 링크: **[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)**

## 초록(Abstract)

이 논문에서는 새로운 종류의 Deep contextualized word representation을 소개한다.

---

## 1. 서론(Introduction)



---

## 2. 관련 연구(Related work)



---

## 3. ELMo: Embeddings from Language Models


<center><img src="/public/img/2019-08-20-ELMo - Deep contextualized word representations/01.png" width="100%" alt="Transformer Architecture"></center>

### 3.1. Bidirectional language models


### 3.2. ELMo


### 3.3. Using biLMs for supervised NLP tasks



### 3.4. Pre-trained bidirectional language model architecture


---

## 4. 평가(Evaluation)


### Question Answering
### Textual entailment
### Semantic rol;e labeling
### Coreference resolution
### Named entity extraction
### Sentiment analysis


---

## 5. 분석(Analysis)

### 5.1. Alternate layer weighting schemes
### 5.2. Where to include ELMo?
### 5.3. What information is captured by the biLM's representations?
#### Word sense disambiguation
#### POS tagging
#### Implications for supervised tasks
### 5.4. Sample efficiency

### 5.5. Visualization of learned weights


---

## 6. 결론(Conclusion)




---

## Refenrences

논문 참조. 61개의 레퍼런스가 있다.

---

