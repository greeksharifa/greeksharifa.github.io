---
layout: post
title: BART 논문 설명(BART - Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension)
author: YouWon
categories: [NLP(Natural Language Processing) / RNNs]
tags: [Transformer, Facebook, Seq2Seq]
---

---


이 글에서는 Facebook AI에서 발표한 BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension 논문을 간략하게 정리한다.

---

# BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

논문 링크: **[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)**

- 2019년 10월(Arxiv)
- Mike Lewis, Yinhan Liu, Naman Goyal et al.

 
---

## Abstract

seq-to-seq 모델을 사전학습시키기 위한 denoising autoencoder인 BART를 제안한다. BART의 학습은

1. 임의의 noising function으로 텍스트를 변형시킨 것과
2. 이를 원래대로 복원하도록 모델을 학습시키는 것으로 이루어진다.

BART는 [Transformer](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/) 기반 표준 NMT 구조를 가지며 [BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/)와 [GPT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/28/OpenAI-GPT-2-Language-Models-are-Unsupervised-Multitask-Learners/) 및 많은 사전학습 schemes를 일반화한 모델이라 할 수 있다.  
본 논문에서는 많은 noising 기법들을 평가하여 문장의 순서를 임의로 섞는 것과 in-filling scheme(spans of text가 하나의 mask token으로 치환됨)을 사용할 때 가장 성능이 좋음을 발견하였다. BART는 특히 텍스트 생성에 대해 fine-tuned되었을 때 효율적이지만 이해력 테스트에서도 잘 작동한다. GLUE와 SQuAD에서 RoBERTa 이상의 성능을,  6 ROUGE score을 더 높게 얻고 abstractive dialogue, question answering, and summarization tasks에서 SOTA를 달성하는 등 성과가 좋다.  

또한 ablation 실험 등을 진행하여 모델의 성능 등을 입증한다.

---

## 1. Introduction

Self-supervised 접근법은 NLP task에서 괄목할 만한 성과를 많이 내었다. 가장 성공적인 방법은 MLM 및 그 변형들이었다. 그러나 이러한 방법들은 특정 task에만 집중하여 일반화하기 어려웠다.

본 노문에서는 아주 넓은 범위에 사용할 수 있는, seq-to-seq 모델로 만든 denoising autoencoder인 **Bidirectional and Auto-Regressive Transformers, BART**를 제안한다. 초록에서 말한 것처럼 BERT, GPT 등의 학습 방법을 일반화한 방식을 사용했음을 아래 그림에서 볼 수 있다.

<center><img src="/public/img/2022-08-09-BART/fig01.png" width="100%"></center>

이 방식의 이점은 noising할 떄 별 제약없이 다양한 function을 사용할 수 있다는 것이다. 

(초록과 같은 내용이라 일부 생략)

BART는 fine-tuning에 대해 새로운 방식을 생각할 수 있게 한다. Machine Translation에 대해 새로운 scheme을 제안하는데 BART는 몇 개의 추가적인 transformer layer 위에 놓인다. 이 layer들은 특히 외국어를 noised 영어로 번역하도록 학습되고 이는 BART에 전해지고, 따라서 BART를 pre-trained target-side language model로 사용하게 된다. 이 접근법은 WMT Romanian-English benchmark에서 강력한 back-translation MT baseline을 1.1 BLEU score만큼 앞지른다. 


---

## 2. Model

BART는 임의로 변형된 문서를 원래대로 되돌리는 denoising autoencoder이다. 구현은 corrupted text에 대한 bidirectional encoder와 left-to-right autoregresisve decoder로 구성된 seq-to-seq 모델로 이루어졌다. 사전학습을 위해서는 원본 문서에 대한 NLL loss를 사용하였다.

### 2.1 Architecture

표준 seq-to-seq [Transformer](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/)을 기반으로 하되 [OpenAI GPT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/28/OpenAI-GPT-2-Language-Models-are-Unsupervised-Multitask-Learners/)처럼 ReLU를 GeLU($\mathcal{N}(0, 0.02)$)로 바꿔 사용하였다. base 모델은 6 layer encoder, large 모델은 12개를 사용하였다.

구조는 BERT와 비슷하지만, 차이점은

1. decoder의 각 layer는 encoder의 final hidden layer와 cross-attention을 수행한다.
2. BERT는 word-prediction을 위해 추가적인 feedforward net을 사용하지만 BART는 그렇지 않다.

전체적으로 BART는 BERT보다 10% 정도 더 많은 pamameter를 갖는다.


### 2.2 Pre-training BART

Corrupted document를 원복하는 방식으로 사전학습을 진행하는데 reconstruction loss는 decoder의 출력과 원본 문서 간 cross-entropy를 쓴다.

사전학습에 사용한 방법은 다음 5가지이다.

<center><img src="/public/img/2022-08-09-BART/fig02.png" width="100%"></center>

1. **Token Masking:** [BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/)의 것과 같다.
2. **Token Deletion:** token을 `[MASK]` token으로 바꾸는 대신 아예 없애버리는 것으로 모델은 사라진 위치를 찾아야 한다.
3. **Text Infilling:** 그 길이가 $\lambda=3$ poisson 분포를 따르는  여러 개의 text spans를 뽑는다. 각 span은 하나의 `[MASK]`으로 대체된다. SpanBERT에서 제안된 방식이지만, 다른 점은 SpanBERT에서는 다른 분포에서 샘플링을 하며 정확히 같은 길이의 `[MASK]` token으로 대체한다. Text infilling은 모델이 span에서 얼마나 많은 token이 사라졌는지 예측해야 한다.
4. **Sentence Permutation:** 문서를 여러 부분으로 나누어 임의로 섞는다. 모델은 원래 순서를 맞춰야 한다.
5. **Document Rotation:** 특정 지점을 잘라서 문서를 그 지점부터 시작하도록 변형한다. 모델은 원래 시작점을 찾아야 한다.


---

## 3. Fine-tuning BART

BART가 생성한 representation은 downstream applications에서 여러 방식으로 사용될 수 있다.

### 3.1 Sequence Classification Tasks

같은 입력이 encoder와 decoder 모두에 주어지고, final decoder token의 final hidden state는 새로운 multi-class linear classifier에 입력으로 주어진다. 이는 BERT의 CLS token과 연관되어 있지만 BART에서는 이 추가적인 token을 *끝*에 추가하여 decoder에서 token의 representation이 decoder states를 처리할 수 있게 했다. (그림 3.a)

<center><img src="/public/img/2022-08-09-BART/fig03.png" width="100%"></center>

### 3.2 Token Classification Tasks

SQuAD의 answer endpoint classification와 같이 전체 문서를 encoder와 decoder에 주고 decoder의 top hidden state를 각 단어의 representation으로 사용하였다. 이 representation은 token을 분류하는 데 사용된다.

### 3.3 Sequence Generation Tasks

BART는 autoregressive decoder를 갖고 있으므로 abstractive question answering나 summarization와 같은 생성 task에 바로 적용할 수 있다.  둘 모두 정보를 입력에서 변형된 상태로 복사되며 이는 denoising pre-training objective와 긴밀한 연관이 있다. 여기서 encoder 입력은 input sequence, decoder는 출력을 autoregressive하게 생성한다.

### 3.4 Machine Translation

BART 모델 전체를 하나의 encoder처럼 생각해서 MT에도 적용할 수 있게 하였다. (그림 3.b)

정확히는, BART의 encoder embedding layer를 랜덤초기화된 새로운 encoder로 교체하였다. 모델은 end-to-end로 학습되며 새로운 encoder가 외국어 단어를 BART아 de-noise할 수 있는 영어 입력으로 mapping하도록 학습된다. 새로운 encoder는 원래 BART 모델와 다른 vocab을 쓸 수 있다.

source encoder는 2단계로 학습하는데 둘 모두에서 BART 모델의 출력의 cross-entropy loss를 backpropagate한다.

1. BART를 freeze하고 새로운 encoder, BART positional embeddings, BART encoder의 첫 layer의 self-attention input projection matrix만 update한다.
2. 이후 반복횟수를 조금만 하여 전체를 update한다.


---

## 4. Comparing Pre-training Objectives

목적함수 비교, 데이터셋 및 Task 설명이다.

### 4.1 Comparison Objectives

여러 pre-training objectives를 최대한 동일하고 공정한 환경에 놓고 비교를 진행했다고 한다.

비교대상: Language Model, Permuted Language Model, Masked Language Model, Multitask Masked Language Model, Masked Seq-to-Seq

### 4.2 Tasks

- **SQuAD:** Wikipedia paragraphs을 사용하는 extractive question answering task이다. 정답은 주어진 document context에서 추출된 text spans이다.
- **MNLI:** 한 문장이 다른 문장을 수반하는지 아닌지를 판단하는 bitext classifitation task
- **ELI5:** long-form abstractive question answering dataset
- **XSum:** 매우 추상적인 요약문을 포함하는 news summarization dataset
- **ConvAI2:** context와 persona 조건을 갖는 dialogue response generation task
- **CNN/DM:** news summarization dataset로 요약은 보통 source sentence와 깊은 연관이 있다.

### 4.3 Results


<center><img src="/public/img/2022-08-09-BART/tab01.png" width="100%"></center>

task에 따라 편차가 있지만, 전체적으로 봤을 때 BART + text infilling(혹은 여기에 Sentence shuffling까지) 방식이 좋다는 것을 확인하였다.


---

## 5. Large-scale Pre-training Experiments

최근 연구들을 보면 모델 크기가 클수록 성능이 좋아진다. BART도 비교 실험을 진행하였고, RoBERTa와 같은 크기로 맞추어 실험하였다.

### 5.1 Experimental Setup

- 12 layer encoder/decoder
- hidden size 1024
- RoBERTa와 비슷하게 batch size는 8000, 반복수는 50만
- Documents는 [GPT-2](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/28/OpenAI-GPT-2-Language-Models-are-Unsupervised-Multitask-Learners/)와 같이 same byte-pair encoding 사용
- text infilling과 sentence permutation을 사전학습 scheme으로 사용
- 학습단계에서 10%를 dropout
- 학습 데이터로 160GB 분량의 news, books, stories, web text 사용

### 5.2 Discriminative Tasks

표 2는 BART의 성능을 SQuAD, GLUE task에서 다른 모델과 비교한 결과이다. 전반적으로 RoBERTa와 비슷하다.

<center><img src="/public/img/2022-08-09-BART/tab02.png" width="100%"></center>



### 5.3 Generation Tasks

생성 task에서도 비교 실험을 진행하였다.

**Summarization**

<center><img src="/public/img/2022-08-09-BART/tab03.png" width="100%"></center>

- CNN / DailyMail의 요약은 source sentences들과 비슷한 경향이 있다. Extractive 모델은 특히 이를 잘 다루지만 BART가 더 우세하다.
- 이에 반해 XSum은 매우 추상적이며 extractive 모델은 여기서 힘을 쓰지 못한다. BART는 점수 수치상 매우 크게 앞선다.

**Dialogue**

모델은 이전 context와 텍스트로 명시된 persona에 기반해서 응답을 생성해야 하는 task이다.

<center><img src="/public/img/2022-08-09-BART/tab04.png" width="60%"></center>

**Abstractive QA**

장문의 자유형식 응답 문장을 생성하는 task에서도 기존 모델과 비교한 결과인데 3가지 metric 모두에서 앞서는 결과를 보여준다. 그러나 데이터셋 자체는 challenging한데 answers는 질문에 의해 weakly specified하기 때문이라 한다.

<center><img src="/public/img/2022-08-09-BART/tab05.png" width="60%"></center>


### 5.4 Translation

<center><img src="/public/img/2022-08-09-BART/tab06.png" width="60%"></center>


---

## 6. Qualitative Analysis

표 7에서 BART의 결과를 볼 수 있다.

- WikiNews 기사에서 가져온 예시들로 모델의 학습 데이터에 있을 가능성을 제거한 상태이다.
- 첫 문장은 대체로 기사를 요약하는 내용이므로 이를 빼고 진행하였다.
- 모델의 출력은 꽤 유창하고 문법적으로 별 문제가 없다. 그러나 상당히 추상적이며 일부 구절은 그대로 가져온 부분이 있다.
- 조금 부족하기는 하나 BART의 사전학습 방식이 자연어 이해와 생성을 꽤 잘 한다는 결과라 볼 수 있다고 저자들은 주장하고 있다.

<center><img src="/public/img/2022-08-09-BART/tab07.png" width="100%"></center>


---

## 7. Related Work

- [Transformer](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/), [ELMo](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/20/ELMo-Deep-contextualized-word-representations/), [OpenAI GPT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/28/OpenAI-GPT-2-Language-Models-are-Unsupervised-Multitask-Learners/), [BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/)
- UniLM은 BERT를 mask의 ensemble로 fine-tune한 것으로 일부는 오직 왼쪽방향 context만 허용되었다. BART와 같이, UniLM은 generative task와 discriminative task 모두에 사용될 수 있다. 차이점은 UniLM의 예측은 조건부 독립이지만 BART는 autoregressive하게 진행된다.
- MASS는 아마도 BART와 가장 비슷한 모델이다. 연속된 span of token이 maked되고 이를 추론하는 사전학습 방식으로 진행되지만 token들이 전혀 겹치지 않게 encoder와 decoder에 들어가 discriminative task에 약하다.
- XL-Net은 순서가 섞인 masked token을 auto-regressive하게 예측하는 방식으로 BERT를 확장했다. 이는 왼쪽과 오른쪽 context를 모두 고려할 수 있게 한다. 이에 반해 BART의 decoder는 왼쪽에서 오른쪽 방향만 사전학습 단계에서 고려한다.


---

## 8. Conclusions

Corrupted documents를 원래대로 복원하는 사전학습 방식을 가진 BART를 제안하였다. Discriminative task에서 RoBERTa와 비슷한 성능을 보이면서도 text generation task에서는 SOTA를 달성하였다. 추후 연구에서는 또 새로운 사전학습 방식을 탐구할 예정이라 한다.

---
