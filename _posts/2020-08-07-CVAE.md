---
layout: post
title: Conditional Variational AutoEncoder (CVAE) 설명
author: Youyoung
categories: [Generative Model]
tags: [Machine Learning, Paper_Review, Bayesian_Statistics]
---

본 글에서는 **Variational AutoEncoder**를 개선한 **Conditional Variational AutoEncoder** (이하 CVAE)에 대해 설명하도록 할 것이다. 먼저 논문을 리뷰하면서 이론적인 배경에 대해 탐구하고, Tensorflow 코드로 살펴보는 시간을 갖도록 하겠다. **VAE**에 대해 알고 싶다면 [이 글](https://greeksharifa.github.io/generative%20model/2020/07/31/Variational-AutoEncoder/)을 참조하길 바란다.  


---
# 1. Learning Structured Output Representation using Deep Conditional Generative Models  
## 1.1. Introduction  
구조화된 Output 예측에서, 모델이 확률적인 추론을 하고 다양한 예측을 수행하는 것은 매우 중요하다. 왜냐하면 우리는 단지 분류 문제에서처럼 many-to-one 함수를 모델링하는 것이 아니라 하나의 Input에서 많은 가능한 Output을 연결짓는 모델이 필요하기 때문이다. CNN이 이러한 문제에 효과적인 모습을 보여주었지만, 사실 CNN은 복수의 mode를 갖는 분포를 모델링하기에는 적합하지 않다.  

이 문제를 다루기 위해서는 Output Representation Learning과 구조화된 예측을 위한 새로운 **Deep Conditional Generative Model**이 필요하다. 즉, 고차원의 Output Space를 Input 관측값에 조건화되어 있는 생성 모델로 모델링해야하는 것이다. **변분 추론**과 **Directed Graphical Model**의 최근 발전에 기반하여 본 논문은 `CVAE`를 새로운 모델로서 제안한다. 이 모델은 **Directed Graphical Model**로서 Input 관측값이 Output을 생성하는 Gaussian 잠재 변수에 대한 Prior를 조절한다. 모델은 조건부 Log Likelihood를 최대화하도록 학습하게 되며, 우리는 이 과정을 `SGVB: Stochastic Gradient Variational Bayes`의 프레임워크 안에서 설명할 것이다. `SGVB`에 대해 미리 알고 싶다면 [이 글](https://greeksharifa.github.io/generative%20model/2020/07/31/Variational-AutoEncoder/)을 참조하도록 하라. 또한 더욱 Robust한 예측 모델을 만들기 위해 우리는 **Input Noise Injection**이나 **Multi-scale Prediction Training Method** 등을 소개할 것이다.  

실험에서 본 모델의 효과성을 보이도록 할 것인데, 특히 데이터가 일부만 주어졌을 때 구조화된 Output을 모델링하는 데에 있어 확률적 뉴런의 중요성을 보여줄 것이다. 데이터셋은 Caltech-UCSD Birds 200과 LFW를 사용하였다.  


## 1.2. Related Work  
(중략)

## 1.3. Preliminary: Variational Auto-Encoder  
이 Chapter 역시 대부분 생략하도록 하겠다. 자세한 설명은 글 서두에 있는 링크를 클릭하여 살펴보도록 하자. 최종적으로 VAE의 목적함수만 정리하고 넘어가겠다.  

$$ \tilde{\mathcal{L}}_{VAE} (\theta, \phi; \mathbf{x}^{(i)}) = -KL (q_{\phi} (\mathbf{z}|\mathbf{x}^{(i)}) || p_{\theta} (\mathbf{z}) ) + \frac{1}{L} \Sigma_{l=1}^L logp_{\theta} (\mathbf{x | z^{(l)}}) $$  


## 1.4. Deep Conditional Generative Models for Structured output Prediction  
변수에는 3가지 종류가 있다. Input 변수 x, Output 변수 y, 잠재 변수 z가 바로 그것이다. $\mathbf{x}$ 가 주어졌을 때, $\mathbf{z}$ 는 아래와 같은 사전 확률로 부터 추출된다.  

$$ p_{\theta}(\mathbf{z|x}) $$  

그리고 output $\mathbf{y}$ 는 아래 분포로 부터 생성된다.  

$$ p_{\theta}(\mathbf{y | x, z}) $$  

baseline CNN과 비교하여 잠재 변수 $\mathbf{z}$ 는 Input이 주어졌을 때 Output 변수에 대한 조건부 분포에서 복수의 mode를 모델링하는 것을 허용하기 때문에 제안된 **CGM** (조건부 생성 모델) 을 one-to-many mapping 모델링에 적합하게 만든다. 위 식에 따르면 잠재 변수의 사전 확률은 Input 변수에 의해 조절되는 것처럼 보이지만, 이러한 제한은 잠재 변수를 Input 변수에 독립적으로 만들어 해소할 수 있다.  

$$ p_{\theta} (\mathbf{z | x}) = p_{\theta} (\mathbf{z}) $$  

**Deep CGM**은 조건부 Conditional Log Likelihood를 최대화하면서 학습된다. 이 목적 함수는 종종 intractable 하기 때무에 우리는 `SGVB` 프레임워크를 적용할 것이다. **ELBO**는 아래와 같다.  

$$ log p_{\theta} (\mathbf{y|x}) \geq \tilde{\mathcal{L}}_{VAE} (\mathbf{x, y} ; \theta, \phi) = -KL( q_{\theta} (\mathbf{z | x, y}) || p_{\theta} (\mathbf{z|x}) ) + E_{q_{\phi} (\mathbf{z}|\mathbf{x, y})} \{ logp_{\theta} (\mathbf{y}|\mathbf{x, z}) \} $$  

물론 위 식의 우변의 두 번째 항은 **Monte-Carlo Estimation**을 통해 경험적으로 값을 얻을 수 있다. 이 기법에 대해 알고 싶다면 [이 글](https://greeksharifa.github.io/bayesian_statistics/2020/07/30/Monte-Carlo-Approximation/)을 참조하라. 다시 포현하면 아래와 같다.  

$$ \tilde{\mathcal{L}}_{VAE} (\mathbf{x, y} ; \theta, \phi) = -KL( q_{\theta} (\mathbf{z | x, y}) || p_{\theta} (\mathbf{z|x}) ) + \frac{1}{L} \Sigma_{l=1}^L logp_{\theta} (\mathbf{x | z^{(l)}}) $$

$L$ 은 Sample의 개수이며 이 때,  

$$ \mathbf{z}^{(l)} = g_{\phi} (\mathbf{x, y} , {\epsilon}^{(l)} ), {\epsilon}^{(l)} \sim \mathcal{N} (\mathbf{0}, \mathbf{I}) $$  

본 논문은 이 모델을 `CVAE`라고 부를 것이다. 이 모델은 복수의 MLP로 구성되는 데 크게 3가지의 요소를 갖고 있다.  

1) Recognition Network  

$$ q_{\phi} (\mathbf{z | x, y}) $$  

2) Prior Network  

$$ p_{\theta} (\mathbf{z | x})  $$  

3) Generation Network  

$$ p_{\theta} (\mathbf{y|x, z}) $$  

이 네트워크 구조를 디자인할 때, baseline CNN 위에 `CVAE`의 구성요소를 올릴 것이다. 아래 그림에서 (d)를 확인해보자.  

<center><img src="/public/img/Machine_Learning/2020-08-07-CVAE/01.JPG" width="100%"></center>  

직접적인 Input $\mathbf{x}$ 뿐만 아니라 CNN으로부터 만들어진 최초의 예측 값 $\hat{\mathbf{y}}$ 는 **Prior Network**로 투입된다. 이러한 순환 연결은 구조화된 Output 예측 문제에서 효과적으로 합성곱 네트워크를 깊게 만들면서 이전의 추측을 수정하여 예측값을 연속적으로 업데이트하는 과정에 적용된 바 있다. 우리는 또한 이러한 순환 연결이, 설사 단 한 번의 반복에 그치더라도, 굉장한 성능 향상을 이끌어낸다는 사실을 발견했다. 네트워크 구조에 대한 자세한 사항은 이후에 설명할 것이다.  

### 1.4.1. Output Inference and Estimation of the Conditional Likelihood  




---




---
# References  
1) [논문 원본](https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models)  
2) s
