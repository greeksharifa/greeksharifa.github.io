---
layout: post
title: VQ-VAE 2 논문 설명(Generating Diverse High-Fidelity Images with VQ-VAE-2)
author: YouWon
categories: [Discrete Representation]
tags: [Paper_Review, Multimodal, VQVAE]
---

---

이 글에서는 2019년 NIPS에 게재된 Generating Diverse High-Fidelity Images with VQ-VAE-2(VQ-VAE2) 논문을 살펴보고자 한다.


중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.

---

# Generating Diverse High-Fidelity Images with VQ-VAE-2

논문 링크: **[Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446)**

## 초록(Abstract)


이 논문에서는 [VQ-VAE(Vector Quantized Variational AutoEncoder)](https://greeksharifa.github.io/discrete%20representation/2021/11/07/VQVAE/)로 large-scale 이미지 생성 문제를 다루려 한다. 이를 위해 prior를 더욱 강화하여 이전보다 더 일관성, 현실성 있는 이미지를 만들었다. 모델은 단순한 feed-forward encoder와 decoder를 사용하였으며 회귀모델은 오직 latent space 안에서만 쓰여 pixel space에서는 기존보다 더욱 빠른 속도를 갖는다. 최종적으로 GAN과 비슷한 성능을 보이면서도 mode collapsing과 같은 GAN의 고질적인 문제를 겪지 않는 생성 모델을 만들었다.


---

## 1. 서론(Introduction)

Deep Generative model은 최근 크게 발전하며 실제와 분간이 힘든 이미지를 생성할 수 있는 수준에 이르렀다. GAN은 minimax 방식을 사용하여 현실적인 이미지를 생성하였지만 실제 세상의 이미지 분포를 그대로 따라가지 못하는 문제가 존재한다. 또한 생성한 이미지를 평가하기도 매우 어려워서 근사하는 방식인 FID 등의 metric으로 평가를 해야 한다.


<center><img src="/public/img/2021-11-26-VQVAE2/fig01.png" width="100%" alt="VQVAE"></center>


이와는 대조적으로 likelihood 기반 방식은 NLL을 최적화하며 이는 모델 비교와 아직 보지 못한 일반화 성능을 측정할 수 있게 한다. 또 모델은 학습 셋의 **모든** example에 대한 확률을 최대화하므로 분포 문제에서도 자유로운 편이다. 그러나 이 방식 역시 어려운 점이 있는데, 

- Pixel space에서의 NLL은 sample의 품질을 측정하기 적합하지 않고
- 다른 model class 사이의 비교하는 데 사용하기도 어렵다.

본 논문에서는 무시해도 되는 정보를 모델링하는 데 드는 노력을 최소화하도록 lossy compression으로부터 얻은 아이디어를 사용한다. 사실 JPEG 압축 방식은 이미지 품질에 영향을 주지 않는 선에서 80%의 용량을 줄일 수 있는 방식이다. 이와 비슷한 아이디어로, 본 논문에서는 이미지를 매우 작은 discrete latent space로 옮겨 저장하는 방식을 취한다. 이는 30배 더 작은 공간을 차지하면서도 decoder가 원래 이미지를 (눈으로 보기에) 거의 비슷하게 복원할 수 있게 한다.

이산표현에서 prior는 PixelCNN + self-attention으로 모델링이 가능하여(PixelSnail), prior로부터 sampling할 때 디코딩된 이미지는 복원 시 고품질과 높은 coherence를 가진다.  인코딩된 이미지는 원본 이미지보다 30배 가량 작고 또 모델의 속도도 빨라서 기존의 생성모델에 비견될 만한 성능을 보인다.

---

## 2. 배경(Background)


### 2.1 Vector Quantized Variational AutoEncoder

[VQ-VAE](https://greeksharifa.github.io/discrete%20representation/2021/11/07/VQVAE/)는 Variational AutoEncoder와 거의 비슷한 구조를 갖는데, 가장 중요한 차이점은 latent space가 discrete하다는 것이다. 지정된 크기(ex. 512)의 codebook(embedding space)을 유지하면서 이미지의 각 부분을 하나의 embedding의 인덱스에 대응시켜 이미지를 표현한다. Encoder는 입력 $x$에 대해 $x \mapsto E(x)$의 역할을 수행한다. 이 벡터 $E(x)$는  codebook vector $e_k$와의 거리에 기반하여 quantized되며 이는 decoder로 전달된다.

$$ \text{Quantize}(E(x)) = \textbf{e}_k \qquad \text{where} \quad k = \argmin_j \Vert E(x) - \textbf{e}_j \Vert $$


Decoder는 이미지를 표현한 인덱스를 다시 대응되는 codebook vector로 mapping한다. 

VQ-VAE는 다음 식으로 최적화된다.


$$ L = \log p(x \vert z_q(x)) + \Vert \text{sg}[z_e(x)]-e \Vert_2^2 + \beta \Vert z_e(x)-\text{sg}[e] \Vert_2^2 , \qquad (3)$$

Encoder의 출력과 codebook의 벡터 공간을 일치시키는 것에 대해 두 가지 loss를 포함한다.

- *codebook loss*: 오직 codebook에만 영향을 준다.  선택된 codebook $\textbf{e}$를 encoder의 출력 $E(x)$에 가까워지도록 한다.
- *the commitment loss*: encoder의 weight에만 영향을 준다. encoder의 출력이 선택된 codebook 벡터에 가까워지도록 해서 여러 codebook 사이를 자꾸 왔다갔다하지 않도록 하는 역할을 한다.

본 논문에서는 여기에 exponential moving average를 적용시킨다.


---

## 3. 방법(Method)

두 단계로 이루어진다.

1. 이미지를 discrete latent space로 mapping하기 위해 VQ-VAE를 학습시킨다.
2. 모든 데이터를 사용하여 PixelCNN prior를 이 discrete latent space에 fit시킨다.

<center><img src="/public/img/2021-11-26-VQVAE2/alg01.png" width="100%" alt="VQVAE"></center>


### 3.1 Stage 1: Learning Hierarchical Latent Codes



### 3.2 Stage 2: Learning Priors over Latent Codes


### 3.3 Trading off Diversity with Classifier Based Rejection Sampling


---

## 4. 관련 연구(Related Works)


---


## 5. 실험(Experiments)




### 5.1 Modeling High-Resolution Face Images



### 5.2 Quantitative Evaluation




#### 5.2.1 Negative Log-Likelihood and Reconstruction Error




#### 5.2.2 Precision - Recall Metric




### 5.3 Classification Accuracy Score




#### 5.3.1 FID and Inception Score




---

## 6. 결론(Conclusion)




---

## 참고문헌(References)

논문 참조!

--- 



## 부록 A(Architecture Details and Hyperparameters)

### A.1 PixelCNN Prior Networks



### A.2 VQ-VAE Encoder and Decoder


## 부록 B(Additional Samples)

