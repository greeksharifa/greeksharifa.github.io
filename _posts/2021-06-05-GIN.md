---
layout: post
title: Graph Isomorphism Network (GIN) 설명
author: Youyoung
categories: [Machine_Learning]
tags: [Machine_Learning, Recommendation System, Paper_Review]
---

본 글에서는 2018년에 발표된 **How Powerful are Graph Neural Networks**라는 논문에 대한 Review를 진행할 것이다. 본 논문에서는 Graph Neural Network에서 거의 필수적으로 진행되는 Aggregation 과정에서의 문제점을 지적하고 이를 해결할 방법으로 `Graph Isomorphism Network`를 제시하고 있다.  

참고로 논문에서 직접적으로 언급하고 있는 **GraphSAGE**에 대해 궁금하다면, [이 글](https://greeksharifa.github.io/machine_learning/2020/12/31/Graph-Sage/)을 참조하면 좋을 것이다.    

---
# Graph Isomorphism Networks 리뷰  
## 1. Introduction  
GNN은 **Neighborhood Aggregation** 혹은 **Message Passing**이라는 반복적인 과정을 수행하여 각 Node의 새로운 feature 벡터를 형성하기 위해 이웃 Node의 이웃을 통합하게 된다. 이러한 통합이 과정이 k번 수행되고 나면, 그 Node는 변형된 feature 벡터로 표현될 것이고, 이는 그 Node의 k-hop Neighborhood 안에 존재하는 구조적인 정보를 포착하게 된다.  

GNN이 괄목할 만한 성과를 거두어왔던 것은 사실이지만, GNN의 새로운 디자인은 대부분 경험적 직관, 휴리스틱, 혹은 실험에 기반한 것으로 GNN의 특징과 한계점에 대한 이론적인 연구가 부족했던 것을 사실이다. 또한 GNN의 `Representional Capacity`에 대한 이전의 연구도 제한적이었다.  

따라서 본 논문에서는 GNN의 `Representational Power`에 대해 분석하는 이론적인 프레임워크를 제시할 것이다. 이 프레임워크는 GNN과 **Weisfeiler-Lehman Graph Isomorphism Test** 사이의 유사성에 기반하여 만들어졌는데, 이 WL Test는 여러 종류의 Graph를 구별할 수 있는 효과적인 테스트 방법으로 알려져 있다.  

GNN과 비슷하게, WL Test도 Node가 주어졌을 때, 이 Node의 이웃들의 Feature 벡터를 통합하여 Node Feature 벡터를 반복적으로 업데이트한다. WL Test의 경우 다른 Node 이웃들을 다른 Feature 벡터로 매핑하는 **Injective Aggregation Update**의 존재로 인해 강력한 효과를 지니게 된다.  

이 말이 무엇을 의미하는지 잘 생각해 보아야 하므로 예시를 들어 보겠다. (물론 논문 본문에서 잘 설명하고 있다.) 만약 아래와 같은 Node와 Feature 벡터가 주어졌다고 해보자.  

|Node|Feature Vector|
|:--------:|:--------:|
|알라딘 서점| [1, 0, 0.5] |
|스타벅스 커피| [0, 1, 0.5] |
|이디야 커피| [0.5, 0.5, 0.5] |

이 때, 알라딘 서점과 스타벅스 커피는 사람 A의 이웃 Node이고, 이디야 커피는 사람 B의 이웃 Node라고 생각해보자. 그리고 사람 A와 사람 B의 Feature 벡터를 얻기 위해 **GraphSAGE**에서 제시한 Mean Aggregation 과정을 진행한다고 해보자.  

그렇다면 사람 A의 이웃 Node의 Feature 벡터는 [0.5, 0.5, 0.5]가 될 것이고, 사람 B의 이웃 Node의 Feature 벡터 또한 [0.5, 0.5, 0.5]가 될 것이다. 물론 위 예시에 대해 애초에 Feature 값이 이들을 표현하기에 불충분하다고 생각할 수도 있지만, 충분히 실제 데이터 상에서 나타날 수 있는 예시일 것이다. 어쨌든 위 결과에 따르면 사람 A와 사람 B의 이웃 Feature 벡터는 똑같은 형상을 하게 될 것이고, 이는 학습에 있어서 데이터 소실 혹은 왜곡이라는 문제로 귀결되게 될 것이다.  

본 논문에서는 GNN의 Aggregation Scheme이 굉장히 **Expressive**하고 **Injective Function**을 모델링할 수 있다면 GNN 또한 WL Test 처럼 굉장히 강력한 Disciminative Power를 지니게 될 것이라고 이야기 하고 있다.  

수학적으로 위 인사이트를 공식화하기 위해, 본 논문의 Framework는 먼저 Node가 주어졌을 때, 이 Node의 이웃들의 Feature 벡터를 **Multiset**, 즉 repeating elements가 존재할 수 있는 집합으로 표현할 것이다. 그러면 GNN에 있는 Neighbor Aggregation이 **Aggregation Function over the multiset**으로 표현될 수 있을 것이다. 따라서 강력한 Representational Power를 가지기 위해서는 GNN은 다른 multiset을 다른 representation으로 표현할 수 있어야 한다.  

본 논문에서는 몇몇 multiset function의 변형 버전과 그들의 discriminative power, 즉 aggregation function이 다른 multiset들을 얼마나 잘 구별할 수 있는지를 이론적으로 분석하였다. 더 disciminative 할 수록, GNN 속에 내재되어 있는 Representational Power는 더 강력할 것이다.  

본 논문의 핵심적인 결과는 아래와 같다.  

```
1) GNN은 Graph 구조를 판별하는데에 있어 WL Test 만큼이나 효과적이다.  
2) 위 문장이 성립하기 위해서는 Neighbor Aggregation이나 Graph Readout Function에 관하여 조건들이 필요한데, 본 논문에서는 이를 제시하였다.  
3) 본 논문에서는 GCN이나 GraphSAGE는 구분할 수 없는 Graph 구조에 대해 언급하고, GNN 기반의 모델들이 포착할 수 있는 Graph 구조들에 대해 정확하게 분석하였다.  
4) 본 논문에서는 GIN이라고 부르는 간단한 신경망 구조를 고안하였고, 이 알고리즘이 WL Test 만큼이나 discriminative/representational power를 갖고 있음을 증명하였다.  
```

---
## 2. Preliminaries
이 Section은 GNN의 기본에 관한 것이므로 간단히만 짚고 넘어가도록 하겠다. 자세한 내용은 논문 원본을 참고하면 좋을 것이다.  

**Graph Neural Networks**  
GNN은 Graph 구조와 Node Feature $X_v$ 를 활용하여 Node에 대한 Representatino 벡터 $h_v$ 혹은 전체 Graph $h_G$ 를 학습하는 것을 목적으로 한다. 현대의 GNN은 이웃의 표현을 통합하여 Head Node의 Feature를 업데이트하기 위해 **Neighborhood Aggregation**이란 방법을 사용한다. GNN의 k번째 layer는 아래와 같이 표현할 수 있을 것이다.  

$$ a_v^{k} = AGGREGATE^{k} (\{ h_u^{k-1}: u \in \mathcal{N}(v) \}, h_v^{k} = COMBINE^{k} (h_v^{k-1}, a_v^{k}) $$  

$h_v^{(k)}$ 는 Node $v$ 의 $k$ 번째 반복 혹은 layer의 Feature 벡터를 의미하게 된다. 위에서 표기한 Aggregate 및 Combine 함수는 정말로 중요한 부분이다. 이를 어떻게 정의하느냐에 따라 GNN의 디자인과 효과는 굉장히 상이할 수 있다. **GraphSAGE**에서는 Aggregate 부분을 아래와 같이 정의하고 있다.  

$$ a_v^{k} = MAX( ReLU(W \cdot h_u^{k-1}), \forall u \in \mathcal{N} (v) ) $$  

Combine 부분은 아래와 같이 concatenation 방식으로 정의된다.  

$$ W [h_v^{k-1}, a_v^k] $$  

**GCN**에서는 Aggregate 및 Combine 방식을 아래와 같이 통합하여 element-wise man pooling을 사용하였다.  

$$ h_v^k = ReLU( W \cdot MEAN \{ h_u^{k-1}, \forall u \in \mathcal{N} (v) \cup \{ v \} \} ) $$  

Node Classification의 경우 최종 Layer의 Representation이 예측을 위해 사용되며, Graph Classification의 경우 Readout 함수가 아래와 같이 Node Feature들을 통합하게 된다.  

$$ h_G = Readout ( \{ h_v^K \vert v \in G \} ) $$  

**Weisfeiler-Lehman Test**  
Graph Isomorphism (동형 이성) 문제는 2개의 Graph가 위상적으로 동일한지 판단하는 문제를 의미한다. 이는 굉장히 큰 문제인데, 왜냐하면 아직까지 polynomial-time 알고리즘을 알려지지 않았기 때문이다. 몇몇 극단적인 예시를 제외하고는 WL Test의 경우 Graph의 broad class를 구별하는 효과적인 테스트로 평가받고 있다.  

WL Test는 반복적으로 Node의 Label과 이웃을 통합한 후, 이렇게 통합된 label 혹은 이웃을 *unique*한 새로운 Label로 **Hash**한다. 이 알고리즘은 만약 일정 수준의 반복 이후 2개의 Graph에 존재하는 Node들의 Label이 달라지게 되면 2개의 Graph는 non-isomorphic 하다고 판단한다.  


---
## 3. Theoretical Frameworks: Overview  
<center><img src="/public/img/Machine_Learning/2021-06-05-GIN/01.JPG" width="70%"></center>  

GNN은 반복적으로 Node의 Feature 벡터를 업데이트하여 Network 구조와 다른 이웃 Node의 Feature들을 포착한다. 본 논문에서는 Node Input Feature는 countable universe에서 왔다고 가정할 것이다. 유한한 수의 Graph에 대해 어떠한 고정된 모델 하의 깊은 Layer에 있는 Node Feature 벡터 또한 countable universe에서 왔다. 단순하게 기호를 표현하기 위해, 본 논문에서는 각 Feature 벡터에 대해 고유한 Label을 $a, b, c ...$ 와 같이 붙여줄 것이다. 그리고 나서 이들의 이웃 Node의 Feature 벡터로 **multiset**을 형성할 것이다. 이 때 다른 Node라 하더라도 같은 Feature 벡터를 가질 수 있으므로 같은 element도 여러 번 등장할 수 있다.  

**Multiset**의 정의를 다시 짚고 넘어가기 위해 아래 논문 원본을 참조하면 좋을 것이다.  

<center><img src="/public/img/Machine_Learning/2021-06-05-GIN/01.JPG" width="70%"></center>  

GNN의 Representational Power를 연구하기 위해, 본 논문에서는 GNN의 2개의 Node를 Embedding 공간 상에서 같은 위치에 매핑할 때를 분석하였다. 직관적으로 이러한 경우가 발생하기 위해서는, 각 Node가 존재하는 subtree 구조가 동일해야 할 것이다. 이 때 subtree 구조는 Node의 이웃에 의해 재귀적으로 정의되므로 이제 논의 주제는, 과연 GNN이 2개의 이웃 집합(Neighborhood)을 같은 Embedding 혹은 Representation으로 매핑할 것인가 하는 문제로 생각해 볼 수 있다.  

**Maximally Powerful GNN**이라면 2개의 다른 이웃 집합(혹은 multisets of feature 벡터)을 같은 Representation에 매핑하지 않을 것이다. 이 때의 Aggregation Scheme은 반드시 `Injective`해야 한다. 따라서 본 논문에서는 GNN의 Aggregatino Scheme을 이 신경망이 표현하는 multiset에 대한 함수의 class로 추상화하고, 이 신경망이 **injective multiset function**을 표현할 수 있는지 분석할 것이다.  

다음 Section에서는 이러한 추론을 바탕으로 가장 강력한 GNN을 개발할 것이다. Section 5에서는 유명한 GNN 변형 버전들에 대해 이야기하고, 이들의 Aggregation Scheme이 내재적으로 `injective`하지 않고, 그렇기 때문에 덜 powerful하며 Graph의 흥미로운 특징들을 잘 담아내지 못한다는 것을 증명할 것이다.  


---
## 4. Building Powerful Graph Neural Networks  


---
## 5. Less Powerful but still interesting GNNs  


---
## 6. Other Related Work  


---
## 7. Experiments  


---
## 8. Conclusion  




---
# References  
1) [논문 원본](https://arxiv.org/pdf/1810.00826.pdf)  

