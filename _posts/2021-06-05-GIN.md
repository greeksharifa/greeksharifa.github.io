---
layout: post
title: Graph Isomorphism Network (GIN) 설명
author: Youyoung
categories: [Machine_Learning]
tags: [Machine_Learning, Recommendation System, Paper_Review]
---

본 글에서는 2018년에 발표된 **How Powerful are Graph Neural Networks**라는 논문에 대한 Review를 진행할 것이다. 본 논문에서는 Graph Neural Network에서 거의 필수적으로 진행되는 Aggregation 과정에서의 문제점을 지적하고 이를 해결할 방법으로 `Graph Isomorphism Network`를 제시하고 있다.  

참고로 논문에서 직접적으로 언급하고 있는 **GraphSAGE**에 대해 궁금하다면, [이 글](https://greeksharifa.github.io/machine_learning/2020/12/31/Graph-Sage/)을 참조하면 좋을 것이다.    

---
# Graph Isomorphism Networks 리뷰  
## 1. Introduction  
GNN은 **Neighborhood Aggregation** 혹은 **Message Passing**이라는 반복적인 과정을 수행하여 각 Node의 새로운 feature 벡터를 형성하기 위해 이웃 Node의 이웃을 통합하게 된다. 이러한 통합이 과정이 k번 수행되고 나면, 그 Node는 변형된 feature 벡터로 표현될 것이고, 이는 그 Node의 k-hop Neighborhood 안에 존재하는 구조적인 정보를 포착하게 된다.  

GNN이 괄목할 만한 성과를 거두어왔던 것은 사실이지만, GNN의 새로운 디자인은 대부분 경험적 직관, 휴리스틱, 혹은 실험에 기반한 것으로 GNN의 특징과 한계점에 대한 이론적인 연구가 부족했던 것을 사실이다. 또한 GNN의 `Representional Capacity`에 대한 이전의 연구도 제한적이었다.  

따라서 본 논문에서는 GNN의 `Representational Power`에 대해 분석하는 이론적인 프레임워크를 제시할 것이다. 이 프레임워크는 GNN과 **Weisfeiler-Lehman Graph Isomorphism Test** 사이의 유사성에 기반하여 만들어졌는데, 이 WL Test는 여러 종류의 Graph를 구별할 수 있는 효과적인 테스트 방법으로 알려져 있다.  

GNN과 비슷하게, WL Test도 Node가 주어졌을 때, 이 Node의 이웃들의 Feature 벡터를 통합하여 Node Feature 벡터를 반복적으로 업데이트한다. WL Test의 경우 다른 Node 이웃들을 다른 Feature 벡터로 매핑하는 **Injective Aggregation Update**의 존재로 인해 강력한 효과를 지니게 된다.  

이 말이 무엇을 의미하는지 잘 생각해 보아야 하므로 예시를 들어 보겠다. (물론 논문 본문에서 잘 설명하고 있다.) 만약 아래와 같은 Node와 Feature 벡터가 주어졌다고 해보자.  

|Node|Feature Vector|
|:--------:|:--------:|
|알라딘 서점| [1, 0, 0.5] |
|스타벅스 커피| [0, 1, 0.5] |
|이디야 커피| [0.5, 0.5, 0.5] |

이 때, 알라딘 서점과 스타벅스 커피는 사람 A의 이웃 Node이고, 이디야 커피는 사람 B의 이웃 Node라고 생각해보자. 그리고 사람 A와 사람 B의 Feature 벡터를 얻기 위해 **GraphSAGE**에서 제시한 Mean Aggregation 과정을 진행한다고 해보자.  

그렇다면 사람 A의 이웃 Node의 Feature 벡터는 [0.5, 0.5, 0.5]가 될 것이고, 사람 B의 이웃 Node의 Feature 벡터 또한 [0.5, 0.5, 0.5]가 될 것이다. 물론 위 예시에 대해 애초에 Feature 값이 이들을 표현하기에 불충분하다고 생각할 수도 있지만, 충분히 실제 데이터 상에서 나타날 수 있는 예시일 것이다. 어쨌든 위 결과에 따르면 사람 A와 사람 B의 이웃 Feature 벡터는 똑같은 형상을 하게 될 것이고, 이는 학습에 있어서 데이터 소실 혹은 왜곡이라는 문제로 귀결되게 될 것이다.  

본 논문에서는 GNN의 Aggregation Scheme이 굉장히 **Expressive**하고 **Injective Function**을 모델링할 수 있다면 GNN 또한 WL Test 처럼 굉장히 강력한 Disciminative Power를 지니게 될 것이라고 이야기 하고 있다.  

수학적으로 위 인사이트를 공식화하기 위해, 본 논문의 Framework는 먼저 Node가 주어졌을 때, 이 Node의 이웃들의 Feature 벡터를 **Multiset**, 즉 repeating elements가 존재할 수 있는 집합으로 표현할 것이다. 그러면 GNN에 있는 Neighbor Aggregation이 **Aggregation Function over the multiset**으로 표현될 수 있을 것이다. 따라서 강력한 Representational Power를 가지기 위해서는 GNN은 다른 multiset을 다른 representation으로 표현할 수 있어야 한다.  

본 논문에서는 몇몇 multiset function의 변형 버전과 그들의 discriminative power, 즉 aggregation function이 다른 multiset들을 얼마나 잘 구별할 수 있는지를 이론적으로 분석하였다. 더 disciminative 할 수록, GNN 속에 내재되어 있는 Representational Power는 더 강력할 것이다.  

본 논문의 핵심적인 결과는 아래와 같다.  

```
1) GNN은 Graph 구조를 판별하는데에 있어 WL Test 만큼이나 효과적이다.  
2) 위 문장이 성립하기 위해서는 Neighbor Aggregation이나 Graph Readout Function에 관하여 조건들이 필요한데, 본 논문에서는 이를 제시하였다.  
3) 본 논문에서는 GCN이나 GraphSAGE는 구분할 수 없는 Graph 구조에 대해 언급하고, GNN 기반의 모델들이 포착할 수 있는 Graph 구조들에 대해 정확하게 분석하였다.  
4) 본 논문에서는 GIN이라고 부르는 간단한 신경망 구조를 고안하였고, 이 알고리즘이 WL Test 만큼이나 discriminative/representational power를 갖고 있음을 증명하였다.  
```


---
## 2. Preliminaries

<center><img src="/public/img/Machine_Learning/2021-06-05-GIN/01.JPG" width="60%"></center>  




---
## 3. Theoretical Frameworks: Overview  


---
## 4. Building Powerful Graph Neural Networks  


---
## 5. Less Powerful but still interesting GNNs  


---
## 6. Other Related Work  


---
## 7. Experiments  


---
## 8. Conclusion  




---
# References  
1) [논문 원본](https://arxiv.org/pdf/1810.00826.pdf)  

