---
layout: post
title: HowTo100M 설명(HowTo100M - Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips, Antoine Miech et al, 7 Jun 2019, 1906.03327)
author: YouWon
categories: [Computer Vision]
tags: [Paper_Review, Dataset]
---

---

이 글에서는 2019년 ICCV에 발표된 HowTo100M - Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips 논문을 살펴보고자 한다.


중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.

---

# HowTo100M - Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips

논문 링크: **[HowTo100M - Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips](https://arxiv.org/abs/1906.03327)**

데이터셋, 코드 및 모델: [Project webpage](https://www.di.ens.fr/willow/research/howto100m/)

## 초록(Abstract)

Text-video 임베딩을 학습하는 것은 대개 video clip과 수동으로 만든 caption을 필요로 한다. 그러나 이러한 데이터셋은 만드는 것이 매우 비싸며 오래 걸리고 따라서 큰 규모의 데이터셋을 얻기 힘들다. 본 연구에서, 이러한 임베딩을 video에서 학습하는 대신 쉽게 얻을 수 있는 자동생성된 narration을 annotation으로 사용한다. 이 논문이 기여한 바는,

1. HowTo100M 데이터셋 제안: 23k개의 다른  visual task를 묘사하는 절차적 web video에서 얻은 136M개의 video clip을 포함
2. 위 데이터로 학습한 Text-video 임베딩이 text-to-video retrieval과 action localization에서 SOTA로 이끔을 보임
3. 이 임베딩이 Youtube video나 movie 등 다른 domain에서 잘 transfer되는 것을 보임


---

## 1. 서론(Introduction)

실세계를 이해하고 상호작용하는 것은 시각적(영상) 정보와 언어(텍스트) 정보를 이해하고 상호작용하는 것과 매우 유사성이 높다. 그러나 text-to-video retrieval, video captioning, VQA 등의 task는 여전히 AI 시스템에게 있어 상당히 어려운 과제이다.

언어롤 묘사하는 visual concept을 다루는 일반적인 방법은 text와 video를 공통 임베딩 공간으로 mapping하여 관련된 text와 video clip들이 비슷한 곳에 모여 있게 하는 것이다. 그러나 이러한 방법은 매우x2 큰 데이터셋을 필요로 하지만, 현존하는 데이터셋(MSR-VTT, DiDeMo, EPIC-KITCHENS) 등은 수동 생성한 annotation이 수천~수만 정도의 규모밖에 되지 않으며 그 품질 또한 낮기도 하다.

본 연구에서는 video와 language의 결합표현을 학습할 수 있는 video clip과 text caption 쌍을 얻는 방법을 탐색하였다. YouTube에서 *narrated instructional video*는 규모가 크고 대량의 visual/language 데이터를 얻을 수 있다. Instructional video는 대개 narration을 포함하며 명시적으로 영상의 내용을 설명하기에 학습용으로 적합하다.  
따라서 YouTube에서 23k 종류 이상의 서로 다른 task를 수행하는 것을 묘사하는 1.22M개의 *narrated instructional video*를 수집하였다. 각 clip은 자동 생성된 narration 형식으로 text annotation와 연결되어 있다.


> 기여한 바가 적혀 있는데, 이는 초록의 내용과 같다.



---

## 2. 관련 연구(Related Work)

Visual/textual cue의 이해에 기반한 여러 CV task가 제안되어 왔다.

**Vision, language and speech**

Visual/Textual cue가 서로 비슷한 경우에만 공용 공간 내에서 인접하게 하는 방식이 일반적이다. 이들은 대부분 잘 annotated된 데이터셋을 필요로 하기에 상당히 비용이 비싸다.  

본 논문에서는 수동으로 annotation을 일절 하지 않고 자동 생성된 narration만을 사용한다. Spoken text를 사용하는 모델이 있었지만 음성 녹음이 안 되어 있거나 그 규모가 매우 작은 편이었다.

<center><img src="/public/img/2021-08-18-HowTo100M/fig01.png" width="70%" alt="HERO"></center>

**Learning from instructional videos**

복잡한 과제를 해결하는 학습 과정으로써 최근 주목받고 있다. Visual-linguistic reference resolution, action segmentation 등의 task에서 여러 모델이 제안되었으나 역시 적은 수의 미리 지정된 label에서 추출한 trasncription만이 존재했다.

이후 WikiHow, COIN, CrossTask 등의 데이터셋이 만들어졌고 이들은 대개 YouTube에서 대량의 영상 정보를 획득하였다. HowTo100M도 비슷하게 YouTube에서 영상을 얻었으나 그 양이 전례없이 매우 방대하다.

**Large scale data for model pretraining**

Noisy할 수도 있는 web data로부터 얻은 대규모 데이터셋은 language 및 vision 모델에서 유망한 부분이다. [BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/), [GPT-2](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/28/OpenAI-GPT-2-Language-Models-are-Unsupervised-Multitask-Learners/) 등은 대규모 데이터를 사용하여 많은 task에서 SOTA를 달성하였다. GPT-2와 WebText는 Reddit에서 얻은 40GB의 text를 사용하여 좋은 결과를 얻었다.

이에 영감을 받아 video와 language의 공통 임베딩을 학습하고자 하였다. YouTube에서 얻은 1M 이상의 영상을 사용하여, 미세조정 없이도 instructional video에서 이전 결과를 넘을 뿐 아니라 non-instructinoal video에 대해서도 잘 작동한다.



---

## 3. The HowTo100M dataset

요리, 수공예, 개인 생활/관리, 원예 등의 활동에 대한 내용을 포함하는 1.22M개의 영상을 포함한다. 각 영상은 (올린 사람이 만들었을) 자막 혹은 ASR로 자동 생성된 narration을 가진다.



### 3.1 Data collection


**Visual tasks**

- Instructional video는 어떻게 특정 활동을 수행하는지를 묘사하기 때문에 어떤 활동이 있는지를 먼저 WikiHow("어떻게 무엇을 하는가"라는 12만 개의 article 포함)에서 가져왔다.  
- 많은 영상들 중 추상적인 것("선물 선택하기")보다는 "시각적인" 활동("땅콩버터 만들기")에 초점을 맞추었다. Task는 12종류로 한정하였고 관계/경제/사업 등 추상적인 task는 제외하였다.
- "물리적인" 활동이 포함되는 것(make, build, change)을 남기고 그렇지 않은 것(be, accept, feel)은 반자동으로 제거하였다. 
- 최종적으로 23,611개의 visual task가 생성되었다.


<center><img src="/public/img/2021-08-18-HowTo100M/fig02.png" width="100%" alt="HERO"></center>


**Instructional videos**

`How to <task name>` 형태의 제목을 가지며 영어 자막(사람이 올렸거나 혹은 YouTube API인 ASR로 얻은 것)을 가지는 영상을 찾았다. 이후 다음 과정을 통해 품질과 일관성을 높였다.

- 검색 결과의 상위 200까지만 사용하고 나머지는 관련성이 별로 없을 가정하에 제외함
- 조회수 100 미만의 영상은 제외
- 100 단어 이하의 영상은 배울 것이 별로 없으므로 제외
- 2,000초 이하의 영상만 사용
- 중복 영상(동일 ID) 제거


### 3.2 Paired video clips and captions

자막은 text의 뭉치들로 이루어져 있으며 때때로 완전한 문장의 형태가 아닌 경우도 있다. 각 줄은 영상의 특정 구간과 연관된다(특히 그 자막이 읽히는 부분). 각 자막을 caption으로 사용하여 video clip의 연관된 구간과 쌍을 만든다(그림 2).

MSR-VTT와 같은 clip-caption 쌍으로 구성된 다른 데이터셋과 달리 HowTo100M은 자동으로 만든 narration을 사용한다. 그래서 (아마도) 약하게 짝을 이룬(*weakly paired*) 것으로 생각된다. 영상에서는 구독자와 소통하는 등 주제와 상관 없는 내용이 있을 수도 있고 문장도 완전하지 않거나 문법적으로 틀릴 수도 있다. 임의로 400개의 clip-caption 쌍을 선택해서 확인한 결과 자막에 포함된 물체나 행동 등이 적어도 한 번 나올 확률은 51%였다.

**Statistics**

12개의 분류로 나누어져 있으며 예시는 그림 2에서 볼 수 있다. 자세한 내용은 부록 A에 있다.  
각 데이터셋의 clip-caption의 개수는 표 1에서 볼 수 있다.

<center><img src="/public/img/2021-08-18-HowTo100M/table01.png" width="70%" alt="HERO"></center>

HowTo100M은,

- 다른 데이터셋에 비해 훨씬 크다.
- 자동 생성된 annotation을 사용하여 자막의 품질이 깨끗하지 않다.
- 평균적으로 하나의 영상은 110개의 clip-caption 쌍을 만들며 clip당 4초, 4단어 정도이다.
- 100개를 임의로 확인한 결과 71%는 instructional한 영상, 12%는 vlog, 7%는 리뷰나 광고였다.
    - vlog나 리뷰, 광고는 시각적인 내용과 narration 사이의 관련성이 있을 수 있다.   
- non-instructional videoi를 특별히 제거하지는 않았는데, 공통 임베딩을 학습하는 데 도움을 줄 수 있어서다.


---



## 4. Text-video joint embedding model



---

[ELMo](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/20/ELMo-Deep-contextualized-word-representations/)

[OpenAI GPT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/28/OpenAI-GPT-2-Language-Models-are-Unsupervised-Multitask-Learners/)

[BERT](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/23/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/)

[ERNIE 1.0](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2021/06/14/ERNIE/)

[Transformer](https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/17/Attention-Is-All-You-Need/)


## 5. 실험(Experiments)

### 5.1. Implementation details



### 5.2. Datasets and evaluation setups



### 5.3. Study of negative pair sampling strategy



### 5.4. Scale matters



### 5.5. Comparison with stateoftheart



**CrossTask**


**YouCook2**



**MSR-VTT**



**LSMDC**




### 5.6. Cross-dataset fine-tuning evaluation



### 5.7. Qualitative results



---

## 6. 결론(Conclusion)


---

## 참고문헌(References)

논문 참조!

--- 

