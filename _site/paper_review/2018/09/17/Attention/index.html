<!DOCTYPE html>
<html lang="en-us">
<head>
  <head>
  <!-- Description of Blog -->
  <meta name="description" content="Python, Machine & Deep Learning">
  <link rel="canonical" href="https://greeksharifa.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Python, Machine & Deep Learning">
  <meta property="og:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta property="og:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta property="og:url" content="https://greeksharifa.github.io/">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python, Machine & Deep Learning">
  <meta name="twitter:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta name="twitter:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta name="twitter:domain" content="https://greeksharifa.github.io/">

  <!-- link -->
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Attention
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="icon-144x144" sizes="144x144" href="/public/img/icon-144x144.png">
  <link rel="shortcut icon" href="/public/img/icon_32x32.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  

  <!-- Ads -->
  <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
  </script>
</head>

  <!-- for Google AdSense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9951774327887666",
    enable_page_level_ads: true
  });
</script>

  <style>blockquote {
    font-size: 1em;
    line-height: 1.4
  }</style>
  <link href='http://fonts.googleapis.com/css?family=Gill+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Consolas' rel='stylesheet' type='text/css'>
</head>
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/3c2986ad7ac1f2230ea3596f44563328">
          <img src="/public/img/maple_tree.jpg" title="Cover Photo" alt="Maple tree" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>Developer and Analyst</strong>, YW & YY.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/greeksharifa">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:greeksharifa@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://greeksharifa.github.io/">
          Github Project
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 YW & YY. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://greeksharifa.github.io">YW & YY</a>
    </p>
  </div>
</div>


<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
     content to avoid any CSS collisions with our real content. -->
<div class="wrap">
  <div class="masthead">
    <div class="container">
      <h3 class="masthead-title" align="center">
        <a href="/" title="Home" title="YW & YY">
          <img class="masthead-logo" src="/public/img/logo.png"/>
        </a>
        <small>YW & YY's Python, Machine & Deep Learning</small>
        <!-- HTML elements for search -->
        <a href="/search/" id="search_icon">
          <img src="/public/img/search.png" width="25" height="25"
               align="right" style="margin-top:5px; margin-bottom:0;"
               onmouseover="this.style.opacity=0.7" onmouseout="this.style.opacity=0.5"
               alt="search">
        </a>
      </h3>
    </div>
  </div>

  <div class="container content">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.9/styles/github.min.css"> 
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.9/highlight.min.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 수직형 디스플레이 광고1 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-9951774327887666"
     data-ad-slot="7237421728"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<div class="post">
  <h1 class="post-title">Attention</h1>
  <span class="post-date">17 Sep 2018</span>
   |
  
  <a href="/blog/tags/#nlp" class="post-tag">NLP</a>
  
  <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
  
  
  <article>
    <p><strong>목차</strong></p>
    <ul>
  <li><a href="#neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#background-nmt">Background: NMT</a></li>
  <li><a href="#learning-to-align-and-translate">Learning to align and translate</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#details-about-model-architecture">Details about Model Architecture</a></li>
</ul>

    <h3 id="neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural Machine Translation by Jointly Learning to Align and Translate</h3>
<blockquote>
  <p>본 글은 Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio가 2014년에 Publish한 위 논문을 리뷰한 것이다.</p>
</blockquote>

<h3 id="introduction">Introduction</h3>
<p>Basic Encoder-Decoder 모델은 source sentence의 모든 정보를 fixed-length vector로 압축하는 방식을 골자로 한다.<br />
그러나 이 모델은 긴 문장을 대상으로 할 때 어려움을 겪는 것이 일반적이다.<br />
이를 해결하기 위해 본 논문에서 제안된 새로운 모델은 <strong>target word</strong>를 예측하는 것과 관련된 source sentence의 부분을 자동적으로 soft-search하여 이와 같은 문제를 해결해낸다.<br />
(learns to align and translate jointly)</p>

<blockquote>
  <p>encodes the input sentence into sequence of vectors and<br />
chooses a subset of these vectors adaptively while decoding the translation.</p>
</blockquote>

<h3 id="background-nmt">Background: NMT</h3>
<p>translation의 핵심은 source sentence <strong>x</strong>가 주어졌을 때의 <strong>y</strong>의 조건부확률을 최대화하는 target sentence <strong>y</strong>를 찾는 것이다.</p>

<script type="math/tex; mode=display">arg\max_{y} p(\vec{y}|x)</script>

<p>번역 모델에 의해 이러한 조건부 분포가 학습되면, source sentence가 주어졌을 때 상응하는 번역된 문장은 위의 조건부 확률을 최대화하는 문장을 찾음으로써 generate된다.</p>

<p><em>RNN Encoder-Decoder</em>는 이전 리뷰에서 다룬 적이 있으므로 생략하도록 한다.</p>

<h3 id="learning-to-align-and-translate">Learning to align and translate</h3>
<p>모델의 구성 요소를 하나하나 살펴보기 이전에 notation에 관한 정리를 진행하겠다.</p>

<p>$y_i$: i time에서의 target word<br />
$s_i$: i time에서의 디코더 Hidden State<br />
$c_i$: i time에서의 context vector = annotations의 가중합<br />
$\alpha_{ij}$: attention weight = normalized score = 연결 확률<br />
$h_j$: j time에서의 인코더 Hidden State = annotations<br />
$e_{ij}$: attention score = unnormalized score<br />
$f, g$ = 비선형 함수</p>

<p>명확히 하자면, subscript <strong>i</strong>는 디코더를 명시하며, subscript <strong>j</strong>는 인코더를 명시한다.</p>

<p>이제 모델의 구성 요소를 살펴볼 것이다.<br />
먼거 타겟 word $y_i$를 예측하기 위한 조건부 확률은 아래와 같이 정의된다.</p>

<script type="math/tex; mode=display">p(y_i|y_1, ..., y_{i-1}, \vec{x}) = g(y_{i-1}, s_i, c_i)</script>

<p>이 중 디코더의 i time Hidden State인 $s_i$를 먼저 살펴보면,</p>

<script type="math/tex; mode=display">s_i = f(s_{i-1}, y_{i-1}, c_i)</script>

<p>Basic Encoder-Decoder 모델과 달리 target word를 예측하기 위한 조건부 확률은 분리된 context vector $c_i$에 의존한다.</p>

<p><strong>Context Vector</strong> $c_i$는 annotations $h_j$의 가중합이다.</p>

<script type="math/tex; mode=display">c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j</script>

<p>여기서 $h_j$는 j time annotation으로, input sequence의 i번째 단어 주위 부분에 강하게 집중하여 input sequence에 대한 정보를 담게 된다.</p>

<p><em>Bidirectional RNN</em><br />
이 $h_j$는 forward RNN의 Hidden States와 backward RNN의 Hidden States를 세로로 합친 열벡터이다.</p>

<script type="math/tex; mode=display">h_j = [\overrightarrow{h_j}^T | \overleftarrow{h_j}^T]^T</script>

<p>이러한 방식으로 $h_j$는 두 방향 모두로 words들을 요약한 정보를 담게 된다.</p>

<p>이제 <strong>attention weight</strong> $a_{ij}$가 어떻게 계산되는지 살펴보겠다.</p>

<script type="math/tex; mode=display">a_{ij} = \frac{ exp(e_{ij}) } {\sum_{k=1}^{T_x} exp(e_{ik}) }</script>

<p>이 $a_{ij}$는 <strong>Normalized Score</strong>라고 할 수 도 있다. 왜냐하면 softmax함수의 확률로서 계산되기 때문이다.</p>

<p><strong>Unnormalized Score</strong>인 $e_{ij}$는 아래와 같이 계산된다.</p>

<script type="math/tex; mode=display">e_{ij} = a(s_{i-1}, h_j)</script>

<p>여기서 a함수는 <strong>alignment model</strong>이다. 이 a를 다른 component와 함께 학습되는 순전파 신경망으로서 모수화한다.<br />
이 alignment model은 j time 인풋이 i time 아웃풋과 얼마나 유사한지를 평가하게 된다.<br />
또한 이 모델은 잠재변수로 설정되지 않고, soft alignment를 직접적으로 계산하여 cost function의 gradient가 역전파될 수 있도록 하게 만든다.<br />
계산 방법은 마지막 부분에서 설명하도록 하겠다.</p>

<p>위 설명을 보면, 결국 i번째 <strong>Context Vector</strong>인 $c_i$는 expected annotation over all the annotations with probabilities $\alpha_{ij}$라고 할 수 있다.<br />
이 $\alpha_{ij}$는 다음 Hidden State인 $s_i$를 결정하고 target word $y_i$를 generate하는 데에 있어 $h_j$의 중요성을 결정하는 역할을 하게 된다.</p>

<p>즉 이는 일종의 <strong>attention</strong>이라는 개념으로 설명될 수 있다.<br />
디코더는 source sentence의 어떤 부분에 <strong>집중</strong>해야 하는지 결정하게 되는 것이다.</p>

<h3 id="conclusion">Conclusion</h3>
<p>제안된 모델은 다음 target word를 generate하는 데에 관련이 있는 정보에만 집중하며 이 때문에 source sentence의 길이에 상당히 robust하다.<br />
다만 unknown or rare words를 다루는 데 있어서는 좀 더 보완이 필요하다.</p>

<h3 id="details-about-model-architecture">Details about Model Architecture</h3>
<p>이 부분에서는 Appendix에 나와 있는 수식들을 종합하여, 본 논문에서 제안한 RNNSearch라는 모델의 구조에 대해 정리하도록 하겠다.<br />
논문이 굉장히 친절하여 Matrix의 차원이 정확하고 자세하게 나와있으므로 반드시 참고할 필요가 있다.</p>

<ol>
  <li>
    <p>상수에 대한 설명은 아래와 같다.<br />
$m$: word embedding 차원, 본 모델에선 620 <br />
$n$: 인코더/디코더 Hidden Units의 수, 본 모델에선 1000<br />
$n’$: Alignment Model 내에서의 Hidden Units의 수, 본 모델에선 1000<br />
$l$: , 본 모델에선 500<br />
$T_x$: source sentence의 길이<br />
$K_x$: source language의 vocab_size</p>
  </li>
  <li>
    <p>벡터들의 크기는 아래와 같다.<br />
$y_i$: (k, 1)<br />
$s_i$: (n, 1)<br />
$h_i$: (n, 1)<br />
$v_a$: (n’, 1)<br />
$z_i$: (n, 1)<br />
$r_i$: (n, 1)</p>
  </li>
  <li>
    <p>행렬들의 크기는 아래와 같다. W, U, C는 모두 Parameter Matrix이다.<br />
$X$: ($T_x$, $K_x$)<br />
$Y$: ($T_y$, $K_y$)<br />
$E$: (m, K), x와 결합할 때는 $K_x$, y와 결합할 때는 $K_y$<br />
$W$: (n, m), $W, W_z, W_r$에 한정<br />
$W_a$: (n’, n), Alignment 모델에서 사용<br />
$U$: (n, n), $U, U_z, U_r$에 한정<br />
$U_a$: (n’, 2n), Alignment 모델에서 사용<br />
$C$: (n, 2n), $C, C_z, C_r$에 한정</p>
  </li>
</ol>

<p><strong>Encoder</strong><br />
source sentence Matrix <strong>X</strong>는 번역 대상인 하나의 문장을 뜻한다.<br />
각각의 열벡터는 $\vec{x_i}$로 표기되며 이 벡터의 크기는 $K_x$로,<br />
source language의 vocab_size를 의미한다.</p>

<p>인코더의 Bidirectional RNN은 아래와 같이 계산된다.<br />
(Bias항은 잠시 삭제한다.)</p>

<script type="math/tex; mode=display">h_j = (1 - z_i) \odot h_{j-1} + z_i \odot \tilde{h_j}</script>

<p>위에서 $z_i$가 Update Gate이며, 각 Hidden State가 이전 activation을 유지하느냐 마느냐를 결정한다.</p>

<script type="math/tex; mode=display">\tilde{h_j} = tanh(W*Ex_j + U[r_j \odot h_{j-1}])</script>

<p>위에서 $r_j$가 Reset Gate이며, 이전 State의 정보를 얼마나 Reset할지 결정한다.</p>

<script type="math/tex; mode=display">z_j = \sigma(W_z * Ex_j + U_z * h_{j-1})</script>

<script type="math/tex; mode=display">r_j = \sigma(W_r * Er_j + U_x * h_{j-1})</script>

<p>위에서 계산한 식은 $\overrightarrow{h_j}$, $\overleftarrow{h_j}$ 모두에게 통용되며,<br />
이를 stack하여 annotation $h_j$를 만들게 되는 것이다.</p>

<p><strong>Decoder</strong><br />
디코더의 Hidden State인 $s_i$ 역시 계산 방식은 유사하다.</p>

<script type="math/tex; mode=display">s_i = (1 - z_i) \odot s_{i-1} + z_i \odot \tilde{s_i}</script>

<script type="math/tex; mode=display">\tilde{s_i} = tanh(W*Ex_i + U[r_i \odot s_{i-1}] + C*c_i)</script>

<script type="math/tex; mode=display">z_i = \sigma(W_z * Ex_i + U_z * s_{i-1} + C_zc_i)</script>

<script type="math/tex; mode=display">r_i = \sigma(W_r * Er_j + U_x * s_{i-1} + C_rc_i)</script>

<script type="math/tex; mode=display">c_i = \sum_{j=1}^{T_x}a_{ij}h_j</script>

<script type="math/tex; mode=display">a_{ij} = \frac{ exp(e_{ij}) } {\sum_{k=1}^{T_x} exp(e_{ik}) }</script>

<script type="math/tex; mode=display">e_{ij} = v_a^T tanh(W_a * s_{i-1} + U_a * h_j)</script>

<p>최종적으로 Decoder State $s_{i-1}$, Context Vector $c_i$, 마지막 generated word $y_{i-1}$을 기반으로, target word $y_i$의 확률을 아래와 같이 정의한다.</p>

<script type="math/tex; mode=display">p(y_i|s_i, y_{i-1}, c_i) \propto exp(y_i^T W_o t_i)</script>

<p>즉 오른쪽 편에 있는 스칼라값에 정비례한다는 뜻이다.<br />
잠시 행렬의 차원을 정의하고 진행하겠다.</p>

<p>$W_o$: ($K_y$, $l$)<br />
$U_o$: ($2l$, n)<br />
$V_o$: ($2l$, m)<br />
$C_o$: ($2l$, 2n)<br />
이들은 모두 Parameter이다.</p>

<p>이제 $t_i$를 정의할 것인데, 그 전에 두 배 크기인 candidate $\tilde{t_i}$를 먼저 정의하겠다.</p>

<script type="math/tex; mode=display">\tilde{t_i} = U_o * s_{i-1} + V_o * Ey_{i-1} + C_oc_i</script>

<p>차원을 맞춰보면 위 벡터는 크기가 ($2l$, 1)인 것을 알 수 있을 것이다.<br />
이제 이 벡터에서 아래와 같은 maxout과정을 거치면,</p>

<center><img src="/public/img/Paper_Review/2018-09-27-Attention/a1.png" width="50%" /></center>

<p>$t_i$는 아래와 같이 정의된다.</p>

<script type="math/tex; mode=display">t_i = [ max(\tilde{t_{i, 2j-1}}, \tilde{t_{i, 2j}}) ]_{j=1, ..., l}^T</script>

<p>아주 멋지다.<br />
<strong>The End</strong></p>

  </article>
  <script type="text/javascript" async
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  <script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

</div>

<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-9951774327887666">
</amp-auto-ads>

<script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-9951774327887666"
     data-ad-slot="6606866336"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
    <li>
      <h3>
        <a href="/github/2020/05/27/github-usage-09-overall/">
          GitHub 사용법 - 09. Overall
          <small>27 May 2020</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/generative/model/2020/05/25/VAE/">
          Variational AutoEncoder 설명
          <small>25 May 2020</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/machine_learning/2020/05/01/AFM/">
          추천 시스템의 기본 - 06. AFM 논문 리뷰 및 Tensorflow 구현
          <small>01 May 2020</small>
        </a>
      </h3>
    </li>
    
  </ul>
</div>

<div id="disqus_thread"></div>
<script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

  var disqus_config = function () {
    this.page.url = 'http://localhost:4000/paper_review/2018/09/17/Attention/';
    this.page.identifier = 'http://localhost:4000/paper_review/2018/09/17/Attention/';
    //this.page.url = 'https://greeksharifa.github.com/';  // Replace PAGE_URL with your page's canonical URL variable
    //this.page.identifier = 'greeksharifa'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };

  (function () { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://greeksharifa.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by
  Disqus.</a></noscript>

  </div>
</div>

<label for="sidebar-checkbox" class="sidebar-toggle"></label>

<script>
  (function (document) {
    let toggle = document.querySelector('.sidebar-toggle');
    let sidebar = document.querySelector('#sidebar');
    let checkbox = document.querySelector('#sidebar-checkbox');

    document.addEventListener('click', function (e) {
      let target = e.target;

      if (target === toggle) {
        checkbox.checked = !checkbox.checked;
        e.preventDefault();
      } else if (checkbox.checked && !sidebar.contains(target)) {
        /* click outside the sidebar when sidebar is open */
        checkbox.checked = false;
      }
    }, false);
  })(document);
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    };
    i[r].l = 1 * new Date();
    a = s.createElement(o);
    m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-00000000-1', 'auto');
  ga('send', 'pageview');
</script>


<!-- Naver Analytics -->	
<script type="text/javascript" src="//wcs.naver.net/wcslog.js"></script>
<script type="text/javascript">
  if(!wcs_add) var wcs_add = {};
    wcs_add["wa"] = "18cbce78e94161";
  wcs_do();
</script>

</body>

<script id="dsq-count-scr" src="//greeksharifa-github-io.disqus.com/count.js" async></script>

</html>
