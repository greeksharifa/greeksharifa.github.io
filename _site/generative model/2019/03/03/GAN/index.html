<!DOCTYPE html>
<html lang="en-us">
<head>
  <head>
  <!-- Description of Blog -->
  <meta name="description" content="Python, Machine & Deep Learning">
  <link rel="canonical" href="https://greeksharifa.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Python, Machine & Deep Learning">
  <meta property="og:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta property="og:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta property="og:url" content="https://greeksharifa.github.io/">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python, Machine & Deep Learning">
  <meta name="twitter:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta name="twitter:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta name="twitter:domain" content="https://greeksharifa.github.io/">

  <!-- link -->
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      GAN(Generative Adversarial Networks)
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="icon-144x144" sizes="144x144" href="/public/img/icon-144x144.png">
  <link rel="shortcut icon" href="/public/img/icon_32x32.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  

  <!-- Ads -->
  <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
  </script>
</head>

  <!-- for Google AdSense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9951774327887666",
    enable_page_level_ads: true
  });
</script>

  <style>blockquote {
    font-size: 1em;
    line-height: 1.4
  }</style>
  <link href='http://fonts.googleapis.com/css?family=Gill+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Consolas' rel='stylesheet' type='text/css'>
</head>
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/3c2986ad7ac1f2230ea3596f44563328">
          <img src="/public/img/maple_tree.jpg" title="Cover Photo" alt="Maple tree" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>Developer and Analyst</strong>, YW & YY.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/greeksharifa">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:greeksharifa@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://greeksharifa.github.io/">
          Github Project
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 YW & YY. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://greeksharifa.github.io">YW & YY</a>
    </p>
  </div>
</div>


<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
     content to avoid any CSS collisions with our real content. -->
<div class="wrap">
  <div class="masthead">
    <div class="container">
      <h3 class="masthead-title" align="center">
        <a href="/" title="Home" title="YW & YY">
          <img class="masthead-logo" src="/public/img/logo.png"/>
        </a>
        <small>YW & YY's Python, Machine & Deep Learning</small>
        <!-- HTML elements for search -->
        <a href="/search/" id="search_icon">
          <img src="/public/img/search.png" width="25" height="25"
               align="right" style="margin-top:5px; margin-bottom:0;"
               onmouseover="this.style.opacity=0.7" onmouseout="this.style.opacity=0.5"
               alt="search">
        </a>
      </h3>
    </div>
  </div>

  <div class="container content">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.9/styles/github.min.css"> 
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.9/highlight.min.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 수직형 디스플레이 광고1 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-9951774327887666"
     data-ad-slot="7237421728"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<div class="post">
  <h1 class="post-title">GAN(Generative Adversarial Networks)</h1>
  <span class="post-date">03 Mar 2019</span>
   |
  
  <a href="/blog/tags/#gan" class="post-tag">GAN</a>
  
  <a href="/blog/tags/#machine-learning" class="post-tag">Machine Learning</a>
  
  <a href="/blog/tags/#cnn" class="post-tag">CNN</a>
  
  <a href="/blog/tags/#generative-model" class="post-tag">Generative Model</a>
  
  <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
  
  
  <article>
    <p><strong>목차</strong></p>
    <ul>
  <li><a href="#generative-adversarial-networksgan">Generative Adversarial Networks(GAN)</a>
    <ul>
      <li><a href="#초록abstract">초록(Abstract)</a></li>
      <li><a href="#서론introduction">서론(Introduction)</a></li>
      <li><a href="#관련-연구related-works">관련 연구(Related Works)</a></li>
      <li><a href="#적대적-망adversarial-nets">적대적 망(Adversarial nets)</a></li>
      <li><a href="#이론적-결과theoretical-results">이론적 결과(Theoretical Results)</a></li>
      <li><a href="#실험experiments">실험(Experiments)</a></li>
      <li><a href="#장단점advantages-and-disadvantages">장단점(Advantages and disadvantages)</a>
        <ul>
          <li><a href="#단점">단점</a></li>
          <li><a href="#장점">장점</a></li>
        </ul>
      </li>
      <li><a href="#결론-및-추후-연구conclusions-and-future-work">결론 및 추후 연구(Conclusions and future work)</a></li>
      <li><a href="#참고문헌references">참고문헌(References)</a></li>
    </ul>
  </li>
  <li><a href="#보충-설명">보충 설명</a>
    <ul>
      <li><a href="#목적함수">목적함수</a>
        <ul>
          <li><a href="#목적함수-최적화의-의미">목적함수 최적화의 의미</a></li>
        </ul>
      </li>
      <li><a href="#학습-방법">학습 방법</a></li>
      <li><a href="#단점-및-극복방안">단점 및 극복방안</a>
        <ul>
          <li><a href="#mode-collapsing">Mode Collapsing</a></li>
          <li><a href="#oscillation">Oscillation</a></li>
          <li><a href="#g와-d-사이의-imbalance">G와 D 사이의 Imbalance</a></li>
          <li><a href="#해결방안">해결방안</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#튜토리얼">튜토리얼</a>
    <ul>
      <li><a href="#50줄로-짜보는-튜토리얼">50줄로 짜보는 튜토리얼</a></li>
      <li><a href="#mnist-튜토리얼">MNIST 튜토리얼</a></li>
    </ul>
  </li>
  <li><a href="#이후-연구들">이후 연구들</a></li>
</ul>

    <hr />

<p>이 글에서는 2014년 6월 <em>Ian J. Goodfellow</em> 등이 발표한 Generative Adversarial Networks(GAN, 생성적 적대신경망)를 살펴보도록 한다.</p>

<p>간단히 GAN은 두 가지 모델을 동시에 학습시키는 구조이다. G(Generator, 생성자)라는 모델은 직접 볼 수 없는 진짜 데이터와 최대한 비슷하게 생긴 가짜 데이터를 만드려고 하고, D(Distriminator, 식별자 또는 감별자)라는 모델은 자신에게 주어진 데이터가 진짜 데이터인지 가짜 데이터인지 최대한 구분하려고 한다.</p>

<p>GAN을 도식화한 구조는 다음과 같다. <a href="https://github.com/hwalsuklee/tensorflow-generative-model-collections">출처</a></p>

<center><img src="/public/img/2019-03-03-GAN/04.PNG" width="50%" /></center>

<p>논문에서는 설명을 위한 예시로 화폐 위조범($G$)와 경찰($D$)을 제시하였다. 다만 차이가 있다면,</p>
<ul>
  <li>위조범은 진짜를 볼 수 없다는 것(그래서 장님blind라 불린다)</li>
  <li>경찰은 자신이 판별한 결과를 위조범에게 알려준다
는 것이 있다.</li>
</ul>

<p><em>참고</em>: $G$로 들어가는 입력 벡터를 뜻하는 noise는 latent variable이라고도 하며, Auto-encoder에서 출력 영상을 만들기 위한 source와 비슷하기에 이 표현도 사용된다.<br />
또 GAN은 특정한 모델 구조를 가진 것이 아니므로 코드가 특별히 정해진 것은 아니다.</p>

<p>논문을 적절히 번역 및 요약하는 것으로 시작한다. 많은 부분을 생략할 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.</p>

<hr />

<h1 id="generative-adversarial-networksgan">Generative Adversarial Networks(GAN)</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a></strong></p>

<h2 id="초록abstract">초록(Abstract)</h2>

<p>이 논문에서는 적대적으로 동작하는 두 생성 모델을 동시에 학습시키는 새 framework를 제안한다. 생성자 G는 원본 data distribution을 흉내내려 하고, D는 눈앞의 데이터가 G에게서 온 것인지를 판별한다. G의 목적은 D가 최대한 실수하게 만드는 것이고, D는 당연히 최대한 정확하게 진짜/가짜를 판별하는 것이다.<br />
이는 2인 minimax 게임과 비슷하다. 어떤 유일한 해가 존재하여 최종적으로 D는 실수할 확률이 0.5가 된다(즉 찍는 수준).<br />
G와 D가 multi-layer perceptron으로 구성되면 전체 시스템은 backpropagation으로 학습될 수 있다.<br />
GAN에는 어느 과정에서든 마르코프 체인이나 기타 다른 네트워크가 필요가 전혀 없다.</p>

<hr />

<h2 id="서론introduction">서론(Introduction)</h2>

<p><strong><em>적대적</em></strong>인 두 네트워크를 학습시킨다. D는 원본 data distribution인지 G에서 온 것인지를 판별하고, G는 D가 실수하도록 가짜 데이터를 잘 만들어내는 것이 목표이다.<br />
이 framework는 많은 특별한 학습 알고리즘과 optimizer를 사용할 수 있다. 앞서 말한 대로 multi-layer perception을 쓰면 다른 복잡한 네트워크는 필요 없이 오직 forward/backpropagation만으로 (이 논문에서는 dropout을 또 쓴다) 학습이 가능하다.</p>

<hr />

<h2 id="관련-연구related-works">관련 연구(Related Works)</h2>

<p>궁금하면 읽어보자.</p>
<ul>
  <li>RBMs: restricted Boltzmann machines, 잠재 변수를 가진 유향 그래프 모델에 대한 대안으로, 무향 그래프 모델</li>
  <li>DBMs: deep Boltzmann machines, RBMs와 비슷함. 다양한 변형이 존재</li>
  <li>MCMC: Markov chain Monte Carlo methods, 위 모델의 측정 방법</li>
  <li>DBNs: Deep belief networks, 하나의 무향 레이어와 여러 유향 레이어의 hybrid 모델. 계삭적 문제가 있음</li>
  <li>NCE: noise-contrasive estimation, log-likelihood를 근사하거나 경계값을 구하지 않는 방법</li>
  <li>GSN: generative stochastic network, 확률분포를 명시적으로 정의하지 않고 분포 샘플을 생성하도록 학습시키는 방법을 사용</li>
  <li><strong>adversarial nets</strong>: 적대적 망은 생성 중 feedback loop를 필요로 하지 않아 sampling에서 Markov chain이 필요가 없다. 이는 backpropagation 성능 향상으로 이어진다.</li>
  <li>auto-encoding varitional Bayes와 stochastic backpropagation은 생성 머신을 학습시키는 방법들 중 하나이다.</li>
</ul>

<hr />

<h2 id="적대적-망adversarial-nets">적대적 망(Adversarial nets)</h2>

<table>
  <thead>
    <tr>
      <th>기호</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$x$</td>
      <td>데이터</td>
    </tr>
    <tr>
      <td>$p_g$</td>
      <td>$x$에 대한 생성자의 분포</td>
    </tr>
    <tr>
      <td>$p_z(z)$</td>
      <td>input noise 변수</td>
    </tr>
    <tr>
      <td>$\theta_g$</td>
      <td>multilayer perceptrions의 parameters</td>
    </tr>
    <tr>
      <td>$G$</td>
      <td>$\theta_g$에 의해 표현되는 미분가능한 함수</td>
    </tr>
    <tr>
      <td>$G(z; \theta_g$)</td>
      <td>data space에 대한 mapping</td>
    </tr>
    <tr>
      <td>$D(x)$</td>
      <td>$x$가 $p_g$가 아니라 원본 데이터에서 나왔을 확률</td>
    </tr>
    <tr>
      <td>$D(x; \theta_d)$</td>
      <td>두 번째 multilayer perceptron</td>
    </tr>
  </tbody>
</table>

<p>D의 목적은 데이터가 ‘원본’인지 ‘G가 생성한 데이터’인지 판별하는 것이므로 어떤 데이터에 대해 정확한 label(‘원본’ 또는 ‘G로부터’)을 붙이는 것이다. G의 목적은 D가 실수하게 만드는 것, 즉 어떤 데이터가 주어졌을 때 D가 ‘원본’이라고 판별할 확률과 ‘G로부터 나온 데이터’라고 판별할 확률을 모두 높이는 것(정확히는 같게)이다.<br />
즉 $log(1-D(G(z)))$를 최소화하도록 G를 훈련시킨다.</p>

<p>D와 G 모두에 대해 value function $V(G, D)$를 정의하면,</p>

<script type="math/tex; mode=display">min_G max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[log D(x)] + \mathbb{E}_{x \sim p_{z}(z)}[log (1-D(G(z)))]</script>

<p>위 식의 의미는,</p>
<ul>
  <li>$min_G$: G는 V를 최소화하려고 한다.</li>
  <li>$max_D$: D는 V를 최대화하려고 한다. 2-player minimax 게임과 같으므로 당연하다.</li>
  <li>$\mathbb{E}$: 기댓값</li>
  <li>$x \sim p_{data}(x)$: $x$가 원본 데이터 분포에서 왔을 때</li>
</ul>

<p>D가 <em>아주 똑똑한 경찰</em>이라면, $x$가 실제로 원본에서 온 것이라면 $D(x)=1$이 될 것이고, $G(z)$에서 온 것이라면 $D(G(z))=0$이 된다. 만약 G가 <em>완벽한 위조범</em>이 되었다면, $D(x) = {1 \over 2}$이다.<br />
따라서 D의 입장에서 V의 최댓값은 0이 되며, G의 입장에서 V의 최솟값은 $-\infty$임을 알 수 있다.</p>

<p>학습시킬 때, inner loop에서 D를 최적화하는 것은 매우 많은 계산을 필요로 하고 유한한 데이터셋에서는 overfitting을 초래하기 때문에, $k$ step만큼 D를 최적화하고 G는 1 step만 최적화하도록 한다.<br />
학습 초반에는 G가 형편없기 때문에 D는 진짜인지 G가 생성한 것인지를 아주 잘 구분해 낸다.<br />
또 G가 $log(1-D(G(z)))$를 최소화하도록 하는 것보다는 $log(D(G(z)))$를 최대화하도록 하는 것이 더 학습이 잘 된다. 이는 G가 형편없을 때는 $log(1-D(G(z)))$의 gradient를 계산했을 때 너무 작은 값이 나와 학습이 느리기 때문이라고 한다.</p>

<center><img src="/public/img/2019-03-03-GAN/01.PNG" width="100%" /></center>

<p>파란 점선은 disctiminative distribution(D), 검정색은 원본 데이터($p_x$), 초록색은 생성된 분포$p_g$(G), $x$는 원본 데이터 분포를, 화살표는 $x=G(z)$ mapping을 나타낸다. (a) 초기 상태. (b) D 학습 후, (c) G 학습 후, 분포가 비슷해지는 것을 볼 수 있다. (d) 여러 번의 학습 끝에 G가 완전히 원본을 흉내낼 수 있는 경지에 도달함. 즉 $p_g = p_{data}$. D는 이제 진짜인지 가짜인지 구분할 수 없다. 즉 $D(x) = {1 \over 2}$.</p>

<hr />

<h2 id="이론적-결과theoretical-results">이론적 결과(Theoretical Results)</h2>

<p>수학을 좋아한다면 직접 읽어보자.</p>
<ul>
  <li>Algorithm 1
    <ul>
      <li>for epochs do
        <ul>
          <li>for k steps do
            <ul>
              <li>noise prior $p_g(z)$로부터 $m$개의 noise sample $z^{(1)}, …, z^{(m)}$을 뽑는다.</li>
              <li>noise prior $p_{data}(x)$로부터 $m$개의 noise sample $x^{(1)}, …, x^{(m)}$을 뽑는다.</li>
              <li>D를 다음 stochastic gradient로 update한다. (ascending)
                <ul>
                  <li>$ \nabla_{\theta_d} {1 \over m} \sum^m_{i=1} [log D(x^{(i)}) + log (1-D(G(z^{(i)})))] $</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>noise prior $p_g(z)$로부터 $m$개의 noise sample $z^{(1)}, …, z^{(m)}$을 뽑는다.</li>
          <li>G를 다음 stochastic gradient로 update한다. (descending)
            <ul>
              <li>$ \nabla_{\theta_d} {1 \over m} \sum^m_{i=1} [log (1-D(G(z^{(i)})))] $</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>이 minimax 게임은 $p_g = p_{data}$에 대한 global optimum을 가진다.
    <ul>
      <li>G를 고정했을 때, optimal한 D는 $ D^*<em>G(x) = {p</em>{data}(x) \over p_{data}(x) + p_g(x)} $</li>
    </ul>
  </li>
  <li>Algorithm 1은 수렴한다.</li>
</ul>

<hr />

<h2 id="실험experiments">실험(Experiments)</h2>

<p>MNIST, Toronto Face Database(TFD), CIFAR-10에 대해 학습을 진행했다.</p>

<ul>
  <li>G는 rectifier linear activations와 sigmoid를 사용했고, D는 maxout activations를 사용했다.</li>
  <li>Dropout은 D를 학습시킬 때 사용했다.</li>
  <li>noise는 G에서 가장 밑의 레이어에만 input으로 넣었다.</li>
</ul>

<p>자세한 실험 조건은 직접 읽어보자.</p>

<center><img src="/public/img/2019-03-03-GAN/02.PNG" width="100%" /></center>

<p>가장 오른쪽 열은 바로 옆에 있는 생성된 이미지와 가장 비슷한 학습 샘플이다. a) MNIST b) TFD c) CIFAR-10(fully connected model) d) CIFAR-10(convolutional D와 “deconvolutional” G)</p>

<center><img src="/public/img/2019-03-03-GAN/03.PNG" width="100%" /></center>

<p>숫자 간 보간을 했을 때는 위와 같이 된다. 물론 GAN을 통해 생성한 것이다.</p>

<hr />

<h2 id="장단점advantages-and-disadvantages">장단점(Advantages and disadvantages)</h2>

<h3 id="단점">단점</h3>

<ul>
  <li>$p_g(x)$가 명시적으로 존재하지 않는다.</li>
  <li>D는 G와 균형을 잘 맞추어서 성능이 향상되어야 한다(G는 D가 발전하기 전 너무 발전하면 안 된다).</li>
</ul>

<h3 id="장점">장점</h3>
<ul>
  <li>마르코프 체인이 전혀 필요 없이 backprop만으로 학습이 된다.</li>
  <li>특별히 어떤 추론(inference)도 필요 없다.</li>
  <li>다양한 함수들이 모델에 접목될 수 있다.</li>
  <li>마르코프 체인을 썼을 때에 비해 훨씬 선명한(sharp) 이미지를 결과로 얻을 수 있다.</li>
</ul>

<hr />

<h2 id="결론-및-추후-연구conclusions-and-future-work">결론 및 추후 연구(Conclusions and future work)</h2>

<ol>
  <li>conditional generative model로 발전시킬 수 있다(CGAN).</li>
  <li>Learned approximate inference는 $x$가 주어졌을 때 $z$를 예측하는 보조 네트워크를 학습함으로써 수행될 수 있다.</li>
  <li>parameters를 공유하는 조건부 모델을 학습함으로써 다른 조건부 모델을 대략 모델링 할 수 있다. 특히, deterministic MP-DBM의 stochastic extension의 구현에 대부분의 네트워크를 쓸 수 있다.</li>
  <li>Semi-supervised learning에도 활용 가능하다. classifier의 성능 향상을 꾀할 수 있다.</li>
  <li>효율성 개선: G와 D를 조정하는 더 나은 방법이나 학습하는 동안 sample $z$에 대한 더 나은 distributions을 결정하는 등의 방법으로 속도를 높일 수 있다.</li>
</ol>

<h2 id="참고문헌references">참고문헌(References)</h2>

<p>논문 참조!</p>

<hr />

<h1 id="보충-설명">보충 설명</h1>

<h2 id="목적함수">목적함수</h2>

<p>D의 목적함수는 G를 고정한 채로 진짜 데이터 $m$개와 가짜 데이터 $m$개를 D에 넣고, G에 대한 V를 계산한 뒤 gradient를 구하고 V를 높여 D를 최종적으로 업데이트한다.</p>

<script type="math/tex; mode=display">max_D V(D) = {1 \over m } \sum^m_{i=1} log D(x^i) + {1 \over m } \sum^m_{i=1} log D(1 - D(G(z^i)))</script>

<p>G의 목적함수는 D를 고정한 채로 가짜 데이터 $m$개를 생성해 V을 계산한 뒤, G에 대한 V의 gradient를 계산하고 V를 낮춰 G를 업데이트한다.<br />
G의 목적함수는 gradient가 0에 가까워지는 것을 막기 위해 논문에서 언급된 팁을 반영한 것이다.</p>

<script type="math/tex; mode=display">min_G V(G) = {1 \over m} \sum^m_{j=1} log(D(G(z^j)))</script>

<h3 id="목적함수-최적화의-의미">목적함수 최적화의 의미</h3>

<p>Machine Learning 관점에서 보면 모델이 loss가 최소화되는 parameter를 찾아가는 과정이다.<br />
또는 진짜 데이터의 분포와 G가 생성한 가짜 데이터 분포 사이의 차이를 줄이는 것과도 같다.</p>

<p>수학적으로는 D가 이미 최적이라는 가정 하에, GAN이 목적함수를 최적화한다는 과정($p_{data}$와 $p_g$를 똑같이 만드려는 것)은 $p_{data}$와 $p_g$ 사이의 <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence">Jensen-Shannon divergence(JSD)</a>를 최소화하는 것과 같다.<br />
JSD는 <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence</a>의 대칭(symmetrized and smoothed) 버전이다. 그래서 GAN은 KLD를 최소화하는 것이라고 말하기도 한다.</p>

<p>분포 $P$와 $Q$에 대해, $KLD = D(P \Vert Q), M = {1 \over 2}(P+Q)$라 할 때, JSD는</p>

<script type="math/tex; mode=display">JSD(P \Vert Q) = {1 \over 2} D(P \Vert M) + {1 \over 2} D(Q \Vert M)</script>

<p>이다.</p>

<hr />

<h2 id="학습-방법">학습 방법</h2>

<p>GAN은 서로 경쟁하는 두 가지 모델을 학습시킨다. GAN을 쓰려면 다음 방법을 따른다.</p>

<ol>
  <li>우선 다음을 정의한다.
    <ol>
      <li>R(Real): 실제 데이터. 논문에선 $x$로 표시</li>
      <li>I(Input 또는 Imaginary): G가 가짜 데이터를 생성할 source. 논문에선 $z$로 표시.
        <ul>
          <li>$G(z)$는 $G$가 $z$를 입력으로 받아 생성한 가짜 데이터이다.</li>
        </ul>
      </li>
      <li>$G$(generator): 생성자, 위조범</li>
      <li>$D$(Distriminator): 감별자 또는 식별자, 경찰</li>
    </ol>
  </li>
  <li>다음 전체 과정을 <code class="highlighter-rouge">num_epochs</code> 동안 반복한다:
    <ol>
      <li>D를 training하는 과정(<code class="highlighter-rouge">d_steps</code>만큼 반복): <strong>D와 G를 모두 사용은 하지만 D의 parameter만 업데이트한다.</strong>
        <ol>
          <li>$D$에 실제 데이터($x$)와 정답(1)을 입력으로 주고 loss를 계산한다.</li>
          <li>$D$에 가짜 데이터($G(z)$)와 정답(0)을 입력으로 주고 loss를 계산한다.</li>
          <li>두 loss를 합친 후 $D$의 parameter를 업데이트한다.</li>
        </ol>
      </li>
      <li>G를 training하는 과정(<code class="highlighter-rouge">g_steps</code>만큼 반복): <strong>D와 G를 모두 사용은 하지만 G의 parameter만 업데이트한다.</strong>
        <ol>
          <li>$D$에 가짜 데이터($G(z)$)와 정답(1)을 입력으로 주고 loss를 계산한다.</li>
          <li>계산한 loss를 이용하여 $G$의 parameter를 업데이트한다.</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h2 id="단점-및-극복방안">단점 및 극복방안</h2>

<p>GAN 논문에서는 수학적인 증명이 포함되어 있지만(최소 해를 가지며, 충분히 학습할 시 항상 그 해답을 찾는다), 여러 요인들로 인해 실제 학습시킬 때에는 학습이 좀 불안정하다는 단점이 있다.</p>

<h3 id="mode-collapsing">Mode Collapsing</h3>

<p>간단히 이 현상은 학습 모델이 실제 데이터의 분포를 정확히 따라가지 못하고 그저 뭉뚱그리기만 하면서 다양성을 잃어버리는 것이다.<br />
예를 들면 1~9까지의 숫자 9개를 만드는 대신 5만 9개 만드는 것과 비슷하며, MNIST의 경우 10종류의 모든 숫자가 아닌 특정 숫자들만 생성하는 경우이다.</p>

<p>이는 GAN이 단순히 목적함수의 loss만을 줄이려는 방향으로 설정되어 있어 생기는 현상이다. 이 현상은 GAN의 개선 모델들에서 대부분 해결된다.</p>

<h3 id="oscillation">Oscillation</h3>

<p>G와 D가 수렴하지 않고 진동하는 모양새를 보일 때가 있다. 이 역시 비슷한 이유로 발생하며, 나중 모델들에서 해결된다.</p>

<h3 id="g와-d-사이의-imbalance">G와 D 사이의 Imbalance</h3>

<p>학습을 진행하면 처음에는 D가 발전하고 나중에 G가 급격히 학습되는 형상을 보이는데, 처음부터 D가 너무 성능이 좋아져버리면 오히려 G가 학습이 잘 되지 않는 문제가 발생한다(D가 시작부터 G의 기를 죽이는 셈).</p>

<h3 id="해결방안">해결방안</h3>

<ul>
  <li>진짜 데이터와 가짜 데이터 간 Least Square Error를 목적함수에 추가한다(LSGAN).</li>
  <li>모델의 구조를 convolution으로 바꾼다(DCGAN)</li>
  <li>mini-batch별로 학습을 진행할 경우 이전 학습이 잘 잊혀지는 것을 막기 위해 이를 기억하는 방향으로 학습시킨다.</li>
</ul>

<hr />

<h1 id="튜토리얼">튜토리얼</h1>

<h2 id="50줄로-짜보는-튜토리얼">50줄로 짜보는 튜토리얼</h2>

<p>원문 링크는 <a href="https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f">여기</a>, 번역본은 <a href="http://ddanggle.github.io/GANinTorch">여기</a>에서 볼 수 있다.<br />
해당 튜토리얼에서는</p>
<ol>
  <li>이 전체 과정을 <code class="highlighter-rouge">num_epochs</code>(여기서는 5000)만큼 반복한다.
    <ol>
      <li>training D(<code class="highlighter-rouge">d_steps</code>만큼 반복):
        <ol>
          <li>가우시안 분포를 따르는 데이터를 Real Data로 생성하고</li>
          <li>그 momentum(mean, std, skews, kurtoses)를 계산하여 D에게 전달, error를 계산한다.</li>
          <li>또 Fake data를 G가 생성하게 하고</li>
          <li>D가 error를 계산하게 한다.</li>
          <li>위 두 과정(1~2, 3~4)으로 D의 parameter를 업데이트한다.</li>
        </ol>
      </li>
      <li>training G(<code class="highlighter-rouge">g_steps</code>만큼 반복):
        <ol>
          <li>G로 Fake data를 생성한다.</li>
          <li>D에게서 판별 결과를 받아온다.</li>
          <li>G가 error를 계산하게 한다.</li>
          <li>G의 parameter를 업데이트한다.</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<p><a href="https://github.com/devnag/pytorch-generative-adversarial-networks">코드</a>는 원문에도 소개되어 있지만 전체는 사실 186줄이다(…) 물론 GAN의 핵심 코드는 50줄 정도이다.</p>

<h2 id="mnist-튜토리얼">MNIST 튜토리얼</h2>

<p>GAN의 핵심 부분을 제외한 부분은 <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/">여기</a>를 참고하면 된다.</p>

<p>우선 기본 설정부터 하자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">imageio</span>


<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="p">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">'GAN tutorial: MNIST'</span><span class="p">)</span>

<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--epochs'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'number of epochs'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--batch-size'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'size of mini-batch'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--noise-size'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'size of random noise vector'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--use-cuda'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'use cuda if available'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--learning-rate'</span><span class="p">,</span> <span class="s">'-lr'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'learning rate of AdamOptimizer'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--beta1'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'parameter beta1 of AdamOptimizer'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--beta2'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'parameter beta2 of AdamOptimizer'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--output-dir'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">'output/'</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'directory path of output'</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">'--log-file'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">'log.txt'</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">'filename of logging'</span><span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="n">os</span><span class="p">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">use_cuda</span> <span class="o">=</span> <span class="n">args</span><span class="p">.</span><span class="n">use_cuda</span> <span class="ow">and</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,))</span>
<span class="p">])</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'data'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Generator는 다음과 같이 선언한다. 레이어는 총 4개, activation function은 LeakyRELU와 Tanh를 사용하였다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">28</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        :param x: input tensor[batch_size * noise_size]
        :return: output tensor[batch_size * 1 * 28 * 28]
        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">()(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear4</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</code></pre></div></div>

<p>Discriminator는 다음과 같다. Linear Layer는 G의 역방향으로 가는 것과 비슷하지만, activation function에는 차이가 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Discriminator</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">28</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        :param x: input tensor[batch_size * 1 * 28 * 28]
        :return: possibility of that the image is real data
        """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">()(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear4</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>

<p>GAN의 핵심 부분은 다음과 같다. 위의 Gaussian 분포 예제와 크게 다르지 않아서 크게 설명은 필요없을 듯 하다. 차이점을 조금 적어보자면</p>
<ol>
  <li>분포의 momentum을 G의 데이터 생성 source로 사용하는 대신 길이 100(MNIST의 경우 보통)짜리 random vector를 사용한다. G는 이 길이 100짜리 벡터를 갖고 MNIST의 숫자 이미지와 비슷한 이미지를 생성하려고 하게 된다.</li>
  <li>또 <code class="highlighter-rouge">.cuda()</code>와 <code class="highlighter-rouge">DataLoader</code>를 사용하는 것 정도가 있겠으나 GAN의 핵심 부분은 아니다.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">D_real_data</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">D_real_data</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            
            <span class="c1"># Training D with real data
</span>            <span class="n">D</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            
            <span class="n">target_real</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">target_fake</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
                <span class="n">D_real_data</span><span class="p">,</span> <span class="n">target_real</span><span class="p">,</span> <span class="n">target_fake</span> <span class="o">=</span> \
                <span class="n">D_real_data</span><span class="p">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target_real</span><span class="p">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target_fake</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
            
            <span class="n">D_real_decision</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">D_real_data</span><span class="p">)</span>
            <span class="n">D_real_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">D_real_decision</span><span class="p">,</span> <span class="n">target_real</span><span class="p">)</span>
            
            <span class="c1"># Training D with fake data
</span>            
            <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">noise_size</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span> <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
            
            <span class="n">D_fake_data</span> <span class="o">=</span> <span class="n">G</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">D_fake_decision</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">D_fake_data</span><span class="p">)</span>
            <span class="n">D_fake_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">D_fake_decision</span><span class="p">,</span> <span class="n">target_fake</span><span class="p">)</span>
            
            <span class="n">D_loss</span> <span class="o">=</span> <span class="n">D_real_loss</span> <span class="o">+</span> <span class="n">D_fake_loss</span>
            <span class="n">D_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            
            <span class="n">D_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="c1"># Training G based on D's decision
</span>            <span class="n">G</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            
            <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">noise_size</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span> <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
            
            <span class="n">D_fake_data</span> <span class="o">=</span> <span class="n">G</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">D_fake_decision</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">D_fake_data</span><span class="p">)</span>
            <span class="n">G_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">D_fake_decision</span><span class="p">,</span> <span class="n">target_real</span><span class="p">)</span>
            <span class="n">G_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            
            <span class="n">G_optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>전체 코드는 <a href="https://github.com/greeksharifa/Tutorial.code/blob/master/Python/GAN_tutorial/gan_tutorial.py">여기</a>를 참조하라.</p>

<hr />

<h1 id="이후-연구들">이후 연구들</h1>

<p>사실 2014년 발표된 original GAN은</p>
<ul>
  <li>학습이 불안정하고</li>
  <li>고해상도 이미지는 생성하지 못하는
한계를 갖고 있었다. 논문에서 optimal point가 있고 그쪽으로 수렴한다는 것을 보였지만, 실제로는 여러 변수 때문에 학습이 항상 잘 되는 것이 아니라는 현상을 보인다. 이러한 문제를 보완하기 위해 GAN 이후로 수많은 발전된 GAN이 연구되어 발표되었다.</li>
</ul>

<p>그 중에서 가장 중요한 것을 3가지 정도만 뽑자면</p>

<ol>
  <li>Convolution을 사용하여 GAN의 학습 불안정성을 많이 개선시킨 <a href="https://greeksharifa.github.io/generative%20model/2019/03/17/DCGAN/">DCGAN(Deep Convolutional GAN, 2015)</a></li>
  <li>단순 생성이 목적이 아닌 원하는 형태의 이미지를 생성시킬 수 있게 하는 시초인 <a href="https://greeksharifa.github.io/generative%20model/2019/03/19/CGAN/">CGAN(Conditional GAN, 2014)</a></li>
  <li>GAN이 임의의 divergence를 사용하는 경우에 대해 local convergence함을 보여주고 그에 대해 실제 작동하는 GAN을 보여준 <a href="https://greeksharifa.github.io/generative%20model/2019/03/19/f-GAN/">f-GAN(2016)</a></li>
</ol>

<p>일 듯 하다.</p>

<p>많은 GAN들(catGAN, Semi-supervised GAN, LSGAN, WGAN, WGAN_GP, DRAGAN, EBGAN, BEGAN, ACGAN, infoGAN 등)에 대한 설명은 <a href="https://greeksharifa.github.io/generative%20model/2019/03/20/advanced-GANs/">여기</a>에서, DCGAN에 대해서는 <a href="https://greeksharifa.github.io/generative%20model/2019/03/17/DCGAN/">다음 글</a>에서 진행하도록 하겠다.</p>

<hr />

  </article>
  <script type="text/javascript" async
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  <script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

</div>

<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-9951774327887666">
</amp-auto-ads>

<script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-9951774327887666"
     data-ad-slot="6606866336"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
    <li>
      <h3>
        <a href="/github/2020/05/27/github-usage-09-overall/">
          GitHub 사용법 - 09. Overall
          <small>27 May 2020</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/generative/model/2020/05/25/VAE/">
          Variational AutoEncoder 설명
          <small>25 May 2020</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/machine_learning/2020/05/01/AFM/">
          추천 시스템의 기본 - 06. AFM 논문 리뷰 및 Tensorflow 구현
          <small>01 May 2020</small>
        </a>
      </h3>
    </li>
    
  </ul>
</div>

<div id="disqus_thread"></div>
<script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

  var disqus_config = function () {
    this.page.url = 'http://localhost:4000/generative%20model/2019/03/03/GAN/';
    this.page.identifier = 'http://localhost:4000/generative%20model/2019/03/03/GAN/';
    //this.page.url = 'https://greeksharifa.github.com/';  // Replace PAGE_URL with your page's canonical URL variable
    //this.page.identifier = 'greeksharifa'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };

  (function () { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://greeksharifa.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by
  Disqus.</a></noscript>

  </div>
</div>

<label for="sidebar-checkbox" class="sidebar-toggle"></label>

<script>
  (function (document) {
    let toggle = document.querySelector('.sidebar-toggle');
    let sidebar = document.querySelector('#sidebar');
    let checkbox = document.querySelector('#sidebar-checkbox');

    document.addEventListener('click', function (e) {
      let target = e.target;

      if (target === toggle) {
        checkbox.checked = !checkbox.checked;
        e.preventDefault();
      } else if (checkbox.checked && !sidebar.contains(target)) {
        /* click outside the sidebar when sidebar is open */
        checkbox.checked = false;
      }
    }, false);
  })(document);
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    };
    i[r].l = 1 * new Date();
    a = s.createElement(o);
    m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-00000000-1', 'auto');
  ga('send', 'pageview');
</script>


<!-- Naver Analytics -->	
<script type="text/javascript" src="//wcs.naver.net/wcslog.js"></script>
<script type="text/javascript">
  if(!wcs_add) var wcs_add = {};
    wcs_add["wa"] = "18cbce78e94161";
  wcs_do();
</script>

</body>

<script id="dsq-count-scr" src="//greeksharifa-github-io.disqus.com/count.js" async></script>

</html>
