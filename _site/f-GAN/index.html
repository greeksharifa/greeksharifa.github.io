<!DOCTYPE html>
<html lang="en-us">
<head>
  <head>
  <!-- Description of Blog -->
  <meta name="description" content="Python, Machine & Deep Learning">
  <link rel="canonical" href="https://greeksharifa.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Python, Machine & Deep Learning">
  <meta property="og:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta property="og:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta property="og:url" content="https://greeksharifa.github.io/">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python, Machine & Deep Learning">
  <meta name="twitter:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta name="twitter:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta name="twitter:domain" content="https://greeksharifa.github.io/">

  <!-- link -->
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      f-GAN
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="icon-144x144" sizes="144x144" href="/public/img/icon-144x144.png">
  <link rel="shortcut icon" href="/public/img/icon_32x32.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  

  <!-- Ads -->
  <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
  </script>
</head>

  <!-- for Google AdSense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9951774327887666",
    enable_page_level_ads: true
  });
</script>

  <style>blockquote {
    font-size: 1em;
    line-height: 1.4
  }</style>
  <link href='http://fonts.googleapis.com/css?family=Gill+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Consolas' rel='stylesheet' type='text/css'>
</head>
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/3c2986ad7ac1f2230ea3596f44563328">
          <img src="/public/img/maple_tree.jpg" title="Cover Photo" alt="Maple tree" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>Developer and Analyst</strong>, YW & YY.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/greeksharifa">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:greeksharifa@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://greeksharifa.github.io/">
          Github Project
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 YW & YY. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://greeksharifa.github.io">YW & YY</a>
    </p>
  </div>
</div>


<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
     content to avoid any CSS collisions with our real content. -->
<div class="wrap">
  <div class="masthead">
    <div class="container">
      <h3 class="masthead-title" align="center">
        <a href="/" title="Home" title="YW & YY">
          <img class="masthead-logo" src="/public/img/logo.png"/>
        </a>
        <small>YW & YY's Python, Machine & Deep Learning</small>
        <!-- HTML elements for search -->
        <a href="/search/" id="search_icon">
          <img src="/public/img/search.png" width="25" height="25"
               align="right" style="margin-top:5px; margin-bottom:0;"
               onmouseover="this.style.opacity=0.7" onmouseout="this.style.opacity=0.5"
               alt="search">
        </a>
      </h3>
    </div>
  </div>

  <div class="container content">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.9/styles/github.min.css"> 
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.9/highlight.min.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 수직형 디스플레이 광고1 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-9951774327887666"
     data-ad-slot="7237421728"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<div class="post">
  <h1 class="post-title">f-GAN</h1>
  <span class="post-date">19 Mar 2019</span>
   |
  
  <a href="/blog/tags/#gan" class="post-tag">GAN</a>
  
  <a href="/blog/tags/#machine-learning" class="post-tag">Machine Learning</a>
  
  <a href="/blog/tags/#cnn" class="post-tag">CNN</a>
  
  <a href="/blog/tags/#generative-model" class="post-tag">Generative Model</a>
  
  <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
  
  
  <article>
    <p><strong>목차</strong></p>
    <ul>
  <li><a href="#f-gan"><em>f</em>-GAN</a>
    <ul>
      <li><a href="#초록abstract">초록(Abstract)</a></li>
      <li><a href="#서론introduction">서론(Introduction)</a></li>
      <li><a href="#방법method">방법(Method)</a>
        <ul>
          <li><a href="#the-f-divergence-family">The <em>f</em>-divergence Family</a></li>
          <li><a href="#variational-estimation-of-f-divergences">Variational Estimation of <em>f</em>-divergences</a></li>
          <li><a href="#variational-divergence-minimizationvdm">Variational Divergence Minimization(VDM)</a></li>
          <li><a href="#representation-for-the-variational-function">Representation for the Variational Function</a></li>
          <li><a href="#example-univariate-mixture-of-gaussians">Example: Univariate Mixture of Gaussians</a></li>
        </ul>
      </li>
      <li><a href="#vdm-알고리즘algorithms-for-variational-divergence-minimizationvdm">VDM 알고리즘(Algorithms for Variational Divergence Minimization(VDM))</a>
        <ul>
          <li><a href="#single-step-gradient-method">Single-Step Gradient Method</a></li>
          <li><a href="#practical-considerations">Practical Considerations</a></li>
        </ul>
      </li>
      <li><a href="#실험experiments">실험(Experiments)</a></li>
      <li><a href="#관련-연구related-work">관련 연구(Related Work)</a></li>
      <li><a href="#토의discussion">토의(Discussion)</a></li>
      <li><a href="#참고문헌references">참고문헌(References)</a></li>
      <li><a href="#부록">부록</a></li>
    </ul>
  </li>
  <li><a href="#이후-연구들">이후 연구들</a></li>
</ul>

    <hr />

<p>이 글에서는 2016년 6월 <em>Sebastian Nowozin</em> 등이 발표한 <em>f</em>-GAN - Training Generative Neural Samplers using Variational Divergence Minimization를 살펴보도록 한다.</p>

<p><em>f</em>-GAN은 특정한 구조를 제안했다기보다는 약간 divergence에 대한 내용을 일반적으로 증명한 수학 논문에 가깝다.</p>

<p>중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.</p>

<p><em>이 논문은 수학이 넘쳐흐르는 논문이다.</em></p>

<hr />

<h1 id="f-gan"><em>f</em>-GAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1606.00709"><em>f</em>-GAN</a></strong></p>

<h2 id="초록abstract">초록(Abstract)</h2>

<p>2016년에 나온 논문임을 생각하라.</p>

<p>Generative neural sampler는 random input vector를 입력으로 받아 network weights에 정의된 확률분포로부터 sample을 만들어내는 확률적 모델이다. 이 모델들은 sample과 도함수 계산이 효율적이지만 우도(likelihood)나 주변화(marginalization)을 계산하진 못한다. 적대생성적 학습방법은 이런 모델이 추가 신경망을 통해 이를 학습할 수 있게 해준다.<br />
우리는 이 적대생성적 접근법이 더 일반적인 변분 발산(variational divergence) 추정 접근의 특별한 경우임을 보일 것이다. 우리는 임의의 <em>f-divergence</em>가 Generative neural sampler에 쓰일 수 있음을 보일 것이다. 우리는 이렇게 다양한 divergence 함수를 쓸 수 있는 것이 학습 복잡도와 생성모델의 품질 면에서 이득임을 논할 것이다.</p>

<hr />

<h2 id="서론introduction">서론(Introduction)</h2>

<p>확률적 생성모델은 주어진 domain $\chi$ 상의 확률분포를 서술한다. 예를 들면 자연언어 문장, 자연 이미지, 녹음된 파형 등의 분포가 있다.</p>

<p>가능한 모델 집합 $Q$에서 생성모델 Q가 주어졌을 때 우리는 일반적으로 다음에 관심이 있다:</p>
<ul>
  <li>Sampling. Q로부터 sample을 생성한다. sample을 살펴보거나 어떤 함숫값을 계산해봄으로써 우리는 분포에 대한 통찰을 얻거나 결정문제를 풀 수 있다.</li>
  <li>Estimation. 알려지지 않은 진짜 분포 P로부터 iid sample ${x_1, x_2, …, x_n}$이 주어졌을 때, 이 진짜 분포를 가장 잘 설명하는 Q $\in Q$를 찾는다.</li>
  <li>Point-wise 우도 측정. sample $x$가 주어지면, 우도 Q($x$)를 계산한다.</li>
</ul>

<p>GAN은 정확한 sampling과 근사추정이 가능한 인상적인 모델이다. 여기서 사용된 모델은 균등분포 같은 random input vector를 받는 feedforward 신경망이다. 최종적으로 모델을 통과하여 나오는 것은 예를 들면 이미지이다. GAN에서 sampling하는 것은 딱 1개의 input이 신경망을 통과하면 정확히 하나의 sample이 나온다는 점에서 효율적이다.</p>

<p>이런 확률적 feedforward 신경망을 <strong>generative neural samplers</strong>라고 부를 것이다. GAN도 여기에 포함되며, 또한 variational autoencoder의 decoder 모델이기도 하다.</p>

<p>original GAN에서, neural sample를 <a href="https://greeksharifa.github.io/generative%20model/2019/03/03/GAN/#%EB%AA%A9%EC%A0%81%ED%95%A8%EC%88%98-%EC%B5%9C%EC%A0%81%ED%99%94%EC%9D%98-%EC%9D%98%EB%AF%B8">JSD</a>의 근사적 최소화로 추정하는 것이 가능함이 증명되어 있다.</p>

<script type="math/tex; mode=display">D_{JS}(P \| Q) = {1 \over 2} D_{KL}(P \| {1 \over 2}(P+Q)) + {1 \over 2} D_{KL}(Q \| {1 \over 2}(P+Q))</script>

<p>$D_{KL}$은 <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence</a>이다.</p>

<p>GAN 학습의 중요한 테크닉은 동시에 최적화된 <strong>Discriminator</strong> 신경망을 만든 것에 있다. 왜냐하면 $D_{JS}$는 진짜 분포 $P$는 충분한 학습을 통해 Q가 $P$에 충분히 가까워졌을 때 분포 간 divergence를 측정하는 적정한 방법이기 때문이다.</p>

<p>우리는 이 논문에서 GAN 학습목적(training objectives)과 이를 임의의 <em>f-divergence</em>로 일반화하고자, GAN을 variational divergence 추정 framework로 확장할 것이다.</p>

<p>구체적으로, 이 논문에서 보여주는 state-of-the-art한 것은:</p>

<ul>
  <li>GAN 학습목적을 모든 <em>f</em>-divergence에 대해 유도하고 여러 divergence 함수를 소개할 것이다: Kullback-Leibler와 Pearson Divergence를 포함한다.</li>
  <li>우리는 GAN의 saddle-point 최적화를 단순화할 것이고 또 이론적으로 증명할 것이다.</li>
  <li>자연 이미지에 대한 generative neural sampler을 측정하는 데 어느 divergence 함수가 적당한지 실험적 결과를 제시하겠다.</li>
</ul>

<hr />

<h2 id="방법method">방법(Method)</h2>

<p>먼저 divergence 추정 framework를 리뷰부터 하겠다. 이후 divergence 추정에서 model 추정으로 확장하겠다.</p>

<h3 id="the-f-divergence-family">The <em>f</em>-divergence Family</h3>

<p>Kullback-Leibler divergence같이 잘 알려진 것은 두 확률분포 간 차이를 측정한다.</p>

<p>두 분포 $P$와 $Q$가 있고, domain $\chi$에서 연속밀도함수 $p$와 $q$에 대해 <em>f-divergence</em>는<br />
$ f : \mathbb{R}_+ \rightarrow \mathbb{R} $이 $f(1)=0$인 볼록이고 하반연속인(convex, lower-semicontinuous) 함수 $f$에 대해</p>

<script type="math/tex; mode=display">D_f(P \Vert Q) = \int_{\chi} q(x) f \Bigl( {p(x) \over q(x)} \Bigr) dx</script>

<p>로 정의된다.</p>

<h3 id="variational-estimation-of-f-divergences">Variational Estimation of <em>f</em>-divergences</h3>

<p><em>Nyugen</em> 등 연구자는 $P$와 $Q$로부터의 sample만 주어진 경우에서 <em>f</em>-divergence를 측정하는 일반적인 변분법을 유도했다. 우리는 이를 고정된 모델에서 그 parameter를 측정하는 것으로까지 확장할 것이고, 이를 <em>variational divergence minimization</em>(VDM)이라 부를 것이다. 또한 적대적 생성 학습법은 이 VDM의 특수한 경우임을 보인다.</p>

<p>모든 볼록이고 <a href="https://ko.wikipedia.org/wiki/%EB%B0%98%EC%97%B0%EC%86%8D%EC%84%B1">하반연속</a>인 볼록 켤레함수 $f^\ast$ (<em>Fenchel conjugate</em>)를 갖는다. 이는</p>

<script type="math/tex; mode=display">f^\ast(t) = \quad sup \quad  \{ ut-f(u) \} \\ u \in dom_f \qquad</script>

<p>로 정의된다.</p>

<p>또한 $f^\ast$ 역시 볼록이며 하반연속이고 $f^{\ast\ast} = f$이므로 $ f(u) = sup_{t \in dom_{f^\ast}} { tu-f^\ast(t) } $로 쓸 수 있다.</p>

<p><em>Nguyen</em> 등 연구자는 lower bound를 구했다: $\tau$가 $T: \chi \rightarrow \mathbb{R} $인 함수들의 임의의 집합일 때,</p>

<script type="math/tex; mode=display">D_f(P \Vert Q) \ge sup_{T \in \tau} (\mathbb{E}_{x \sim P} [T(x)] - \mathbb{E}_{x \sim Q} [f^\ast(T(x))])</script>

<p>변분법을 취해서,</p>

<script type="math/tex; mode=display">T^\ast(x) = f^{'} \Bigl( {p(x) \over q(x)} \Bigr)</script>

<p>아래는 여러 <em>f</em>-divergence를 생성함수와 함께 나타낸 것이다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/01.png" width="100%" /></center>

<h3 id="variational-divergence-minimizationvdm">Variational Divergence Minimization(VDM)</h3>

<p>이제 실제 분포 $P$가 주어졌을 때 생성모델 $Q$를 측정하기 위해 <em>f</em>-divergence $D_f(P\Vert Q)$에 하한을 적용할 수 있다.</p>

<p>벡터 $\theta$를 받는 모델 $Q$를 $Q_{\theta}$, $\omega$를 쓰는 $T$를 $T_{\omega}$로 썼을 때, 우리는 다음 <em>f</em>-GAN 목적함수의 saddle-point를 찾는 것으로 $Q_{\theta}$를 학습시킬 수 있다.</p>

<script type="math/tex; mode=display">F(\theta, \omega) = \mathbb{E}_{x \sim P} [T_{\omega}(x)] - \mathbb{E}_{x \sim Q_{\theta}} [f^\ast({T_\omega}(x))]</script>

<p>주어진 유한한 학습 데이터셋에 대해 위 식을 최적화하려면, minibatch를 통해 기댓값을 근사해야 한다. $\mathbb{E}<em>{x \sim P}[\cdot]$를 근사하기 위해 학습 셋으로부터 비복원추출하여 $B$개를 뽑고, $\mathbb{E}</em>{x \sim Q_{\theta}}[\cdot]$를 근사하기 위해 현재 생성모델 $Q_{\theta}$로부터 $B$개를 뽑는다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/02.png" width="100%" /></center>

<h3 id="representation-for-the-variational-function">Representation for the Variational Function</h3>

<p>위의 식을 다른 <em>f</em>-divergence에도 사용하려면 켤레함수 $f^\ast$의 도메인  $dom_{f^\ast}$를 생각해야 한다. $T_\omega (x) = g_f(V_\omega(x)) $로 바꿔 쓸 수 있다.</p>

<p>이제 GAN 목적함수를 보면, divergence가 sigmoid이므로</p>

<script type="math/tex; mode=display">F(\theta, \omega) = \mathbb{E}_{x \sim P} [log D_{\omega}(x)] - \mathbb{E}_{x \sim Q_{\theta}} [log(1-D_\omega(x))]</script>

<p>출력 활성함수는 Table 6을 보라(부록).</p>

<h3 id="example-univariate-mixture-of-gaussians">Example: Univariate Mixture of Gaussians</h3>

<p>가우시안 sample에 대해 근사한 결과를 적어 놓았다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/03.png" width="100%" /></center>

<hr />

<h2 id="vdm-알고리즘algorithms-for-variational-divergence-minimizationvdm">VDM 알고리즘(Algorithms for Variational Divergence Minimization(VDM))</h2>

<p>이제 우리는 목적함수의 saddle point를 찾기 위한 수치적 방법을 논할 것이다.</p>

<ol>
  <li>Goodfellow가 제안한 교대(alternative) 학습 방법</li>
  <li>더 직접적인 single-step 최적화 과정</li>
</ol>

<p>두 가지를 쓴다.</p>

<h3 id="single-step-gradient-method">Single-Step Gradient Method</h3>

<p>원래 것과는 달리 inner loop가 없고, 단 하나의 back-propagation으로 $\omega$와 $\theta$의 gradient가 계산된다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/04.png" width="100%" /></center>

<p>saddle point 근방에서 $\theta$에 대해 볼록하고 $\omega$ 에 대해  오목한 $F$에 대해 위 알고리즘 1은 saddle point $(\theta^\ast, \omega^\ast)$에서 수렴함을 보일 수 있다.</p>

<p>이를 위해 다음 정리를 논문 부록에서 보이고 있다.</p>

<p><strong>Theorem 1.</strong> $\pi^t := (\theta^t, \omega^t) $ 라 하고, 조금 위의 근방 조건을 만족하는 saddle point $ \pi^\ast = (\theta^\ast, \omega^\ast) $ 가 존재한다고 가정하자. 더욱이 위 근방에 포함되는 $ J(\pi) = {1\over 2} \Vert \nabla F(\pi) \Vert_2^2 $ 를 정의할 수 있고, $F$는 $ \pi^\ast $ 근방 모든 $ \pi, \pi^{‘} $ 에 대해 $ \Vert \nabla J(\pi^{‘}) - \nabla J(\pi) \Vert_2 \le L \Vert \pi^{‘} - \pi \Vert_2 $ 를 만족하는 상수 $ L &gt; 0 $ 가 존재할 수 있게 하는 $F$는 충분히 smooth하다.<br />
알고리즘 1에서 step-size를 $ \eta=\delta / L$ 라 할 때,</p>

<script type="math/tex; mode=display">J(\pi^t) \le \Bigl( 1 - {\lambda^2 \over 2L} \Bigr)^t J(\pi^0)</script>

<p>를 얻을 수 있다.<br />
또 gradient $ \nabla F(x) $ 의 2차 norm은 기하적으로 감소한다.</p>

<h3 id="practical-considerations">Practical Considerations</h3>

<p>Goodfellow가 GAN 논문 당시 제안한 팁 중에<br />
<script type="math/tex">\mathbb{E}_{x \sim Q_{\theta}} [log(1-D_\omega(x))]</script> 
를 최소화하는 대신 
<script type="math/tex">\mathbb{E}_{x \sim Q_{\theta}} [log D_\omega(x)]</script>
 를 최대화하는 것으로 속도를 빠르게 하는 것이 있었다.<br />
이를 더 일반적인 <em>f</em>-GAN에 적용하면</p>

<script type="math/tex; mode=display">\theta^{t+1} = \theta^t + \eta \nabla_\theta \mathbb{E}_{x \sim Q_{\theta^t}} [g_f(V_{\omega^t}(x))]</script>

<p>그렇게 함으로써 generator 출력을 최대화할 수 있다.</p>

<p>실험적으로, 우리는 Adam과 gradient clipping이 LSUN 데이터셋의 대규모 실험에서는 특히 유용함을 발견하였다.</p>

<hr />

<h2 id="실험experiments">실험(Experiments)</h2>

<p>이제 VDM에 기초하여 MNIST와 LSUN에 대해 학습시킨 결과는 다음과 같다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/05.png" width="100%" /></center>

<center><img src="/public/img/2019-03-19-f-GAN/06.png" width="100%" /></center>

<p>결과 요약을 하면… 약간 예상 외로 divergence 함수가 달라져도 결과의 품질은 큰 차이가 없었다고 한다.</p>

<hr />

<h2 id="관련-연구related-work">관련 연구(Related Work)</h2>

<p>오직 신경망에 적용할 수 있는 것에 대해서만 논하겠다.</p>

<ul>
  <li>Mixture density networks: 유한한 mixture 모델의 parameter를 직접 회귀시키는 데 쓸 수 있다.</li>
  <li>NADE and RNADE: 사전에 정의되었고 어느 정도 임의의 출력 차원을 가진 출력의 factorization을 수행한다.</li>
  <li>Diffusion probabilistic models: 자명하고 알려진 분포에서 출발하는 학습된 발산과정의 결과로 목표 분포를 정의한다.</li>
  <li>Noise contrasive estimation(NCE): 임의로 생성된 noise로부터 데이터를 식별하는 비선형 logistic 회귀를 수행하여 비정규화된 확률모델의 parameter를 추정하는 방법이다.</li>
  <li>Variational auto-encoders(VAE): 변분법적 베이지안 학습 목표함수를 갖고 sample을 잠재표현식으로 매핑하는 확률적 encoder와 decoder 모델의 쌍이다.</li>
</ul>

<hr />

<h2 id="토의discussion">토의(Discussion)</h2>

<p>Generative neural samplers는 factorizing 가정 없이도 복잡한 분포를 표현하는 강력한 방법을 제공한다. 그러나 이 논문에서 사용된 순수 generative neural samplers는 관측된 데이터에 대한 조건부로 적용할 수 없고 따라서 그로부터 추론할 것이 없다는 한계를 갖고 있다.</p>

<p>우리는 미래에는 표현의 불확실성을 위한 neural samplers의 진면목이 식별 모델에서 발견될 것이며 생성자와 조건부 GAN 모델에 추가적인 input을 넣음으로써 쉽게 이 경우에 대해 확장할 수 있을 것이라 믿는다.</p>

<hr />

<h2 id="참고문헌references">참고문헌(References)</h2>

<p>논문 참조!</p>

<hr />

<h2 id="부록">부록</h2>

<ul>
  <li>Section A: 이 부분이다.</li>
  <li>Section B: <em>f</em>-divergence의 확장된 리스트(생성함수와 볼록 켤레함수)를 나열하였다.</li>
  <li>Section C: Theorem 1를 증명한다. (논문에는 Theorem 2라 되어 있는데 같은 것이다)</li>
  <li>Section D: 현재 GAN 최적화 알고리즘과 차이를 논한다.</li>
  <li>Section E: 다양한 divergence 측정방법을 써서 Gaussian을 혼합 Gaussian 분포에 맞춤으로써 우리의 접근법을 증명한다.</li>
  <li>Section F: 본문에서 사용한 모델의 세부 구조를 보여준다.</li>
</ul>

<p>증명의 자세한 부분은 논문을 보는 것이 빠르므로 생략하겠다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/07.png" width="100%" /></center>

<center><img src="/public/img/2019-03-19-f-GAN/08.png" width="100%" /></center>

<center><img src="/public/img/2019-03-19-f-GAN/09.png" width="100%" /></center>

<p>MNIST 생성자:<br />
$z \rightarrow Linear(100, 1200) \rightarrow BN \rightarrow ReLU \rightarrow Linear(1200, 1200) \rightarrow BN \rightarrow ReLU \rightarrow Linear(1200, 784) \rightarrow Sigmoid $</p>

<p>모든 weights는 0.05 scale로 초기화되었다.</p>

<p>MNIST Variational Function:<br />
$ x \rightarrow Linear(784, 240) \rightarrow ELU \rightarrow Linear(240, 240) \rightarrow ELU \rightarrow Linear(240, 1) $</p>

<p>ELU는 exponential linear unit이다. 모든 weights는 0.005 scale로 초기화되었다.</p>

<p>LSUN Natural Images:<br />
$ z \rightarrow Linear(100, 6\ast6\ast512)  \rightarrow BN \rightarrow ReLU \rightarrow Reshape(512, 6, 6) \rightarrow Deconv(512, 256) \rightarrow BN \rightarrow ReLU \rightarrow Deconv(256, 128) \rightarrow BN \rightarrow ReLU \rightarrow Deconv(128, 64) \rightarrow BN \rightarrow ReLU \rightarrow Deconv(64, 3) $</p>

<p>deconv는 kernel size 4, stride 2를 사용하였다.</p>

<p>Deconv는 Deconvolution을 의미하는데, <a href="https://greeksharifa.github.io/generative%20model/2019/03/17/DCGAN/">DCGAN 글</a>에서도 설명하였듯 잘못된 표현이다.</p>

<hr />

<h1 id="이후-연구들">이후 연구들</h1>

<p>GAN 이후로 수많은 발전된 GAN이 연구되어 발표되었다.</p>

<p>많은 GAN들(catGAN, Semi-supervised GAN, LSGAN, WGAN, WGAN_GP, DRAGAN, EBGAN, BEGAN, ACGAN, infoGAN 등)에 대한 설명은 <a href="https://greeksharifa.github.io/generative%20model/2019/03/20/advanced-GANs/">다음 글</a>에서 진행하도록 하겠다.</p>

<hr />

  </article>
  <script type="text/javascript" async
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  <script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

</div>

<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-9951774327887666">
</amp-auto-ads>

<script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-9951774327887666"
     data-ad-slot="6606866336"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
    <li>
      <h3>
        <a href="/github-usage-09-overall/">
          GitHub 사용법 - 09. Overall
          <small>27 May 2020</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/VAE/">
          Variational AutoEncoder 설명
          <small>25 May 2020</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/AFM/">
          추천 시스템의 기본 - 06. AFM 논문 리뷰 및 Tensorflow 구현
          <small>01 May 2020</small>
        </a>
      </h3>
    </li>
    
  </ul>
</div>

<div id="disqus_thread"></div>
<script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

  var disqus_config = function () {
    this.page.url = 'http://localhost:4000/f-GAN/';
    this.page.identifier = 'http://localhost:4000/f-GAN/';
    //this.page.url = 'https://greeksharifa.github.com/';  // Replace PAGE_URL with your page's canonical URL variable
    //this.page.identifier = 'greeksharifa'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };

  (function () { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://greeksharifa.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by
  Disqus.</a></noscript>

  </div>
</div>

<label for="sidebar-checkbox" class="sidebar-toggle"></label>

<script>
  (function (document) {
    let toggle = document.querySelector('.sidebar-toggle');
    let sidebar = document.querySelector('#sidebar');
    let checkbox = document.querySelector('#sidebar-checkbox');

    document.addEventListener('click', function (e) {
      let target = e.target;

      if (target === toggle) {
        checkbox.checked = !checkbox.checked;
        e.preventDefault();
      } else if (checkbox.checked && !sidebar.contains(target)) {
        /* click outside the sidebar when sidebar is open */
        checkbox.checked = false;
      }
    }, false);
  })(document);
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    };
    i[r].l = 1 * new Date();
    a = s.createElement(o);
    m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-00000000-1', 'auto');
  ga('send', 'pageview');
</script>


<!-- Naver Analytics -->	
<script type="text/javascript" src="//wcs.naver.net/wcslog.js"></script>
<script type="text/javascript">
  if(!wcs_add) var wcs_add = {};
    wcs_add["wa"] = "18cbce78e94161";
  wcs_do();
</script>

</body>

<script id="dsq-count-scr" src="//greeksharifa-github-io.disqus.com/count.js" async></script>

</html>
