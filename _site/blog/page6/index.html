<!DOCTYPE html>
<html lang="en-us">
<head>
  <head>
  <!-- Description of Blog -->
  <meta name="description" content="Python, Machine & Deep Learning">
  <link rel="canonical" href="https://greeksharifa.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Python, Machine & Deep Learning">
  <meta property="og:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta property="og:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta property="og:url" content="https://greeksharifa.github.io/">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python, Machine & Deep Learning">
  <meta name="twitter:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta name="twitter:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta name="twitter:domain" content="https://greeksharifa.github.io/">

  <!-- link -->
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="icon-144x144" sizes="144x144" href="/public/img/icon-144x144.png">
  <link rel="shortcut icon" href="/public/img/icon_32x32.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  

  <!-- Ads -->
  <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
  </script>
</head>

  <!-- for Google AdSense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9951774327887666",
    enable_page_level_ads: true
  });
</script>

  <style>blockquote {
    font-size: 1em;
    line-height: 1.4
  }</style>
  <link href='http://fonts.googleapis.com/css?family=Gill+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Consolas' rel='stylesheet' type='text/css'>
</head>
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/3c2986ad7ac1f2230ea3596f44563328">
          <img src="/public/img/maple_tree.jpg" title="Cover Photo" alt="Maple tree" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>Developer and Analyst</strong>, YW & YY.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/greeksharifa">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:greeksharifa@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://greeksharifa.github.io/">
          Github Project
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 YW & YY. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://greeksharifa.github.io">YW & YY</a>
    </p>
  </div>
</div>


<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
     content to avoid any CSS collisions with our real content. -->
<div class="wrap">
  <div class="masthead">
    <div class="container">
      <h3 class="masthead-title" align="center">
        <a href="/" title="Home" title="YW & YY">
          <img class="masthead-logo" src="/public/img/logo.png"/>
        </a>
        <small>YW & YY's Python, Machine & Deep Learning</small>
        <!-- HTML elements for search -->
        <a href="/search/" id="search_icon">
          <img src="/public/img/search.png" width="25" height="25"
               align="right" style="margin-top:5px; margin-bottom:0;"
               onmouseover="this.style.opacity=0.7" onmouseout="this.style.opacity=0.5"
               alt="search">
        </a>
      </h3>
    </div>
  </div>

  <div class="container content">
    <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/paper_review/2019/07/11/2019-ICML-Papers/">
        2019 ICML Papers
      </a>
    </h1>

    <span class="post-date">11 Jul 2019</span>
     |
    
    <a href="/blog/tags/#icml" class="post-tag">ICML</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <hr />

<p>이 글에서는 2019년 ICML(International Conference on Machine Learning)에서 어떤 논문들이 accept되어 발표되었는지를 알아볼 것이다. 3424개의 논문이 접수되어 774개의 논문만이 구두 및 포스터 발표로 진행되었다.</p>

<p>논문 리스트는 목차와 같다. 774개를 다 살펴볼 수는 없으므로 몇 개만 추려서 최근 동향을 살펴보도록 하자.</p>

<hr />

<h2 id="training-neural-networks-with-local-error-signals">Training Neural Networks with Local Error Signals</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">      개요      </th>
      <th>내용</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">저자</td>
      <td><em>Francesco Locatello et al</em>. Google Research</td>
    </tr>
    <tr>
      <td style="text-align: center">논문 링크</td>
      <td><a href="https://arxiv.org/abs/1811.12359">https://arxiv.org/abs/1811.12359</a></td>
    </tr>
    <tr>
      <td style="text-align: center">블로그</td>
      <td><a href="https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html">https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html</a></td>
    </tr>
    <tr>
      <td style="text-align: center">제출일</td>
      <td>Submitted on 29 Nov 2018 (v1), last revised 18 Jun 2019 (this version, v4)</td>
    </tr>
  </tbody>
</table>

<p>이 논문은 표현(representation)에 대한 것인데, 논문에 쓰인 표현들이 참 어렵다.</p>

<p>초록을 대략 번역하자면,</p>

<p>풀린 표현(<em>disentangled</em> representation)의 무감독 학습(<em>unsupervised</em> learning)의 핵심 아이디어는 ‘실제 세계의 데이터는 무감독 학습 알고리즘에 의해 복구될 수 있는 몇 가지 설명요인에 의해 생성된다’는 것이다.</p>

<p>이 논문에서, 풀린 표현의 무감독 학습은 모델과 데이터 모두에 대해 귀납적 편향(inductive biases) 없이는 본질적으로 불가능하다는 것을 이론적으로 보일 것이다. 또한 6개의 최신 무감독 풀림 학습(unsupervised disentangled learning) 방법과 풀림 측정방식(disentangled measures)을 구현하여 이를 여러 데이터셋에 대해 12000개 이상의 모델을 학습시킬 것이다.</p>

<p>이로써</p>

<ul>
  <li>여러 방법들이 ‘대응되는 loss에 의한’ 성질들을 강제함에도 불구하고 감독 없이는 잘 풀린(well-disentangled) 모델은 식별될 수 없다는 사실과(<em>역자 주: 모델이 식별될 수 없다는 것은 이를테면 두 가지 모델이 생성한 각각의 결과가 있을 때, 그 결과만 보고 원래 모델이 무엇이었을지를 알 수 없다는 뜻이다</em>),</li>
  <li>‘풀린 정도가 증가한(increased disentanglement)’ 것도 downstream task의 학습의 샘플 복잡도의 감소로 이어지지는 않는다는 것</li>
</ul>

<p>을 알아내었다.</p>

<p>이같은 결과는 앞으로 풀린 학습에 대한 연구는</p>

<ul>
  <li>명백히 귀납적 편향과 감독에 의해야 하며,</li>
  <li>학습된 표현의 풀림을 강제하는 것의 구체적인 이점을 조사하며,</li>
  <li>여러 데이터셋을 다룰 수 있는 재현 가능한 실험 설정을 고려해보아야 한다</li>
</ul>

<p>는 것을 말해준다.</p>

<p>실제 논문 서론에서 주장하는 contribution은,</p>

<ul>
  <li>풀린 표현의 무감독 학습은 ‘학습방법과 데이터셋 모두에 대한 귀납적 편향’ 없이는 본질적으로 불가능함을 이론적으로 보인 것</li>
  <li>현재의 여러 무감독 풀림 학습 방법들을 조사 및 구현하여 이를 여러 데이터셋과 모델을 학습시킨 것</li>
  <li>풀린 표현을 학습하고 평가하는 <em>disentanglement_lib</em>라는 새로운 라이브러리를 공개한 것</li>
  <li>상당한 계산량을 필요로 하는 1만 개 이상의 사전 학습된(pre-trained) 모델을 공개한 것</li>
  <li>무감독 풀림 학습에 대한 여러 생각들을 검증해보았다:
    <ul>
      <li>고려된 모든 방법들이 샘플링된 posterior들의 차원(dimensions)의 독립성을 보장한다고 해도, 표현의 차원을 상관관계가 있다.</li>
      <li>random seed와 hyperparameter이라는 무감독 조건 하에서 고려된 모델들이 풀린 표현을 더 잘 학습한다는 증거가 없다.</li>
      <li>(데이터셋을 통한 훌륭한(학습이 잘 되는) hyperparameter들을 주는 것을 허용한다 할지라도) ‘ground-truth 레이블에 접근할 수 없는’ 잘 학습된 모델은 식별될 수 없다.</li>
      <li>고려된 모델과 데이터셋에 대해, 학습의 샘플 복잡도의 감소와 같은 downstream task에 풀림이 유용하지 않다.</li>
    </ul>
  </li>
  <li>실험 결과에 의해, 향후 연구에 대한 세 가지 중요한 부분을 제안하였다: 이는 초록 부분과 같다.</li>
</ul>

<hr />

<h2 id="rates-of-convergence-for-sparse-variational-gaussian-process-regression">Rates of Convergence for Sparse Variational Gaussian Process Regression</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">      개요      </th>
      <th>내용</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">저자</td>
      <td><em>David R. Burt, et al</em>. University of Cambridge and PROWLER. io</td>
    </tr>
    <tr>
      <td style="text-align: center">논문 링크</td>
      <td><a href="https://arxiv.org/abs/1903.03571">https://arxiv.org/abs/1903.03571</a></td>
    </tr>
    <tr>
      <td style="text-align: center">제출일</td>
      <td>Submitted on 8 Mar 2019 (v1), last revised 3 Jul 2019 (this version, v2)</td>
    </tr>
  </tbody>
</table>

<p><strong>초록</strong></p>

<p>Gaussian process posteriors에 대한 훌륭한 변분 근사법(variational approximations)은 데이터셋 크기 $N$에 대해 $O(N^3)$의 시간이 걸리는 것을 막기 위해 개발되었다. 이 방법은 시간복잡도를 $O(N^3)$에서 $O(NM^2), M \ll N $의 시간으로 줄여 주었다.<br />
$M$은 이 preocess를 요약하는 유도변수(<em>inducing</em> variables)인데, 수행시간은 $N$에 선형 비례함에도 불구하고 실제로는 근사의 품질을 결정하는 $M$이 얼마나 큰지에 실질적인 영향을 더 받는다.<br />
이 논문에서, $N$에 비해 훨씬 느리게 증가하는 어떤 $M$에 대해 높은 확률로 KL divergence를 임의로 작게 만들 수 있음을 보인다. 특히 Square Exponential kernel을 쓰는 D차원의 정규분포 입력에 대한 회귀의 경우 $M = O(log^D N)$이면 충분하다.<br />
이 논문의 결과는 데이터셋이 커질수록 Gaussian process posteriors는 적은 비용으로 근사될 수 있으며, 연속학습 시나리오(continual learning scenarios)에서 $M$을 증가시키는 구체적인 방법을 보이는 것이다.</p>

<p><strong>서론</strong></p>

<p><strong>Gaussian processes(GPs)</strong>는 베이지안 모델에서 convenient priors인 함수에 대한 분포이다. 이는 좋은 불확실성 측정을 해내기 때문에 특히 회귀 모델에서 자주 사용되며, 사후확률(posterior)과 주변확률(marginal likelihood)에 대한 닫힌 표현(closed-form expressions)를 가진다. 이것의 가장 큰 단점은 학습 데이터 수 $N$에 대해 $O(N^3)$의 계산량과 $O(N^2)$의 메모리를 쓴다는 것이다. Low-rank approximations(<em>Quiñonero Candela &amp; Rasmussen</em>, 2005)는 전체 사후확률을 요약하는 $M$개의 유도변수를 사용하여 계산량을 $O(NM^2 + M^3)$, 메모리 사용량을 $O(NM + M^2)$로 줄였다.</p>

<p>유도변수를 추가함으로써 계산량이 줄어드는 것은 알려져 있지만, 얼마나($M$) 필요한지에 대한 정보는 별로 없다. 데이터셋이 커질수록 우리는 품질저하 없이 근사상수의 수용력이 얼마나 될지 기대할 수 없다. 단지 $N$이 커질수록 $M$이 커져야 한다는 것만 알 뿔이다.</p>

<p>근사 GPs는 종종 근사사후확률에서 전체사후확률과정으로의 KL divergence를 최소화하는 변분추론(variational inference)을 써서 학습된다(<em>Titsias</em>, 2009, <em>Matthews et al</em>, 2016). 이 논문에서는 근사사후확률의 품질을 위한 측정방법으로 KL divergence를 사용한다.<br />
직관적인 가정 하에 유도변수의 수는 선형보다 느리게 증가하는 정도면 된다(예: 로그함수). 이는 많은 편향(bias)의 필요 없이 정확도와 불확실성에 대한 정확도를 보유한 근사사후확률만으로 큰 데이터셋에 대해 아주 희박한 근사만 있어도 된다는 것을 보여준다.</p>

<p>이 논문에서 나오는 증명의 핵심 아이디어는 데이터의 공분산행렬에 대한 Nyström 근사의 품질에 의존하는 KL divergence의 상한(상계)를 사용하는 것이다. 이 error는 무한차원의 필수연산자라는 개념으로 이해될 수 있다. Stationery kernel에 대해 메인 결과는 사전확률(priors)는 샘플함수보다 더 매끈하며(smoother) 더 집중되어 있는(more concentrated) 데이터셋은 더 희박한(sparse) 근사만으로도 충분하다는 것이다.</p>

<p><strong>메인 결과</strong></p>

<p>학습 입력은 고정된 독립항등분포로부터 나온 것이라는 가정 하에, 적어도 $1-\delta$의 확률로</p>

<script type="math/tex; mode=display">KL(Q \Vert \hat{P}) \le \mathcal{O} \Bigg( \frac{g(M, N)}{\sigma^2_n \delta}\Big(1 + \frac{c\Vert y \Vert^2_2}{\sigma^2_n}\Big) + N \epsilon \Bigg)</script>

<p>$\hat{P}$은 posterior Gaussian process, $Q$는 변분근사, $y$는 학습 목표(training targets)이다.<br />
함수 $g(M, N)$은 kernel과 입력의 분포에 의존하며, $N$에 따라 선형적으로 증가하며 $M$에 따라 빠르게 감소한다.<br />
$\epsilon$은 초기품질을 결정짓는 인자로서 약간의 계산을 추가하여 임의로 작게 만들 수 있다($N$의 역승).</p>

<p><strong>참고: Gaussian process regression</strong></p>

<p>학습 데이터</p>

<script type="math/tex; mode=display">\mathcal{D}= \{ x_i, y_i \}^N_{i=1}, x_i \in \chi, y_i \in  \mathbb{R}</script>

<p>가 관측되었을 때 Gaussian process regression을 고려해본다. 이 때 목표는 학습데이터의 제한된 수로 인해 $f(\cdot)$에 대한 불확실성을 갖고 있을 때 새로운 입력 $x^\ast$에 대해 출력값 $y^\ast$를 예측하는 것이다. $f$에 대한 사전확률을 두는 베이지안 접근법과 약간의 noise를 가진 곽츤 데이터에 대한 $f$의 우도를 고려할 때, 모델은</p>

<script type="math/tex; mode=display">f \sim \mathcal{GP}(\nu(\cdot), k(\cdot, \cdot)), \ y_i = f(x_i) + \epsilon_i, \ \epsilon_i \sim \mathcal{N}(0, \sigma^2_n)</script>

<p>$\nu : \chi \rightarrow \mathbb{R}$은 평균함수이고 $k : \chi \times \chi \rightarrow \mathbb{R}$은 공분산함수이다.  로그주변우도(log marginal likelihood)는 근사의 품질과 사후확률근사가 연관되어 있다는 점에서 흥미로우며, 이는</p>

<script type="math/tex; mode=display">\mathcal{L} = -\frac{1}{2} y^T K_n^{-1} y - \frac{1}{2} log \vert K_n \vert - \frac{N}{2} log(2\pi), \quad K_n = K_{ff} + \sigma^2_n I, \ [K_{ff}]_{i, j} = k(x_i, x_j)</script>

<p>으로 표현된다.</p>

<hr />

<h2 id="training-neural-networks-with-local-error-signals-1">Training Neural Networks with Local Error Signals</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center">      개요      </th>
      <th>내용</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">저자</td>
      <td><em>Arild Nøkland, Lars Hiller Eidnes</em></td>
    </tr>
    <tr>
      <td style="text-align: center">논문 링크</td>
      <td><a href="https://arxiv.org/abs/1901.06656">https://arxiv.org/abs/1901.06656</a></td>
    </tr>
    <tr>
      <td style="text-align: center">소스코드</td>
      <td><a href="https://github.com/anokland/local-loss">https://github.com/anokland/local-loss</a></td>
    </tr>
    <tr>
      <td style="text-align: center">제출일</td>
      <td>Submitted on 20 Jan 2019 (v1), last revised 7 May 2019 (this version, v2)</td>
    </tr>
  </tbody>
</table>

<p>최근 분류(classification)를 위한 신경망의 감독학습(supervised learning)은 보통 global loss function을 사용하여 이루어졌다. 즉, 모델을 학습시키는 데 있어서 하나의 loss function만을 설정해 두고, prediction 단계에서 계산한 loss로 backward pass 동안 gradient를 계산하며 weights를 업데이트하는 역전파(back-propagation) 과정을 거쳐왔다.</p>

<p>그러나 이 논문에서는 하나의 loss function을 모델의 모든 레이어에 걸쳐 global하게 사용하는 대신 각 레이어별로 loss function을 설정하여 실험하였고, 이 방법은 생물학적으로 그럴듯하고(biologically plausible) 그러면서도 여전히 state-of-the-art 결과를 얻을 수 있음을 보여주었다.</p>

<p>Global loss function의 사용은 다음과 같은 문제를 갖는다.</p>

<ol>
  <li><strong>Backward locking problem:</strong> hidden layer의 weights들은 forward &amp; backward pass가 끝날 때까지 업데이트되지 않는다. 따라서 weights update의 병렬화가 어렵다.</li>
  <li><strong>Preventing reuse of the memory:</strong> hidden layer의 activation들을 backward pass가 끝날 때까지 메모리에 상주시켜야 하기 때문에 메모리 사용량이 늘어난다.</li>
  <li><strong>Biologically implausible:</strong> global loss의 역전파는 신경망이라는 관점에서 생물학적으로 별로 타당하지 않다.</li>
</ol>

<p>이 논문에서, backward locking problem은 지역적으로(각 레이어별로) 측정된 error에 의해 각각 학습시킴으로써 해결될 수 있음을 보인다. Local loss function은 global error에 의존하지 않고, gradient는 해당 레이어를 제외한 그 이전 레이어에 전파되지 않으며, hidden layer는 forward pass 중간에도 업데이트될 수 있다.<br />
추론(inference) 단계에서 네트워크는 global 역전파를 쓰는 것과 같이 움직인다. 그러나 hidden layer가 업데이트될 때, gradient와 activation은 더 이상 메모리에 남아 있을 필요가 없다.<br />
따라서 모든 레이어를 동시에 학습시킴에도, 지역적으로 측정된 error는 각 레이어를 학습시키며 이것을 메모리 사용량과 학습 시간을 줄여줄 수 있게 된다.</p>

<p>관련 연구는 <strong>Local Loss Functions, Similarity Measures in Neuroscience/Machine Learning</strong> 등이 있다(논문 참조).</p>

<p>표준 <strong>convolutional &amp; fully connected</strong> 네트워크 구조를 사용하여, global loss 대신 각 레이어별로 (이전 레이어로 전파되지 않는) local learning signal을 설정했다. 이 signal은 2개의 single-layer sub-networks로 분리되어, 각각은 서로 다른 loss function을 갖는다. 하나는 표준 <strong>cross-entropy loss</strong>이고, 다른 하나는 <strong>similarity matching loss</strong>이다.</p>

<center><img src="/public/img/2019-07-11-2019-ICML-Papers/01.png" width="100%" alt="activation and gradient flow" /></center>

<p>논문에서는 여러 loss를 정의하는데,</p>

<ul>
  <li><strong>sim loss:</strong> mini-batch의 example들 간 pair-wise 유사도를 담고 있는 두 행렬간 L2 거리를 측정하는 similarity matching loss이다.</li>
  <li><strong>pred loss:</strong> target과 local classifier의 prediction 간 cosss-entropy loss를 측정한다.</li>
  <li><strong>sim-bpf loss &amp; pred-bpf loss:</strong> Backprop free version을 만들기 위해, global target이 각 hidden layer로 전파되는 것을 막는다. <strong>sim loss</strong>에서는 one-hot encoded target vector 대신 random transformation target vector를, <strong>pred loss</strong>에서는 binarized random transformation target vector를 사용한다.</li>
  <li><strong>predsim &amp; predsim-bpf loss:</strong> sim과 pred를 조합해서 전체 loss를 만들었다.</li>
</ul>

<script type="math/tex; mode=display">L_{predsim} = (1-\beta)L_{pred} + \beta L_{sim}</script>

<script type="math/tex; mode=display">L_{predsim-bpf} = (1-\beta)L_{pred-bpf} + \beta L_{sim-bpf}</script>

<p>실험은 MNIST, Fashion-MNIST, Kuzushiji-MNIST, CIFAR-10, CIFAR-100, STL_10, SVHN에 대해서 각각 진행하였다.</p>

<p>결과를 요약하자면 단지 local <strong>pred</strong> loss만으로도 global 역전파를 사용한 것과 거의 같은 성능을 보였고, <strong>predsim</strong>이나 <strong>predsim-bpf</strong>를 사용한 경우 state-of-the-art 결과를 얻을 수 있었다고 한다.</p>

<p>따라서 이 논문의 contribution은 loss function을 굳이 global하게 만들지 말고 각 레이어별로 local loss function을 만들어서 backward locking problem과 parallelization을 해결하는 것이 <strong>학습속도, 생물학적 타당성, 분류 성능</strong>을 다 잡을 수 있다는 가능성을 보여준 것이 되겠다.</p>

<hr />

<hr />


    </article>
    <div class="post-more">
      
      <a href="/paper_review/2019/07/11/2019-ICML-Papers/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/paper_review/2019/07/11/2019-ICML-Papers/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/pytorch/2019/06/12/pytorch-usage-04-RNN-Model/">
        PyTorch 사용법 - 04. Recurrent Neural Network(RNN) Model
      </a>
    </h1>

    <span class="post-date">12 Jun 2019</span>
     |
    
    <a href="/blog/tags/#pytorch" class="post-tag">PyTorch</a>
    
    

    <article>
      <hr />

<p><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-00-references/">PyTorch 사용법 - 00. References</a><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-01-introduction/">PyTorch 사용법 - 01. 소개 및 설치</a><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/">PyTorch 사용법 - 02. Linear Regression Model</a><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/">PyTorch 사용법 - 03. How to Use PyTorch</a><br />
<strong><a href="https://greeksharifa.github.io/pytorch/2019/06/12/pytorch-usage-04-RNN-Model/">PyTorch 사용법 - 04. Recurrent Neural Network Model</a></strong></p>

<hr />

<p>이 글에서는 RNN(Recurrent Neural Network) 기본 모델의 Pytorch 프로젝트를 살펴본다.</p>

<p>사용되는 torch 함수들의 사용법은 <a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-00-references/">여기</a>에서 확인할 수 있다.</p>

<hr />

<h2 id="프로젝트-구조">프로젝트 구조</h2>

<ul>
  <li>02_Linear_Regression_Model/
    <ul>
      <li>main.py</li>
      <li>data/
        <ul>
          <li>02_Linear_Regression_Model_Data.csv</li>
        </ul>
      </li>
      <li>results/</li>
    </ul>
  </li>
</ul>

<ol>
  <li>일반적으로 데이터는 <code class="highlighter-rouge">data/</code> 디렉토리에 넣는다.</li>
  <li>코드는 git에 두고, <code class="highlighter-rouge">data/</code>는 <code class="highlighter-rouge">.gitignore</code> 파일에 추가하여 데이터는 git에 올리지 않는다. 파일은 다른 서버에 두고 필요할 때 다운로드한다. 일반적으로 dataset은 그 크기가 수 GB 혹은 그 이상도 될 수 있기 때문에 upload/download 시간이 굉장히 길어지기도 하고, <a href="https://github.com/">Git</a>이 100MB 이상의 큰 파일은 업로드를 지원하지 않기 때문이기도 하다.</li>
</ol>

<p>물론 이 예제 프로젝트는 너무 간단하여 그냥 <code class="highlighter-rouge">data/</code> 디렉토리 없이 해도 상관없다.<br />
그리고 <code class="highlighter-rouge">output/</code> 또는 <code class="highlighter-rouge">results/</code> 디렉토리를 만들도록 한다.</p>

<hr />

<h2 id="import">Import</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>다음 파일을 다운로드하여 <code class="highlighter-rouge">data/</code> 디렉토리에 넣는다.</p>

<p><a href="https://github.com/greeksharifa/Tutorial.code/blob/master/Python/PyTorch_Usage/02_Linear_Regression_Model/data/02_Linear_Regression_Model_Data.csv">02_Linear_Regression_Model_Data.csv</a></p>

<ol>
  <li><a href="https://pytorch.org/">torch</a>: 설명이 필요없다.</li>
  <li><a href="https://pytorch.org/docs/stable/nn.html">from torch import nn</a>: nn은 Neural Network의 약자이다. torch의 nn 라이브러리는 Neural Network의 모든 것을 포괄하며, Deep-Learning의 가장 기본이 되는 1-Layer Linear Model도 <code class="highlighter-rouge">nn.Linear</code> 클래스를 사용한다. 이 예제에서도 <strong>nn.Linear</strong>를 쓴다.
    <ul>
      <li><strong>nn.Module</strong>은 모든 Neural Network Model의 Base Class이다. 모든 Neural Network Model(흔히 Net이라고 쓴다)은 <strong>nn.Module</strong>의 subclass이다. nn.Module을 상속한 어떤 subclass가 Neural Network Model로 사용되려면 다음 두 메서드를 override해야 한다.
        <ul>
          <li><code class="highlighter-rouge">__init__(self)</code>: <strong><em>Initialize.</em></strong> 여러분이 사용하고 싶은, Model에 사용될 구성 요소들을 정의 및 초기화한다. 대개 다음과 같이 사용된다.
            <ul>
              <li>self.conv1 = nn.Conv2d(1, 20, 5)</li>
              <li>self.conv2 = nn.Conv2d(20, 20, 5)</li>
              <li>self.linear1 = nn.Linear(1, 20, bias=True)</li>
            </ul>
          </li>
          <li><code class="highlighter-rouge">forward(self, x)</code>: <strong><em>Specify the connections.</em></strong> <code class="highlighter-rouge">__init__</code>에서 정의된 요소들을 잘 연결하여 모델을 구성한다. Nested Tree Structure가 될 수도 있다. 주로 다음처럼 사용된다.
            <ul>
              <li>x = F.relu(self.conv1(x))</li>
              <li>return F.relu(self.conv2(x))</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>다른 말로는 위의 두 메서드를 override하기만 하면 손쉽게 Custom net을 구현할 수 있다는 뜻이기도 하다.</li>
    </ul>
  </li>
  <li>참고: <strong>torch.autograd.Variable</strong>은 이전에는 auto gradient 계산을 위해 tensor에 필수적으로 씌워 주어야 했으나, PyTorch 0.4.0 버전 이후로 <code class="highlighter-rouge">torch.Tensor</code>와 <code class="highlighter-rouge">torch.autograd.Variable</code> 클래스가 통합되었다. 따라서 PyTorch 구버전을 사용할 예정이 아니라면 Variable은 쓸 필요가 전혀 없다.
    <ul>
      <li>인터넷에 돌아다니는 수많은 코드의 Variable Class는 0.4.0 버전 이전에 PyTorch를 시작한 사람들이 쓴 것이다.</li>
      <li><a href="https://pytorch.org/docs/stable/autograd.html#variable-deprecated/">https://pytorch.org/docs/stable/autograd.html#variable-deprecated/</a></li>
      <li><a href="https://pytorch.org/blog/pytorch-0_4_0-migration-guide/">https://pytorch.org/blog/pytorch-0_4_0-migration-guide/</a></li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="load-data">Load Data</h2>

<h3 id="데이터-준비">데이터 준비</h3>

<p>지금의 경우는 전처리할 필요가 없으므로 그냥 데이터를 불러오기만 하면 된다. 데이터가 어떻게 생겼는지도 확인해 보자.<br />
데이터가 어떤지 살펴보는 것은 모델을 결정하는 데 있어 매우 중요하다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/02_Linear_Regression_Model_Data.csv'</span><span class="p">)</span>
<span class="c1"># Avoid copy data, just refer
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'x'</span><span class="p">].</span><span class="n">values</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">);</span>    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'02_Linear_Regression_Model_Data'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/public/img/PyTorch/2018-11-02-pytorch-usage-02-Linear-Regression-Model/02_Linear_Regression_Model_Data.png" alt="02_Linear_Regression_Model_Data" /></p>

<p><strong>from_numpy</strong>로 불러오는 이유는 데이터를 복사하여 새로 텐서를 생성하는 대신 원 데이터와 메모리를 공유하는 텐서를 쓰기 위함이다. 지금은 상관없지만 대용량의 데이터를 다룰 때에는 어떤 함수가 데이터를 복사하는지 아닌지를 확실하게 알아둘 필요가 있다.<br />
물론, 정말 대용량의 데이터의 경우는 read_csv로 한번에 불러오지 못한다. 이는 데이터를 <em>batch</em>로 조금씩 가져오는 것으로 해결하는데, 이에 대해서는 나중에 살펴보자.</p>

<p>참고: 이 데이터는 다음 코드를 통해 생성되었다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">5</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">data</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'data/02_Linear_Regression_Model_Data.csv'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="p">[</span><span class="s">'x'</span><span class="p">,</span> <span class="s">'y'</span><span class="p">])</span>
</code></pre></div></div>

<hr />

<h2 id="define-and-load-model">Define and Load Model</h2>

<p>매우 간단한 모델이므로 코드도 짧다.<br />
여기서는 여러분의 편의를 위해 함수들의 parameter 이름을 명시하도록 한다.</p>

<p>PyTorch에서 Linear 모델은 <code class="highlighter-rouge">torch.nn.Linear</code> 클래스를 사용한다. 여기서는 단지 x를 y로 mapping하는 일차원 직선($ y = wx + b $)을 찾고 싶은 것이므로, <code class="highlighter-rouge">in_features</code>와 <code class="highlighter-rouge">out_features</code>는 모두 1이다.<br />
<strong>nn.Linear</strong>은 <strong>nn.Module</strong>의 subclass로 in_features개의 input을 선형변환을 거쳐 out_features개의 output으로 변환한다. parameter 개수는 $ (in_features \times out_features [ + out_features]) $ 개이다. 마지막 항은 <strong>bias</strong>이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

<span class="s">"""
Linear(in_features=1, out_features=1, bias=True)
Parameter containing:
tensor([[-0.9360]], requires_grad=True)
Parameter containing:
tensor([0.7960], requires_grad=True)
"""</span>
</code></pre></div></div>

<p>별다른 utility 함수가 필요 없으므로 따로 <code class="highlighter-rouge">utils.py</code>는 만들지 않는다.</p>

<hr />

<h2 id="set-loss-functioncreterion-and-optimizer">Set Loss function(creterion) and Optimizer</h2>

<p>적절한 모델을 선정할 때와 마찬가지로 loss function과 optimizer를 결정하는 것은 학습 속도와 성능을 결정짓는 중요한 부분이다.<br />
지금과 같이 간단한 Linear Regression Model에서는 어느 것을 사용해도 학습이 잘 된다. 하지만, 일반적으로 성능이 좋은 <code class="highlighter-rouge">AdamOptimizer</code>를 사용하도록 하겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="s">"""
tensor([[-0.1399],
        [-1.0759],
        [-2.0119],
        [-2.9478],
        [-3.8838],
        [-4.8197],
        [-5.7557],
        [-6.6917],
        [-7.6276],
        [-8.5636]], grad_fn=&lt;ThAddmmBackward&gt;)
"""</span>
</code></pre></div></div>

<p>참고: 보통 변수명은 criterion 혹은 loss_function 등을 이용한다.</p>

<hr />

<h2 id="train-model">Train Model</h2>

<p>Train은 다음과 같이 이루어진다.</p>

<ol>
  <li>모델에 데이터를 통과시켜 예측값(현재 모델의 weights로 prediction)을 얻은 뒤</li>
  <li>실제 정답과 loss를 비교하고</li>
  <li>gradient를 계산한다.</li>
  <li>이 값을 통해 weights를 업데이트한다(backpropagation).</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="s">"""
        Show your intermediate results
        """</span>
        <span class="k">pass</span>
</code></pre></div></div>

<p>코드의 각 라인을 설명하면 다음과 같다.</p>

<ol>
  <li><code class="highlighter-rouge">prediction</code>: 모델에 데이터(x)를 집어넣었을 때 예측값(y). 여기서는 $ y = wx + b $의 결과들이다.</li>
  <li><code class="highlighter-rouge">loss</code>: criterion이 MSELoss로 설정되어 있으므로, prediction과 y의 평균제곱오차를 계산한다.</li>
  <li><code class="highlighter-rouge">optimizer.zero_grad()</code>: optimizer의 grad를 0으로 설정한다. PyTorch는 parameter들의 gradient를 계산해줄 때 grad는 계속 누적되도록 되어 있다. 따라서 gradient를 다시 계산할 때에는 0으로 세팅해주어야 한다.</li>
  <li><code class="highlighter-rouge">loss.backward()</code>: gradient 계산을 역전파(backpropagation)한다.</li>
  <li><code class="highlighter-rouge">optimizer.step()</code>: 계산한 gradient를 토대로 parameter를 업데이트한다($ w \leftarrow w - \alpha \Delta w, b \leftarrow b - \alpha \Delta b $)</li>
  <li>학습 결과를 중도에 확인하고 싶으면 그래프를 중간에 계속 그려주는 것도 한 방법이다.</li>
</ol>

<hr />

<h2 id="visualize-and-save-results">Visualize and save results</h2>

<p>결과를 그래프로 보여주는 부분은 <code class="highlighter-rouge">matplotlib.pyplot</code>에 대한 내용이므로 여기서는 넘어가도록 하겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">display_results</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">);</span>    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">prediction</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s">'b--'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'loss={:.4}, w={:.4}, b={:.4}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">model</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">model</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="c1"># plt.savefig('results/02_Linear_Regression_Model_trained.png')
</span>
<span class="n">display_results</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/public/img/PyTorch/2018-11-02-pytorch-usage-02-Linear-Regression-Model/02_Linear_Regression_Model_trained.png" alt="02_Linear_Regression_Model_Trained" /></p>

<p>모델을 저장하려면 <code class="highlighter-rouge">torch.save</code> 함수를 이용한다. 저장할 모델은 대개 <code class="highlighter-rouge">.pt</code> 확장자를 사용한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">obj</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="s">'02_Linear_Regression_Model.pt'</span><span class="p">)</span>
</code></pre></div></div>

<p>참고: <code class="highlighter-rouge">.pt</code> 파일로 저장한 PyTorch 모델을 load해서 사용하려면 다음과 같이 한다. 이는 나중에 <strong>Transfer Learning</strong>과 함께 자세히 다루도록 하겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="s">'02_Linear_Regression_Model.pt'</span><span class="p">)</span>

<span class="n">display_results</span><span class="p">(</span><span class="n">loaded_model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>정확히 같은 결과를 볼 수 있을 것이다.</p>

<hr />

<p>전체 코드는 <a href="https://github.com/greeksharifa/Tutorial.code/blob/master/Python/PyTorch_Usage/02_Linear_Regression_Model/main.py">여기</a>에서 살펴볼 수 있다.</p>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/pytorch/2019/06/12/pytorch-usage-04-RNN-Model/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/pytorch/2019/06/12/pytorch-usage-04-RNN-Model/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/deep%20learning%20tutorial/2019/06/10/Deep-Learning-01/">
        Deep Learning Tutorial(딥러닝 튜토리얼) 01. 소개
      </a>
    </h1>

    <span class="post-date">10 Jun 2019</span>
     |
    
    <a href="/blog/tags/#deep-learning" class="post-tag">Deep Learning</a>
    
    

    <article>
      <hr />

<p><strong><a href="https://greeksharifa.github.io/">Deep Learning Tutorial(딥러닝 튜토리얼) 01. 소개</a></strong></p>

<hr />

<p>이 글에서는 Deep Learning(딥러닝)을 소개하고 그 기초를 다룬다.</p>

<hr />

<h2 id="deep-learning딥러닝이란">Deep Learning(딥러닝)이란?</h2>

<h3 id="직관적인-이해">직관적인 이해</h3>

<p>여러분은 A 회사의 주식 가격을 예측하고자 한다. 그러기 위해서 A 회사에 대한 정보를 수집하였다.</p>

<ul>
  <li>A의 설립일자</li>
  <li>A의 재작년 수익</li>
  <li>A의 작년 수익</li>
  <li>A의 대표의 나이</li>
  <li>A의 본사가 위치한 국가의 소득수준</li>
</ul>

<p>그리고 여러분은 수학에서 $x$를 입력하면 $y$가 나오는 함수처럼, 이 정보들을 가지고 주식 가격을 추정해보려고 한다. 위 5개의 요인 중 어떤 것이 중요할 지는 모르지만 대충 다음과 같이 그래프를 그렸다고 하자.</p>

<center><img src="/public/img/2019-06-10-Deep-Learning-01/01.png" width="100%" alt="A의 주가를 예측하라!" /></center>

<p>(저 그래프가 정말 맞는지는 우선 논외로 한다. 이걸 잘 설계하는 것이 딥러닝에서는 <strong>매우</strong> 중요하다)</p>

<p>모든 딥러닝이 이렇게 흘러가지는 않지만, 딥러닝은 대충 이런 것이다. 약간 더 자세히 설명하면,</p>

<ul>
  <li>입력($\mathbf{X} = $ <em>{A의 설립일자, A의 재작년 수익, A의 작년 수익, A의 대표의 나이, A의 본사가 위치한 국가의 소득수준}</em> )을 받아서</li>
  <li>학습을 시켜놓은 <strong>네트워크(심층신경망, DNN, Deep Neural Network)</strong> 에 집어넣으면</li>
  <li>출력($\mathbf{\hat{Y}} = $ <em>{A의 주식 가격}</em>)을 내놓는</li>
</ul>

<p>이런 네트워크를 설계하고, 학습시키고, 테스트하는 그런 과정을 포함한다.</p>

<h3 id="그래서-deep-learning이-뭔데">그래서 Deep Learning이 뭔데?</h3>

<p>간단히 얘기하자면 Deep Neural Network(심층신경망)을 설계하고 학습시켜 다음 출력을 생성하는 것이다.</p>

<p><a href="https://ko.wikipedia.org/wiki/%EB%94%A5_%EB%9F%AC%EB%8B%9D#cite_ref-1">국문 위키피디아</a>를 인용해보자.</p>

<blockquote>
  <p>딥 러닝(영어: deep learning), 심층학습(深層學習)은 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화(abstractions, 다량의 데이터나 복잡한 자료들 속에서 핵심적인 내용 또는 기능을 요약하는 작업)를 시도하는 기계학습(machine learning) 알고리즘의 집합[1] 으로 정의되며, 큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야</p>
</blockquote>

<p><a href="https://en.wikipedia.org/wiki/Deep_learning">영문 위키피디아</a>도 인용해보자.</p>

<blockquote>
  <p>Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on artificial neural networks. Learning can be supervised, semi-supervised or unsupervised.</p>
</blockquote>

<p>해석하면,</p>

<blockquote>
  <p>딥러닝(심층구조학습 또는 구조적학습)은 인공신경망에 근거한 넓은 범위의 기계학습방법의 한 부분이다. 학습 방식에는 지도(감독)을 받거나, 지도을 일부만 받거나, 받지 않는 방법이 있다.</p>
</blockquote>

<p><strong>기계학습(Machine Learning)</strong>은 컴퓨터가 스스로 학습하여 예측모형을 개발하는 인공지능의 한 분야이다.</p>

<p>하나의 용어를 설명하려면 더 많은 용어들을 설명해야 한다. 바로 지도학습으로 넘어가자.</p>

<h4 id="지도학습supervised-learning은-또-무엇인가">지도학습(Supervised Learning)은 또 무엇인가?</h4>

<p>다른 이름으로는 감독학습, 교사학습으로도 불린다.</p>

<p>이번엔 A의 주가 말고 그냥 아라비아 숫자를 생각해보자. 여러분은 다음과 같은 과제를 받았다.</p>

<blockquote>
  <p>손으로 쓴 숫자 이미지가 주어지면, 해당 이미지에는 0~9 중 어떤 숫자가 쓰여 있는지 판별하라.</p>
</blockquote>

<center><img src="/public/img/2019-06-10-Deep-Learning-01/02.png" width="100%" alt="각 숫자 이미지에는 레이블이 있다." /></center>

<p>위의 각 숫자 이미지에는 <strong>Label</strong>이 달려 있는 것을 확인할 수 있다. 지도학습은 이와 같이 각 데이터에 레이블이 있는 상태에서 학습을 시작하는 방법이다. 즉 모든 데이터($X$, 여기서는 숫자 이미지)에 레이블($Y$, 여기서는 0 ~ 9 중 하나의 숫자)이 주어져 있는 경우이다.</p>

<h4 id="그럼-비지도-학습unsupervised-learning은">그럼 비지도 학습(Unsupervised Learning)은?</h4>

<p>무감독 학습, 비교사 학습이라고도 한다.</p>

<p>간단하다. 위의 데이터에서 이미지만 주어지고 레이블은 주어지지 않는 경우이다. 이런 경우에는 보통 clustering(군집화) 등 비슷한 이미지끼리 그룹화하는 등의 task를 수행하게 된다. 위의 숫자 이미지라면 0은 0끼리, 1은 1끼리 그롭화하는 것을 생각할 수 있겠다.<br />
물론 이것말고 비지도 학습의 종류는 많다.</p>

<h4 id="그럼-준지도-학습semi-supervised-learning이란">그럼 준지도 학습(Semi-supervised Learning)이란?</h4>

<p>일부의 데이터에만 레이블 $Y$가 주어져 있는 경우이다.</p>

<h4 id="왜-비지도-학습같이-어려운-것을-하는가">왜 비지도 학습같이 어려운 것을 하는가?</h4>

<p>네트워크의 학습 관점에서, 정답(레이블)이 주어져 있는 경우가 대개 학습이 훨씬 쉽다. 보통 쉬운 순서대로 지도학습, 준지도학습, 비지도학습 순이다.<br />
그런데 왜 비지도 학습 같은 것을 하는가?</p>

<center><img src="/public/img/2019-06-10-Deep-Learning-01/03.png" width="80%" alt="각 숫자 이미지에는 레이블이 없다." /></center>

<p>현실에서 데이터는 엄청나게 많지만 그것에 레이블을 다는 작업은 보통 수동으로 한다(…). 그래서 레이블이 없는 경우가 거의 대부분이며, 많은 연구자들이 기를 쓰고 semi-supervised learning이라도 할 수 있도록 소수의 데이터에라도 레이블을 추가하거나 아니면 아예 컴퓨터가 알아서 레이블링을 하도록 학습을 시키는 이유이기도 하다.</p>

<hr />

<h2 id="deep-learning의-역사">Deep Learning의 역사</h2>

<h3 id="perceptron퍼셉트론">Perceptron(퍼셉트론)</h3>

<p>딥러닝의 근간인 인공신경망(ANN, Artificial Neural Network)의 시초는 <a href="https://psycnet.apa.org/record/1959-09865-001">F. Rosenblatt 가 1958년 발표</a>한 퍼셉트론(perceptron)이다.</p>

<center><img src="/public/img/2019-06-10-Deep-Learning-01/04.png" width="80%" alt="Perceptron" /></center>

<script type="math/tex; mode=display">\hat{y} = g(\sum_{i=1}^n{w_i x_i + b})</script>

<p>웬만한 식에서 $\ \hat{}$ 이 붙은 것($\hat{y}$ 등)은 네트워크 또는 모델이 내놓은 예측치를 의미한다. 이와 대비되는 것으로 실제 정답($y$)이 있다.</p>

<p>수학 시간에서 봤을 함수 $y = ax + b$와 비슷한 상태이다. 다른 점이 있다면</p>

<ul>
  <li>$x$가 하나가 아닌 여러 개($x_1, x_2, …, x_n$)이며</li>
  <li>가중치는 $a$가 아닌 $w_1, w_2, …, w_n$)으로 표시되고</li>
  <li>Activation function($g$)가 있다.</li>
</ul>

<p>Activation function에 대해서는 나중에 설명하도록 한다.</p>

<p>즉 $n$개의 입력값들의 선형 결합에 어떤 특정 함수를 적용하여 $y$라는 값을 예측하겠다는 것인데, 이 모형은 XOR같이 간단한 것조차 구분하지 못했기 때문에 거의 30년간 인공신경망 연구는 묻히게 된다.</p>

<p>Perceptron은 선형 결합(Linear combination)으로 계산되기 때문에, $x$의 개수가 많아져 다차원의 공간에서 Perceptron이 어느 값 이상이냐 미만이냐로 나누는 것은 곧 다차원의 공간에서 hyperplane으로 나눈다는 것을 의미한다. XOR을 표현하기 위한 2차원 공간($x_1, x_2$만 사용)에서는 hyperplane이 직선으로 나타나기 때문에 우리가 보기가 쉬워진다.</p>

<center><img src="/public/img/2019-06-10-Deep-Learning-01/05.png" width="100%" alt="XOR" /></center>

<center><img src="/public/img/2019-06-10-Deep-Learning-01/06.png" width="100%" alt="XOR" /></center>

<blockquote>
  <p>출처: http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/2-Perceptrons.pdf</p>
</blockquote>

<p>위 그림과 같이 XOR은 한 직선으로 구분해내는 것이 불가능하다.</p>

<h3 id="mlpmulti-layer-perceptron-다층-퍼셉트론">MLP(Multi-Layer Perceptron, 다층 퍼셉트론)</h3>

<p>위에서 설명한 것은 Sinle-Layer Perceptron이다. 즉, 퍼셉트론이 한 층으로만 되어 있다는 것인데, 이를 여러 층으로 쌓으면 위에서 본 XOR을 퍼셉트론이로 구분해내는 것이 가능해진다.</p>

<center><img src="/public/img/2019-06-10-Deep-Learning-01/08.png" width="100%" alt="Multi-Layer Perceptron" /></center>

<center><img src="/public/img/2019-06-10-Deep-Learning-01/07.png" width="100%" alt="Multi-Layer Perceptron" /></center>

<blockquote>
  <p>출처: https://gomguard.tistory.com/178</p>
</blockquote>

<p>자세한 것은 <a href="https://gomguard.tistory.com/178">여기</a>를 참조하면 될 것 같다.</p>

<hr />

<p><a href="https://greeksharifa.github.io/">다음 글</a>에서는 더 살펴보도록 한다.</p>

    </article>
    <div class="post-more">
      
      <a href="/deep%20learning%20tutorial/2019/06/10/Deep-Learning-01/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/deep%20learning%20tutorial/2019/06/10/Deep-Learning-01/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/computer%20vision/2019/05/29/Movie-Question-Answering/">
        MovieQA(Movie Question Answering)
      </a>
    </h1>

    <span class="post-date">29 May 2019</span>
     |
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    <a href="/blog/tags/#vqa" class="post-tag">VQA</a>
    
    <a href="/blog/tags/#task-proposal" class="post-tag">Task_Proposal</a>
    
    

    <article>
      <hr />

<p>이 글에서는 MovieQA: Understanding Stories in Movies through Question-Answering에 대해 알아보고자 한다.</p>

<p>VQA task는 이미지(Visual, 영상으로도 확장 가능)와 그 이미지에 대한 질문(Question)이 주어졌을 때, 해당 질문에 맞는 올바른 답변(Answer)을 만들어내는 task이다.</p>

<p>MovieQA는 Vision QA의 확장판과 비슷한 것이라고 보면 된다. 그러나 크게 다른 점은 사진 한 장과 QA셋이 아닌 Movie Clip과 QA셋으로 학습 및 테스트를 진행한다는 것이다. 사진이 영상으로 바뀐 만큼 당연히 난이도 역시 증가하였다.</p>

<p>MovieQA 홈페이지는 http://movieqa.cs.toronto.edu/home/ 이다.</p>

<p>중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.</p>

<hr />

<h1 id="movieqa-understanding-stories-in-movies-through-question-answering">MovieQA: Understanding Stories in Movies through Question-Answering</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1512.02902">MovieQA: Understanding Stories in Movies through Question-Answering</a></strong></p>

<h2 id="초록abstract">초록(Abstract)</h2>

<p>우리는 video와 text 모두를 통해 자동적 스토리 이해를 평하가는 MovieQA 데이터셋을 소개할 것이다. 이 데이터셋은 408개의 영화(movie)에 대한 아주 다양한 의미의 14,944개의 질문으로 이루어져 있다. 이 질문들은 ‘누가’ ‘누구에게’ ‘무엇을’ ‘어떻게’ ‘왜’ 했는지까지의 범위를 포함한다. 각 질문에는 5개의 답이 있는데 1개만 맞는 답이며 4개는 사람이 직접 만든 가짜 답이다. 우리의 데이터셋은 영상클립, 줄거리, 제목, 자막, DVS 등 많은 소스들을 포함한다는 점에서 유일하다. 우리는 이 데이터셋을 다양한 통계적 방법으로 분석했으며 존재하는 QA 기술들을 확장하여 열린 의미의 QA로 하는 것은 어렵다는 것을 보일 것이다. 우리는 이 데이터셋을 평가방법과 함께 일반에 공개하여 도전을 장려할 것이다.</p>

<center><img src="/public/img/2019-05-29-Movie-Question-Answering/01.png" width="100%" alt="ovieQA Dataset" /></center>

<hr />

<h2 id="서론introduction">서론(Introduction)</h2>

<p>이미지 태깅, 물체인식 및 분할, 액션 인식, 이미지/비디오 캡셔닝 등 많은 시각적 task에서 레이블링된 많은 양의 데이터가 사용 가능해진 것과 함께 딥러닝에서 빠른 발전이 있었다. 우리는 시각장애가 있는 사람들을 위한 보조적인 해결책이나, 일반적인 framework에서 이런 모든 task들을 추론에 의해 실제 세계를 전체적으로 인식하는 인지로봇과 같은 application에 한 걸음 더 다가갔다. 그러나 정말 ‘지능적인’ 기계는 동기, 의도, 감정, 의사소통 등 높은 수준의 것을 포함한다. 이러한 주제들은 문학에서나 겨우 탐험이 시작되었다.</p>

<p>(눈에 보이는) 장면을 이해하는 것을 보여주는 훌륭한 방법은 그것에 대한 질문-답변을 하는 것이다. 이러한 생각은 각 이미지에 대해 여러 질문들과 다지선다형 답변을 포함한 질문-답변 데이터셋을 만드는 것으로 이어졌다.<br />
이러한 데이터셋은 RGB-D 이미지 또는 Microsoft COCO와 같은 정지 이미지의 거대한 모음집에 기반한다. 전형적인 질문으로는 ‘무엇이(what)’ 거기에 있고 ‘어디에(where)’ 그것이 있는지와 같은 것, 물체가 어떤 성질을 갖는지, 얼마나 많은 ‘특정 종류의 물건’이 있는지 등이 있다.<br />
이러한 질문들은 전체적인 자연에 대한 우리의 시각적 알고리즘을 확인시켜주기는 하지만, 정지 이미지에 대해 물어볼 수 있는 태생적인 한계가 존재한다. 행동과 그 의도에 대한 높은 수준의 의미 이해는 오직 순간적, 또는 일생에 걸친 시각적 관찰에 의한 추론에 의해서만 가능하다.</p>

<center><img src="/public/img/2019-05-29-Movie-Question-Answering/02.png" width="100%" alt="MovieQA Dataset" /></center>

<p>영화(Movies)는 사람들의 삶과 이야기, 성격에 대한 높은 수준의 이해, 행동과 그 이면에 숨겨진 동기와 같은 것들을 이해할 수 있도록 하는 짤막한 정보를 우리에게 제공한다. 우리의 목표는 ‘복잡한 영상과 그에 맞는 텍스트(자막) 모두를 포함한 것을 이해하는 기계’를 측정하는 질문-답변 데이터셋을 만드는 것이다. 우리는 이 데이터셋이 다음 수준의 자동적인 ‘정말로’ 이해를 하는 기계를 만드는 것을 촉진하는 것이 되었으면 한다.</p>

<p>이 논문은 영화에 대한 거대한 질문-답변 데이터셋, MovieQA를 소개한다. 이는 408개의 영화와 14,944개의 5지선다형 질문을 포함한다. 이 중 140개의 영화(6,462개의 질답)에는 영화의 질문-답변 부분에 해당하는 time stamp가 붙어 있다.<br />
이 질문들은 ‘누가’ ‘무엇을’ ‘누구에게’ 같이 시각적으로만 풀 수 있는 것과 ‘왜’ ‘어떻게’ 무슨 일이 일어났냐는 시각정보와 대사(텍스트)를 모두 사용해야만 답을 추론할 수 있는 질문들을 포함한다.<br />
우리의 데이터셋은 영상클립, 제목, 자막, 줄거리, DVS를 포함하는 다양한 출처의 정보를 포함하는 유일한 데이터셋이다. 우리는 이를 통계적으로 분석할 것이며 또한 존재하는 질답 기술을 우리의 데이터에 적용하고 이러한 open-ended 질답이 어려운 것을 보일 것이다.<br />
우리는 leaderboard를 포함한 <a href="http://movieqa.cs.toronto.edu/leaderboard">온라인 벤치마크 사이트</a>를 만들어 두었다.</p>

<hr />

<h2 id="관련-연구related-works">관련 연구(Related Works)</h2>

<ul>
  <li><strong>Video understanding via language:</strong> 영상 범위에서 시각 및 언어정보를 통합시킨 연구는 더 적은 연구만이 존재한다. LSTM을 사용한 영상클립에 캡션을 다는 것 등이 있었다.</li>
  <li><strong>Question-answering:</strong> 자연언어처리에서 인기 있는 task이다. Memory network나 deep LSTM, Bayesian approach 등이 사용되고 있다.</li>
  <li><strong>QA Datasets:</strong> NYUv2 RGB-D와 같은 데이터셋이나, 100만 단위의 MS-COCO 데이터셋 등이 있다.</li>
</ul>

<hr />

<h2 id="movieqa-데이터셋movieqa-dataset">MovieQA 데이터셋(MovieQA Dataset)</h2>

<p>앞서 언급했듯이 408개의 영화와, 위키피디아에서 가져온 줄거리(시놉시스)를 포함한다. 또한 영상, 제목, DVS, 대사 스크립트를 포함한다.</p>

<p>이 부분의 주된 내용은 영화, 질문, 답변에는 어떤 종류가 있고, 어느 비율만큼 어떤 것이 있는지에 대한 통계 자료들이다. 자세한 내용은 궁금하면 논문을 직접 읽어보는 것이 빠르다.</p>

<center><img src="/public/img/2019-05-29-Movie-Question-Answering/03.png" width="70%" alt="MovieQA Dataset Statistics" /></center>

<center><img src="/public/img/2019-05-29-Movie-Question-Answering/04.png" width="100%" alt="MovieQA Dataset Statistics" /></center>

<center><img src="/public/img/2019-05-29-Movie-Question-Answering/05.png" width="70%" alt="MovieQA Dataset Statistics" /></center>

<center><img src="/public/img/2019-05-29-Movie-Question-Answering/06.png" width="70%" alt="MovieQA Dataset Statistics" /></center>

<h2 id="다지선다형-질문-답변multi-choice-question-answering">다지선다형 질문-답변(Multi-choice Question-Answering)</h2>

<p>여기서는 질답을 위한 여러 지능적인 기준선(intelligent baselines)를 조사하려 한다.</p>

<ul>
  <li>$S$를 이야기(줄거리, 제목, 비디오 샷을 포함한 어떤 정보든 포함)라 한다.</li>
  <li>$q^S$는 하나의 질문이다.</li>
  <li>${a^S_j}^M_{j=1}$은 질문 $q^S$에 대한 여러 답변이다. 여기서 $M=5$이다(5지선다형이므로).</li>
  <li>그러면 다지선다형 질답의 일반적인 문제느 3방향 득점 점수 $f(S, q^S, a^S)$로 나타낼 수 있다.
    <ul>
      <li>이 함수는 이야기와 질문이 주어졌을 때 답변의 “Quality”를 평가한다.</li>
    </ul>
  </li>
  <li>우리의 목표는 이제 $f$를 최대화하는 질문 $q^S$에 대한 답변 $a^S$를 선택하는 것이다:</li>
</ul>

<script type="math/tex; mode=display">j^\ast = \text{argmax}_{j=1 ... M} \ f(S, q^S, a^S_j)</script>

<p>아래는 모델의 한 예시이다.</p>

<center><img src="/public/img/2019-05-29-Movie-Question-Answering/07.png" width="100%" alt="MovieQA Dataset Statistics" /></center>

<p>모델은 ‘The Hasty Student’, ‘Searching Student’, ‘Memory Network’, ‘Video baselines’ 등을 포함한다.</p>

<h2 id="결론conclusion">결론(Conclusion)</h2>

<p>이 논문에서는 영상과 텍스트 모두를 아우르는 자동적 이야기 이해 평가를 목표로 하는 MovieQA 데이터셋을 소개하였다.
우리의 데이터셋은 영상클립, 제목, 대사 스크립트, 줄거리, DVS 등 다양한 출처의 정보를 포함한다는 점에서 유일하다. 우리는 여러 지능적인 기준선과 우리의 task의 난이도를 분석하는 원래 존재하던 질답 기술을 연장시키기도 했다. 평가 서버를 포함한 우리의 벤치마크는 <a href="http://movieqa.cs.toronto.edu">온라인</a>에서 확인할 수 있다.</p>

<hr />

<h2 id="참고문헌references">참고문헌(References)</h2>

<p>논문 참조!</p>

<hr />

<p>모델들에 대한 자세한 설명들은 생략하였다. Student 모델같은 경우에는 이름부터 꽤 흥미롭기 때문에 한번쯤 찾아보는 것을 추천한다.</p>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/computer%20vision/2019/05/29/Movie-Question-Answering/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/computer%20vision/2019/05/29/Movie-Question-Answering/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/computer%20vision/2019/04/17/Visual-Question-Answering/">
        VQA(Visual Question Answering)
      </a>
    </h1>

    <span class="post-date">17 Apr 2019</span>
     |
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    <a href="/blog/tags/#vqa" class="post-tag">VQA</a>
    
    <a href="/blog/tags/#task-proposal" class="post-tag">Task_Proposal</a>
    
    

    <article>
      <hr />

<p>이 글에서는 VQA: Visual Question Answering에 대해 알아보고자 한다.</p>

<p>VQA task는 이미지(Visual, 영상으로도 확장 가능)와 그 이미지에 대한 질문(Question)이 주어졌을 때, 해당 질문에 맞는 올바른 답변(Answer)을 만들어내는 task이다.</p>

<p>아래는 <a href="https://eng.snu.ac.kr/node/16080">서울대학교 공대뉴스광장</a>을 인용하였다.</p>

<blockquote>
  <p>VQA Challenge는 2016년 CVPR을 시작으로 매년 개최되며, 1년마다 발전된 기술을 평가하고 시상하고 있다. 2017년부터는 같은 질문에 비슷한 이미지를 보여주고 다른 답변을 하는 데이터를 VQA 2.0 데이터셋 통해 수집한 후 인공지능의 유효성을 엄밀히 평가한다.<br />
예를 들어 ‘누가 안경을 쓰고 있나?’라는 질문에 비슷한 이미지가 주어지면 ‘남자’ 또는 ‘여자’의 답을 가질 수 있도록 데이터의 분포를 고려하는 것. VQA 2.0 데이터셋은 20만 개의 이미지에 대해 110만 개의 질문과 1100만 이상의 답을 가지며, VQA 1.0보다 1.8배의 데이터를 가지고 있다.</p>
</blockquote>

<p>VQA Challenge는 컴퓨터비전패턴인식학회(IEEE Computer Vision and Pattern Recognition, CVPR) 워크샵 중 하나이며, <a href="https://visualqa.org/">VQA Homepage</a>에서 매년 열린다. 관심 있으면 클릭해 보자.</p>

<p>국내 연구팀의 대표적인 성과로는 2016년 네이버랩스 2위, 2018년 서울대 장병탁교수팀 2위가 있겠다.</p>

<p>VQA Challenge라고 하는 것은 Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, Devi Parikh 등의 연구자가 일종의 Challenge로서 제안한 것이기 때문에, 이를 중심으로 설명한다. 그렇기 때문에 논문이기도 하면서 동시에 새로운 task를 제안하겠다는 느낌이 강하다.</p>

<p>중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.</p>

<hr />

<h1 id="vqa-visual-question-answering">VQA: Visual Question Answering</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1505.00468">VQA: Visual Question Answering)</a></strong></p>

<h2 id="초록abstract">초록(Abstract)</h2>

<p>이 논문에서는 VQA task를 제안한다. VQA task는 이미지(Visual, 영상으로도 확장 가능)와 그 이미지에 대한 질문(Question)이 주어졌을 때, 해당 질문에 맞는 올바른 답변(Answer)을 만들어내는 task이다.<br />
VQA를 성공적으로 수행하기 위한 시스템은 이미지 captioning을 하는 시스템보다 더 높은 수준의 이미지 이해도와 복잡한 추론능력을 가져야 한다. 또한 (간단한 수준의 답변만 하는 것은 좋지 않기 때문에 이를) 자동으로 평가하는 것도 가능해야 한다. 우리는 25만 장의 이미지와, 76만 개의 질문, 1000만 개의 답과 그에 대한 정보를 제공한다. 많은 기준과 방법들은 사람의 수행능력과 비교한다. VQA Demo는 CloudCV에서 볼 수 있다.</p>

<p>참고) 2019.04.17 현재 논문에 링크된 CloudCV Demo는 404 error가 뜨는 중이다.</p>

<hr />

<h2 id="서론introduction">서론(Introduction)</h2>

<p>Computer Vision(CV), Natural Language Processing (NLP), Knowledge Representation &amp; Reasoning (KR)를 결합한 이미지 캡셔닝(captioning)은 지난 몇 년간 급격히 발전해 왔다. 그러나 이 task는 별로 “AI-complete”하지 못하다(그다지 인공”지능”스럽지 않다).<br />
그러면 “AI-complete”하지 못하다는 것은 무엇인가? 이 논문에서는 좀 더 자유로운 형식에 열린 형태인 VQA(Visual Question Answering)을 제안하고자 한다. 이러한 답변을 제대로 하기 위해서는 다양한 AI 능력들이 필요하다:</p>

<ul>
  <li>세밀한 인식(“이 피자엔 어떤 종류의 치즈가 있는가?”)</li>
  <li>물체 감지(“얼마나 많은 자전거가 있는가?”)</li>
  <li>행동인식(“남자는 울고 있는가?”)</li>
  <li>지식기반 추론(“이것은 채식주의자를 위한 피자인가?”)</li>
  <li>상식 추론(“이 사람은 20/20 시력을 갖고 있는가?”, “이 사람은 회사를 원하는가?” 참고: 20/20은 1.0/1.0과 같음)</li>
</ul>

<p>또한 VQA 시스템은 자동으로 평가가 가능해야 한다. 이 논문에서는 열린 문제(open-ended, 답변의 가능성이 다양함)와 다지선다형(multiple-choice) task를 둘 다 본다. 다지선다형 문제는 열린 문제와는 다르게 단지 정해진 답변 중 옳은 것을 고르기만 하면 된다.</p>

<p>데이터셋은 COCO 데이터셋에 5만 개를 더 추가했다. 데이터 수는 초록에도 나와 있다. 또한 이미지 캡셔닝이랑 무엇이 다른지에 대한 설명도 나와 있다.</p>

<hr />

<h2 id="관련-연구related-works">관련 연구(Related Works)</h2>

<ul>
  <li><strong>VQA Efforts:</strong> Visual Question Answering은 이전에도 다뤄진 적이 있긴 한데, 여기서 제안하는 것보다 훨씬 제한된 환경과 제한된 데이터셋 안에서 다룬다. 물체의 종류도 적고, 답변의 단어 등도 제한적이다. 이 VQA task는 그렇지 않다. free-form, open-ended이다.</li>
  <li><strong>Text-based Q&amp;A:</strong> 이 문제는 NLP와 텍스트 처리 분야에서 잘 연구되었다. VQA 기술에 도움이 될 몇 가지 접근법이 있다. 이 경우 질문은 텍스트를 기반으로 이루어진다. VQA는 text와 vision 모두에 의존한다.</li>
  <li><strong>Describing Visual Content:</strong> 이미지 태깅, 이미지 캡셔닝, 비디오 캡셔닝 등이 VQA와 관련이 있다. 그러나 그 캡션은 vision에 특화된 것이 아닌 지나치게 일반적인(많은 이미지에 대해 동일한 캡션을 써도 말이 됨) 경우가 있다.</li>
  <li><strong>Other Vision+Language Tasks:</strong> 이미지 캡셔닝보다 평가가 쉬운 coreference resolution, generating referring expressions 등의 task가 있다.</li>
</ul>

<hr />

<h2 id="vqa-데이터셋vqa-dataset-collection">VQA 데이터셋(VQA Dataset Collection)</h2>

<p>사실 이미지 한장이면 충분할 듯 하다.</p>

<center><img src="/public/img/2019-04-17-Visual-Question-Answering/01.png" width="100%" /></center>

<p>잘 안 보이니까 일부만 확대하겠다.</p>

<center><img src="/public/img/2019-04-17-Visual-Question-Answering/02.png" width="100%" /></center>

<ul>
  <li>약 20만 장의 현실 이미지와 약 5만 장의 추상적인 이미지가 있다.</li>
  <li>Training / Validation / Test 셋이 나누어져 있다. 그 나누는 비율도 정해져 있다(추상 이미지의 경우 20K/10K/20K). subsplit은 없다.</li>
  <li>이미 MS COCO 데이터셋은 이미지당 5개의 한 문장짜리 캡션이 있으므로, 추상 이미지에도 그만큼 붙여서 만들었다.</li>
  <li>흥미롭고, 다양하고, 잘 만들어진 질문을 모으는 것은 매우 중요한 문제이다.
    <ul>
      <li>“저 고양이의 색깔은 무엇인가?”, “지금 몇 개의 의자가 이미지에 있는가?” 같은 질문은 너무 단순하다.</li>
      <li>그러나 우리는 “상식”을 필요로 하는 질문을 원한다. 또, 상식”만”으로 대답할 수 있는 질문은 안 된다.
        <ul>
          <li>예를 들면 “사진의 저 동물은 어떤 소리를 낼 것 같은가?” 같은 질문이다.</li>
          <li>“콧수염은 무엇으로 만들어지는가?” 같은 질문은 의미 없다.</li>
        </ul>
      </li>
      <li>그래서 총 76만 개 정도의 질문을 확보하였다.</li>
    </ul>
  </li>
  <li>많은 질문들에 대해서는 yes/no만 해도 충분하다. 그러나 그렇지 않은 것들도 있다.</li>
  <li>열린 형태(open-ended) 질문들은 다음 metric에 의해 평가된다.
    <ul>
      <li>$ \text{accuracy} = min({\text{그 답변을 한 사람의 수} \over 3}, 1) $</li>
    </ul>
  </li>
  <li>다지선다형(객관식) 문제는 18개의 선택지가 있다.
    <ul>
      <li>이외에도 다양한 형태의 문제가 존재한다.</li>
    </ul>
  </li>
</ul>

<h2 id="vqa-데이터셋-분석vqa-dataset-analysis">VQA 데이터셋 분석(VQA Dataset Analysis)</h2>

<p>데이터의 정확한 수, 질문의 종류 및 수, 답변의 종류 및 수, 질답의 길이 등에 대한 분포 등이 수록되어 있다.</p>

<ul>
  <li>질문에는 “What is…”, “Is there…”, “How many…”, “Does the…” 같은 질문들이 있다. 질문의 길이는 4~8단어가 대부분이다.</li>
  <li>답변에는 yes/no, 색깔, left/right 등의 답변이 많다. 1 / 2 / 3단어인 경우가 대략 90%, 6%, 2.5% 정도씩 있다.</li>
  <li>상식을 필요로 하는 질문은 위에서 설명한 대로 당연이 이미지에서도 정보를 얻어야 답변이 가능하다.</li>
</ul>

<p>task를 제안하는 것인만큼 데이터에 대한 정보가 매우 자세하다. 아래 그림 같은 정보도 있다. 여러 종류의 질문에 대해 답변이 어떤 단어가 어떤 비율로 있는지 등을 나타낸다.</p>

<center><img src="/public/img/2019-04-17-Visual-Question-Answering/03.png" width="100%" /></center>

<hr />

<h2 id="vqa-기준선과-방법vqa-baselines-and-methods">VQA 기준선과 방법(VQA Baselines and Methods)</h2>

<h3 id="baselines">Baselines</h3>

<ul>
  <li><strong>random:</strong> 무작위로 답변을 선택한다.</li>
  <li><strong>prior(“yes”):</strong> “yes” 답변이 가장 많기 때문에 항상 yes를 답으로 내놓는다.</li>
  <li><strong>per Q-type prior:</strong> 각 질문 종류별로 답변 중 최빈값을 답으로 내놓는다.</li>
  <li><strong>nearest neighbor:</strong> 가장 유사한 K개의 질문을 뽑아 그 답변들 중 최빈값을 답으로 내놓는다.</li>
</ul>

<h3 id="methods">Methods</h3>

<ul>
  <li><strong>Image Channel:</strong> 이미지를 위한 embedding을 제공한다.
    <ul>
      <li>I: VGGNet의 마지막 hidden 레이어가 4096차원의 embedding으로 사용된다.</li>
      <li>norm I: 위와 비슷하나 $l_2$ 정규화된 활성함수를 사용</li>
    </ul>
  </li>
  <li><strong>Question Channel:</strong> 질문을 위한 embedding을 제공한다.
    <ul>
      <li>Bag-of-Words Question(BoW Q): 질문의 최빈 1000개의 단어와 30차원의 BoW를 사용하여 1030차원의 질문 embedding을 만든다.</li>
      <li>LSTM Q: 1024차원이다.</li>
      <li>deeper LSTM Q: 2048차원이다.</li>
    </ul>
  </li>
  <li><strong>Multi-Layer Perceptron(MLP):</strong>
    <ul>
      <li>BoW Q + I에 대해서는 단지 concatenate한다.</li>
      <li>LSTM Q + I, deeper LSTM Q + norm I에 대해서는 이미지 embedding은 차원을 맞추기 위해 1024차원으로 변환된 후 LSTM embedding과 element-wise하게 곱해진다.</li>
    </ul>
  </li>
</ul>

<h3 id="results">Results</h3>

<p>방법에 따라서는 28.13%/30.53%(각각 open-ended와 multiple-choice)를 나타낸 것부터 58.16%/63.09%를 나타낸 모델(deeper LSTM Q + norm I)까지 결과는 다양하다.<br />
따라서 적어도 60%는 넘어야 의미 있는 VQA 시스템이라고 할 수 있을 것이다.</p>

<hr />

<h2 id="vqa-challenge-and-workshop">VQA Challenge and Workshop</h2>

<p>CVPR 2016에서부터 1년 간격으로 열린다. 테스트 서버도 준비되어 있다.</p>

<hr />

<h2 id="결론-및-토의conclusion-and-discussion">결론 및 토의(Conclusion and Discussion)</h2>

<p>이 논문에서는 VQA task를 제안하였고, 그에 맞는 데이터를 제공하였다.<br />
우리는 VQA가 자동평가가 가능한 “AI-complete” 문제를 풀기 위한 한계를 끌어올리기에 적합하다고 생각한다. 이를 위한 노력에 드는 시간도 가치가 있다고 여겨진다.</p>

<hr />

<h2 id="참고문헌references">참고문헌(References)</h2>

<p>논문 참조!</p>

<hr />

<p>결론 이후에도 많은 정보가 있으니 참조하면 좋다. 매우 흥미로운 것들이 많다.<br />
대부분은 데이터의 분포에 관한 설명 및 시각화한 그림들이다.</p>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/computer%20vision/2019/04/17/Visual-Question-Answering/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/computer%20vision/2019/04/17/Visual-Question-Answering/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page7">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/page5">Newer</a>
    
  
</div>


  </div>
</div>

<label for="sidebar-checkbox" class="sidebar-toggle"></label>

<script>
  (function (document) {
    let toggle = document.querySelector('.sidebar-toggle');
    let sidebar = document.querySelector('#sidebar');
    let checkbox = document.querySelector('#sidebar-checkbox');

    document.addEventListener('click', function (e) {
      let target = e.target;

      if (target === toggle) {
        checkbox.checked = !checkbox.checked;
        e.preventDefault();
      } else if (checkbox.checked && !sidebar.contains(target)) {
        /* click outside the sidebar when sidebar is open */
        checkbox.checked = false;
      }
    }, false);
  })(document);
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    };
    i[r].l = 1 * new Date();
    a = s.createElement(o);
    m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-00000000-1', 'auto');
  ga('send', 'pageview');
</script>


<!-- Naver Analytics -->	
<script type="text/javascript" src="//wcs.naver.net/wcslog.js"></script>
<script type="text/javascript">
  if(!wcs_add) var wcs_add = {};
    wcs_add["wa"] = "18cbce78e94161";
  wcs_do();
</script>

</body>

<script id="dsq-count-scr" src="//greeksharifa-github-io.disqus.com/count.js" async></script>

</html>
