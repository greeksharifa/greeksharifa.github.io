<!DOCTYPE html>
<html lang="en-us">
<head>
  <head>
  <!-- Description of Blog -->
  <meta name="description" content="Python, Machine & Deep Learning">
  <link rel="canonical" href="https://greeksharifa.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Python, Machine & Deep Learning">
  <meta property="og:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta property="og:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta property="og:url" content="https://greeksharifa.github.io/">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python, Machine & Deep Learning">
  <meta name="twitter:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta name="twitter:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta name="twitter:domain" content="https://greeksharifa.github.io/">

  <!-- link -->
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="icon-144x144" sizes="144x144" href="/public/img/icon-144x144.png">
  <link rel="shortcut icon" href="/public/img/icon_32x32.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  

  <!-- Ads -->
  <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
  </script>
</head>

  <!-- for Google AdSense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9951774327887666",
    enable_page_level_ads: true
  });
</script>

  <style>blockquote {
    font-size: 1em;
    line-height: 1.4
  }</style>
  <link href='http://fonts.googleapis.com/css?family=Gill+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Consolas' rel='stylesheet' type='text/css'>
</head>
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/3c2986ad7ac1f2230ea3596f44563328">
          <img src="/public/img/maple_tree.jpg" title="Cover Photo" alt="Maple tree" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>Developer and Analyst</strong>, YW & YY.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/greeksharifa">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:greeksharifa@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item active" href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://greeksharifa.github.io/">
          Github Project
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 YW & YY. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://greeksharifa.github.io">YW & YY</a>
    </p>
  </div>
</div>


<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
     content to avoid any CSS collisions with our real content. -->
<div class="wrap">
  <div class="masthead">
    <div class="container">
      <h3 class="masthead-title" align="center">
        <a href="/" title="Home" title="YW & YY">
          <img class="masthead-logo" src="/public/img/logo.png"/>
        </a>
        <small>YW & YY's Python, Machine & Deep Learning</small>
        <!-- HTML elements for search -->
        <a href="/search/" id="search_icon">
          <img src="/public/img/search.png" width="25" height="25"
               align="right" style="margin-top:5px; margin-bottom:0;"
               onmouseover="this.style.opacity=0.7" onmouseout="this.style.opacity=0.5"
               alt="search">
        </a>
      </h3>
    </div>
  </div>

  <div class="container content">
    <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/github/2020/05/27/github-usage-09-overall/">
        GitHub 사용법 - 09. Overall
      </a>
    </h1>

    <span class="post-date">27 May 2020</span>
     |
    
    <a href="/blog/tags/#github" class="post-tag">GitHub</a>
    
    <a href="/blog/tags/#usage" class="post-tag">usage</a>
    
    

    <article>
      <p><strong><em>주의: 이 글을 읽는 여러분이, 만약 git을 많이 써 봐서 익숙한 것이 아니라면, 반드시 손으로 직접 따라 칠 것을 권한다. 눈으로만 보면 100% 잊어버린다.</em></strong></p>

<p><a href="https://greeksharifa.github.io/github/2018/08/19/github-usage-08-conflict/">저번 글</a>에서는 Conflict에 대해서 알아보았다.<br />
이번 글에서는, 전체 Git 명령어들의 사용법을 살펴본다.</p>

<hr />

<h2 id="git-directory-생성">Git Directory 생성</h2>

<h3 id="git-init">git init</h3>

<p>빈 디렉토리나, 기존의 프로젝트를 <strong>git 저장소</strong>(=<strong>git repository</strong>)로 변환하고 싶다면 이 문단을 보면 된다.</p>

<p>일반적인 디렉토리(=git 저장소가 아닌 디렉토리)를 git 디렉토리로 만드는 방법은 다음과 같다. <strong>명령창</strong>(cmd / terminal)에서 다음을 입력한다.</p>

<pre><code class="language-git">git init

# 결과 예시
Initialized empty Git repository in blabla/sample_directory/.git/
</code></pre>

<p>그러면 해당 디렉토리에는 <code class="highlighter-rouge">.git</code> 이라는 이름의 숨김처리된 디렉토리가 생성된다. 이 디렉토리 안에 든 것은 수동으로 건드리지 않도록 한다.</p>

<p>참고) <code class="highlighter-rouge">git init</code> 명령만으로는 인터넷(=<strong>원격 저장소</strong> = <strong>remote repository</strong>)에 그 어떤 연결도 되어 있지 않다. <a href="">여기</a>를 참조한다.</p>

<h3 id="git-clone">git clone</h3>

<p>인터넷에서 이미 만들어져 있는 git 디렉토리를 본인의 컴퓨터(=<strong>로컬</strong>)로 가져오고 싶을 때에는 해당 git repository의 <code class="highlighter-rouge">https://github.com/blabla.git</code> 주소를 복사한 뒤 다음과 같은 명령어를 입력한다.</p>

<pre><code class="language-git">git clone &lt;git-address&gt;

# 명령어 예시 
git clone https://github.com/greeksharifa/git_tutorial.git

# 결과 예시
Cloning into 'git_tutorial'...
remote: Enumerating objects: 56, done.
remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 56
Unpacking objects: 100% (56/56), done.
</code></pre>
<p>그러면 현재 폴더에 해당 프로젝트 이름의 하위 디렉토리가 생성된다. 이 하위 디렉토리에는 인터넷에 올라와 있는 모든 내용물을 그대로 가져온다(<code class="highlighter-rouge">.git</code> 디렉토리 포함).<br />
단, 다른 branch의 내용물을 가져오지는 않는다. 다른 branch까지 가져오려면 <a href="">추가 작업</a>이 필요하다.</p>

<hr />

<h2 id="git-repository-연결">Git Repository 연결</h2>

<p>로컬 저장소를 원격(remote) 저장소에 연결하는 방법은 다음과 같다.</p>

<p>git add remote origin</p>

<hr />

<h2 id="git-stage에-파일-추가">Git stage에 파일 추가</h2>

<p>로컬 저장소의 수정사항이 반영되는 과정은 총 3단계를 거쳐 이루어진다.</p>

<ol>
  <li><code class="highlighter-rouge">git add</code> 명령을 통해 stage에 변경된 파일을 추가하는 과정</li>
  <li><code class="highlighter-rouge">git commit</code> 명령을 통해 여러 변경점을 하나의 commit으로 묶는 과정</li>
  <li><code class="highlighter-rouge">git push</code> 명령을 통해 로컬 commit 내용을 원격 저장소에 올려 변경사항을 반영하는 과정</li>
</ol>

<p>이 중 <code class="highlighter-rouge">git add</code> 명령은 첫 단계인, <strong>stage</strong>에 파일을 추가하는 것이다.</p>

<pre><code class="language-git">git add &lt;filename1&gt; [&lt;filename2&gt;, ...]
git add &lt;directory-name&gt;
git add *
git add --all
git add .

# 명령어 예시
git add third.py fourth.py
git add temp_dir/*
</code></pre>

<p><code class="highlighter-rouge">*</code>은 와일드카드로 그냥 쓰면 변경점이 있는 모든 파일을 stage에 추가한다(<code class="highlighter-rouge">git add *</code>). 특정 directory 뒤에 쓰면 해당 directory의 모든 파일을, <code class="highlighter-rouge">*.py</code>와 같이 쓰면 확장자가 <code class="highlighter-rouge">.py</code>인 모든 파일이 stage에 올라가게 된다.<br />
<code class="highlighter-rouge">git add .</code>을 현재 directory(<code class="highlighter-rouge">.</code>)의 모든 파일을 추가하는 명령으로 <code class="highlighter-rouge">git add --all</code>과 효과가 같다.</p>

<p><code class="highlighter-rouge">git add</code> 명령을 실행하고 이미 stage에 올라간 파일을 또 수정한 뒤 <a href=""><code class="highlighter-rouge">git status</code></a> 명령을 실행하면 같은 파일이 Changes to be committed 분류와 Changes not staged for commit 분류에 동시에 들어가 있을 수 있다. 딱히 오류는 아니고 해당 파일을 다음 commit에 반영할 계획이면 한번 더 <code class="highlighter-rouge">git add</code>를 실행시켜주자.</p>

<h3 id="한-파일-내-수정사항의-일부만-stage에-추가">한 파일 내 수정사항의 일부만 stage에 추가</h3>

<p>예를 들어 <code class="highlighter-rouge">fourth.py</code>를 다음과 같이 변경한다고 하자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 변경 전
</span><span class="k">print</span><span class="p">(</span><span class="s">'hello'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'bye'</span><span class="p">)</span>

<span class="c1">#변경 후
</span>



</code></pre></div></div>

<hr />

<h2 id="git-directory-상태-확인">Git Directory 상태 확인</h2>

<h3 id="git-status">git status</h3>

<p>현재 git 저장소의 상태를 확인하고 싶다면 다음 명령어를 입력한다.</p>

<pre><code class="language-git">git status

# 결과 예시 1:
On branch master
Your branch is up to date with 'origin/master'.

nothing to commit, working tree clean

# 결과 예시 2:

On branch master
Your branch is up to date with 'origin/master'.

Changes to be committed:
  (use "git reset HEAD &lt;file&gt;..." to unstage)

        modified:   first.py

Changes not staged for commit:
  (use "git add/rm &lt;file&gt;..." to update what will be committed)
  (use "git checkout -- &lt;file&gt;..." to discard changes in working directory)

        modified:   .gitignore
        deleted:    second.py

Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)

        third.py
</code></pre>

<p><code class="highlighter-rouge">git status</code>로는 로컬 git 저장소에 변경점이 생긴 파일을 크게 세 종류로 나누어 보여준다.</p>

<p><strong>1. Changes to be committed</strong>
    - Tracking되는 파일이며, stage(스테이지)에 이름이 올라가 있는 파일들. 이 단계에 있는 파일들만이 commit 명령을 내릴 시 다음 commit에 포함된다. (그래서 to be commited이다)
    - 마지막 commit 이후 <code class="highlighter-rouge">git add</code> 명령으로 stage에 추가가 된 파일들.
<strong>2. Changes not staged for commit:</strong>
    - Tracking되는 파일이지만, 다음 commit을 위한 stage에 이름이 올라가 있지 않은 파일들. 
    - 마지막 commit 이후 <code class="highlighter-rouge">git add</code> 명령의 대상이 된 적 없는 파일들.
<strong>3. Untracked files:</strong>
    - Tracking이 안 되는 파일들. 
    - 생성 이후 한 번도 <code class="highlighter-rouge">git add</code> 명령의 대상이 된 적 없는 파일들.</p>

<p>위와 같이 stage 또는 tracked 목록에 올라왔는지가 1차 분류이고, 2차 분류는 해당 파일이 처음 생성되었는지(ex. <code class="highlighter-rouge">third.py</code>), 변경되었는지(modified), 삭제되었는지(deleted)로 나눈다.</p>

<hr />

<h2 id="history-검토">History 검토</h2>

<h3 id="git-log">git log</h3>

<p>저장소 commit 메시지의 모든 history를 역순으로 보여준다. 즉, 가장 마지막에 한 commit이 가장 먼저 보여진다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git log

<span class="c"># 결과 예시</span>
commit da446019230a010bf333db9d60529e30bfa3d4e3 <span class="o">(</span>HEAD -&gt; master, origin/master, origin/HEAD<span class="o">)</span>
Merge: 4a521c5 2eae048
Author: greeksharifa &lt;greeksharifa@gmail.com&gt;
Date:   Sun Aug 19 20:59:24 2018 +0900

    Merge branch <span class="s1">'3rd-branch'</span>

commit 2eae048f725c1d843cad359d655c193d9fd632b4
Author: greeksharifa &lt;greeksharifa@gmail.com&gt;
Date:   Sun Aug 19 20:29:48 2018 +0900

    Unwanted commit from 2nd-branch

...
:
</code></pre></div></div>

<p>이때 commit의 수가 많으면 다음 명령을 기다리는 커서가 깜빡인다. 여기서 space bar를 누르면 다음 commit들을 계속해서 보여주고, 끝에 다다르면(저장소의 최초 commit에 도달하면) <code class="highlighter-rouge">(END)</code>가 표시된다.<br />
끝에 도달했거나 이전 commit들을 더 볼 필요가 없다면, <code class="highlighter-rouge">q</code>를 누르면 log 보기를 중단한다(quit).</p>

<h4 id="git-log-옵션-patch-p--number-onelineprettyoneline">git log 옵션: –patch(-p), -&lt;number&gt;, –oneline(–pretty=oneline)</h4>

<p>각 commit의 diff 결과(commit의 세부 변경사항, 변경된 파일의 변경된 부분들을 보여줌)를 보고 싶으면 다음을 입력한다.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git log --patch

# 결과 예시
commit 2eae048f725c1d843cad359d655c193d9fd632b4
Author: greeksharifa <span class="nt">&lt;greeksharifa</span><span class="err">@</span><span class="na">gmail</span><span class="err">.</span><span class="na">com</span><span class="nt">&gt;</span>
Date:   Sun Aug 19 20:29:48 2018 +0900

    Unwanted commit from 2nd-branch

diff --git a/first.py b/first.py
index 2d61b9f..c73f054 100644
--- a/first.py
+++ b/first.py
@@ -9,3 +9,5 @@ print("This is the 1st sentence written in 3rd-branch.")
 print('2nd')

 print('test git add .')
+
+print("Unwanted sentence in 2nd-branch")
</code></pre></div></div>

<p>가장 최근의 commit들 3개만 보고 싶다면 다음과 같이 입력한다.</p>
<pre><code class="language-git">git log -3
</code></pre>

<p>commit의 대표 메시지와 같은 핵심 내용만 보고자 한다면 다음과 같이 입력한다.</p>
<pre><code class="language-git">git log --oneline

# 결과 예시
da44601 (HEAD -&gt; master, origin/master, origin/HEAD) Merge branch '3rd-branch'
2eae048 Unwanted commit from 2nd-branch
4a521c5 Desired commit from 2nd-branch
</code></pre>

<p>참고로, 다음과 같이 입력하면 commit의 고유 id의 전체가 출력된다.</p>
<pre><code class="language-git">git log --pretty=oneline

# 결과 예시
da446019230a010bf333db9d60529e30bfa3d4e3 (HEAD -&gt; master, origin/master, origin/HEAD) Merge branch '3rd-branch'
2eae048f725c1d843cad359d655c193d9fd632b4 Unwanted commit from 2nd-branch
4a521c56a6c2e50ffa379a7f2737b5e90e9e6df3 Desired commit from 2nd-branch
</code></pre>

<p>옵션들은 중복이 가능하다.</p>
<pre><code class="language-git">git log --oneline -5
</code></pre>

<hr />

<h2 id="git-branch">Git Branch</h2>

<h3 id="branch-목록-보기">branch 목록 보기</h3>

<p>로컬 branch 목록을 보려면 다음을 입력한다.</p>

<pre><code class="language-git">git branch
git branch --list
git branch -l

# 결과 예시
* master
</code></pre>

<p>branch 목록을 보여주는 모든 명령에서, 현재 branch(작업 중인 branch)는 맨 앞에 asterisk(<code class="highlighter-rouge">*</code>)가 붙는다.</p>

<p>모든 branch 목록 보기:</p>

<pre><code class="language-git">git branch --all
git branch -a

# 결과 예시
* master
  remotes/origin/2nd-branch
  remotes/origin/3rd-branch
  remotes/origin/HEAD -&gt; origin/master
  remotes/origin/master
</code></pre>

<p><code class="highlighter-rouge">remotes/</code>가 붙은 것은 원격 branch라는 뜻이며, branch의 이름에는 <code class="highlighter-rouge">remotes/</code>가 포함되지 않는다.</p>

<p>원격 branch 목록 보기:</p>

<pre><code class="language-git">git branch --remotes
git branch -r

# 결과 예시
  origin/2nd-branch
  origin/3rd-branch
  origin/HEAD -&gt; origin/master
  origin/master
</code></pre>

<h3 id="원격-branch-목록-업데이트">원격 branch 목록 업데이트</h3>

<p>로컬 저장소와 원격 저장소는 실시간 동기화가 이루어지는 것이 아니기 때문에(일부 git 명령을 내릴 때에만 통신이 이루어짐), 원격 branch 목록은 자동으로 최신으로 유지되지 않는다. 목록을 새로 확인하려면 다음을 입력한다.</p>

<pre><code class="language-git">git fetch
</code></pre>

<p>별다른 변경점이 없으면 아무 것도 표시되지 않는다.</p>

<hr />

<h3 id="branch-전환">branch 전환</h3>

<p>단순히 branch 간 전환을 하고 싶으면 다음 명령어를 입력한다.</p>

<pre><code class="language-git">git checkout &lt;branch-name&gt;

# 명령어 예시
git checkout master

# 결과 예시
Switched to branch 'master'
M       .gitignore
D       second.py
Your branch is ahead of 'origin/master' by 1 commit.
  (use "git push" to publish your local commits)
</code></pre>

<p>전환을 수행하면,</p>
<ul>
  <li>변경된 파일의 목록과</li>
  <li>현재 로컬 브랜치가 연결되어 있는 원격 브랜치 사이에 얼마만큼의 commit 차이가 있는지</li>
</ul>

<p>도 알려준다.</p>

<p>로컬에 새 branch를 생성하되, 그 내용을 원격 저장소에 있는 어떤 branch의 내용으로 하고자 하면 다음 명령을 사용한다.</p>

<pre><code class="language-git">git checkout --track -b &lt;local-branch-name&gt; &lt;remote-branch-name&gt;

# 명령어 예시
git checkout --track -b 2nd-branch origin/2nd-branch

# 결과 예시
Switched to a new branch '2nd-branch'
M       .gitignore
D       second.py
Branch '2nd-branch' set up to track remote branch '2nd-branch' from 'origin'.
</code></pre>

<p>출력에서는 <code class="highlighter-rouge">2nd-branch</code>라는 이름의 새 branch로 전환하였고, 파일의 현재 수정 사항을 간략히 보여주며, 로컬 branch <code class="highlighter-rouge">2nd-branch</code>가 <code class="highlighter-rouge">origin</code>의 원격 branch <code class="highlighter-rouge">2nd-branch</code>를 추적하게 되었음을 알려준다.<br />
즉 원격 branch의 로컬 사본이 생성되었음을 알 수 있다.</p>

<h3 id="새-branch-생성">새 branch 생성</h3>

<pre><code class="language-git">git branch &lt;new-branch-name&gt;

# 명령어 예시
git branch fourth-branch
</code></pre>

<p>위 명령은 branch를 생성만 한다. 생성한 브랜치에서 작업을 시작하려면 checkout 과정을 거쳐야 한다.</p>

<h3 id="branch-생성과-같이-checkout하기">branch 생성과 같이 checkout하기</h3>

<pre><code class="language-git">git checkout -b &lt;new-branch-name&gt; &lt;parent-branch-name&gt;

# 명령어 예시
git checkout -b fourth-branch master

# 결과 예시
Switched to a new branch 'fourth-branch'
</code></pre>

<p>새로운 branch는 부모 브랜치와</p>

    </article>
    <div class="post-more">
      
      <a href="/github/2020/05/27/github-usage-09-overall/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/github/2020/05/27/github-usage-09-overall/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/generative/model/2020/05/25/VAE/">
        Variational AutoEncoder 설명
      </a>
    </h1>

    <span class="post-date">25 May 2020</span>
     |
    
    <a href="/blog/tags/#machine-learning" class="post-tag">Machine Learning</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <p>본 글의 주제는  2014년에 발표된 생성 모델인 Variational AutoEncoder에 대해 설명하고 이를 코드로 구현하는 내용을 담고 있다.</p>

<h2 id="1-auto-encoding-variational-bayes-논문-리뷰">1. Auto-Encoding Variational Bayes 논문 리뷰</h2>
<h3 id="11-introduction">1.1. Introduction</h3>
<p>연속형 잠재 변수와 파라미터가 다루기 힘든 사후 분포를 갖는 방향성 확률 모델에 대해 효율적인 근사 추론 및 학습을 수행할 수 있는 방법이 없을까? <strong>Variational Bayesian</strong> 접근법은 다루기 힘든 사후 분포에 대한 근사의 최적화를 내포한다.</p>

<p>불행히도, 일반적인 평균 필드(mean-field) 접근법은 근사적 사후 분포에 대해 기댓값의 분석적 해결법을 요구하는데 이는 보통 굉장히 다루기 어려운 방법이다. 본 논문은 Variational Lower Bound의 <strong>Reparameterization</strong>이 Lower Bound의 미분 가능한 불편향 estimator를 만드는 방법에 대해 보여줄 것이다. 이 <strong>Stochastic Gradient Variational Bayes: SGVB estimator</strong>는 연속형 잠재변수나 파라미터를 갖고 있는 대부분의 모델에 대해 효율적인 근사 사후 추론을 가능하게 하며, 표준 Stochastic Gradient Ascent 스킬을 사용하여 최적화하기에 굉장히 편리하다.</p>

<p>IID 데이터셋이고, 데이터포인트 별로 연속형 잠재변수를 갖고 있는 경우에 대해 본 논문은 <code class="highlighter-rouge">Auto-Encoding VB</code> 알고리즘을 제안한다. 이 알고리즘에서는 <strong>Simple Ancestral Sampling</strong>을 이용하여 근사 사후 추론을 하는 인식 모델을 최적화하기 위해 SGVB estimator를 사용하여 추론과 학습을 효율적으로 해낸다. 이 과정은 MCMC와 같이 데이터포인트 별로 반복적인 추론을 행하여 많은 연산량을 요구하지 않는 장점을 가진다.</p>

<p>학습된 근사 사후 추론 모델은 recognition, denoising, representation, visualization의 목적으로 활용될 수 있다. 본 알고리즘이 인식(recognition) 모델에 사용될 때, 이를 <code class="highlighter-rouge">Variational Auto-Encoder</code>라고 부를 것이다.</p>

<h3 id="12-method">1.2. Method</h3>
<p>본 섹션에서는 연속형 잠재 변수를 내포하는 다양한 방향성 그래픽 모델에서 Stochastic 목적 함수인 <strong>Lower Bound Estimator</strong>를 끌어내는 과정을 설명할 것이다. 데이터포인트 별 잠재변수는 iid한 상황이라는 가정 하에 본 논문에서는 파라미터에 대해 Maximul Likelihood와 Maximum Posteriori 추론을 수행하고 잠재변수에 대해 <strong>Variational Inference</strong>를 수행할 것이다. 이러한 방법은 온라인 러닝에도 사용될 수 있지만 본 논문에서는 간단히 하기 위해 고정된 데이터셋을 사용할 것이다.</p>

<h4 id="121-problem-scenario">1.2.1. Problem Scenario</h4>
<p>N개의 Sample을 가진 $X$라는 데이터가 있다고 해보자. 본 논문은 이 데이터가 관측되지 않은 연속형 확률 변수 $z$를 내포하는 어떤 Random Process에 의해 형성되었다고 가정한다.</p>

<p>이 과정은 2가지 단계로 구성된다.<br />
1) $z^{i}$라는 값은 어떤 사전 분포 $p_{\theta ^<em>}(z)$에서 발생한다.<br />
2) $x^{i}$라는 값은 어떤 조건부 분포 $p_{\theta ^</em>}(x|z)$에서 발생한다.</p>

<p>(여기서 $z$는 원인, $x$는 결과라고 보면 이해가 쉬울 것이다.)</p>

<table>
  <tbody>
    <tr>
      <td>우리는 사전확률 $p_{\theta <em>}(z)$와 Likelihood $p_{\theta ^</em>}(x</td>
      <td>z)$가 $p_{\theta}(z)$, $p_{\theta}(x</td>
      <td>z)$의 parametric families of distributions에서 왔다고 가정하고, 이들의 확률밀도함수는 거의 모든 $\theta, z$에 대해 미분가능하다고 전제한다.</td>
    </tr>
  </tbody>
</table>

<p>불행히도, 이러한 과정의 많은 부분은 우리가 직접 확인하기 어렵다. True 파라미터인 $\theta ^*$와 잠재 변수의 값 $z^{i}$은 우리에게 알려져 있지 않다.</p>

<p>본 논문은 주변 확률이나 사후 확률에 대한 단순화를 위한 일반적인 가정을 취하지 않고 분포가 다루기 힘들고 큰 데이터셋을 마주하였을 경우를 위한 효율적인 알고리즘에 대해 이야기하고자 한다.</p>

<p><strong>1) Intractability</strong>(다루기 힘듦)<br />
(1) marginal likelihood $p_{\theta}(x)$의 적분인 $\int p_{\theta}(x) p_{\theta}(x|z) dz $가 다루기 힘든 경우</p>

<table>
  <tbody>
    <tr>
      <td>(2) true posterior density $p_{\theta}(z</td>
      <td>x) = p_{\theta}(x</td>
      <td>z)p_{\theta}(z)/p_{\theta}(x)$가 다루기 힘들어 EM 알고리즘이 사용될 수 없는 경우</td>
    </tr>
  </tbody>
</table>

<p>(3) 어떠한 합리적인 평균-필드 VB알고리즘을 위한 적분이 다루기 힘든 경우</p>

<table>
  <tbody>
    <tr>
      <td>이러한 Intractability는 굉장히 흔하며, 복잡한 우도(likelihood) 함수 $p_{\theta}(x</td>
      <td>z)$를 갖는 신경망 네트워크에서 발견할 수 있다.</td>
    </tr>
  </tbody>
</table>

<p><strong>2) A Large Dataset</strong><br />
데이터가 너무 크면 배치 최적화는 연산량이 매우 많다. MC-EM과 같은 Sampling Based Solution은 데이터 포인트별로 Sampling Loop를 돌기 때문에 너무 느리다.</p>

<p>위 시나리오에서 설명한 문제들에 대해 본 논문은 아래와 같은 해결책을 제시한다.</p>

<p>1) 파라미터 $\theta$에 대한 효율적인 근사 ML/MAP estimation. 이 파라미터들은 숨겨진 랜덤 과정을 흉내내고 실제 데이터를 닮은 인공적인 데이터를 생성할 수 있게 해준다.<br />
2) 파라미터 $\theta$의 선택에 따라 관측값 $x$이 주어졌을 때 잠재 변수 $z$에 대한 효율적인 근사 사후 추론<br />
3) 변수 $x$에 대해 효율적인 근사 주변 추론. 이는 $x$에 대한 prior이 필요한 모든 추론 task를 수행할 수 있게 해준다.</p>

<table>
  <tbody>
    <tr>
      <td>위 문제를 해결하기 위해 인식 모델 $q_{\phi}(z</td>
      <td>x)$이 필요하다. 이 모델은 다루기 힘든 True Posterior $p_{\theta}(z</td>
      <td>x)$의 근사 버전이라고 할 수 있다. 본 논문에서는 인식 모델 파라미터인 $\phi$와 생성 모델 파라미터인 $\theta$를 동시에 학습하는 방법에 대해 이야기할 것이다.</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>코딩 이론의 관점에서 보면, 관측되지 않은 변수 $z$는 잠재 표현 또는 <em>code</em>라고 해석될 수 있다. 본 논문에서는 따라서 인식 모델 $q_{\phi}(z</td>
      <td>x)$를 <strong>encoder</strong>라고 부를 것인데, 왜냐하면 데이터 포인트 $x$가 주어졌을 때 이 <strong>encoder</strong>가 데이터 포인트 $x$가 발생할 수 code $z$의 가능한 값에 대한 분포를 생산하기 때문이다. 비슷한 맥락에서 우리는 $q_{\theta}(x</td>
      <td>z)$를 <strong>확률적 decoder</strong>라고 명명할 것인데, 왜냐하면 code $z$가 주어졌을 때 이 <strong>decoder</strong>가 상응하는 가능한 $x$의 값에 대해 분포를 생산하기 때문이다.</td>
    </tr>
  </tbody>
</table>

<h3 id="22-the-variational-bound">2.2. The Variational Bound</h3>

<hr />

<h2 id="2-이론에-대한-보충-설명">2. 이론에 대한 보충 설명</h2>
<h3 id="21-용어-정리">2.1. 용어 정리</h3>
<p><strong>1) Variational Inference</strong><br />
$q(x)$라는 쉬운 분포를 통해 target 분포 $p(x)$를 근사 추론하는 방법론이다.</p>

<script type="math/tex; mode=display">q^* = argmin_{q \in Q} KL(q||p)</script>

<p><strong>2) KL Divergence</strong></p>

<p><strong>3) s</strong></p>

<h3 id="22">2.2.</h3>

<hr />

<h2 id="reference">Reference</h2>
<p>1) https://ratsgo.github.io/generative%20model/2018/01/27/VAE/<br />
2) https://www.youtube.com/watch?v=SAfJz_uzaa8<br />
3) https://taeu.github.io/paper/deeplearning-paper-vae/
4)</p>

    </article>
    <div class="post-more">
      
      <a href="/generative/model/2020/05/25/VAE/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/generative/model/2020/05/25/VAE/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/machine_learning/2020/05/01/AFM/">
        추천 시스템의 기본 - 06. AFM 논문 리뷰 및 Tensorflow 구현
      </a>
    </h1>

    <span class="post-date">01 May 2020</span>
     |
    
    <a href="/blog/tags/#machine-learning" class="post-tag">Machine_Learning</a>
    
    <a href="/blog/tags/#recommendation-system" class="post-tag">Recommendation System</a>
    
    <a href="/blog/tags/#afm" class="post-tag">AFM</a>
    
    

    <article>
      <p>본 글의 전반부에서는 먼저 <strong>Attentional Factorization Machines: Learning theWeight of Feature Interactions via Attention Networks</strong> 논문을 리뷰하면서 본 모델에 대해 설명할 것이다. 후반부에서는 Tensorflow를 이용하여 직접 코딩을 하고 학습하는 과정을 소개할 것이다. 논문의 전문은 <a href="https://www.ijcai.org/Proceedings/2017/0435.pdf">이곳</a>에서 확인할 수 있다.</p>

<hr />

<h2 id="1-attentional-factorization-machines-learning-theweight-of-feature-interactions-via-attention-networks-논문-리뷰">1. Attentional Factorization Machines: Learning theWeight of Feature Interactions via Attention Networks 논문 리뷰</h2>

<h3 id="10-absbract">1.0. Absbract</h3>
<p>FM은 2차원 피쳐 상호작용을 잘 통합하여 선형 회귀를 개선한 지도학습 알고리즘이다. 이 알고리즘은 효과적이긴 하지만, 모든 피쳐에 대해 같은 weight로 학습을 진행시킨다는 점에서 비효율적이다. 왜냐하면 종종 일부 피쳐는 학습에 있어 필수적이지 않은 경우가 있기 때문이다. 오히려 이러한 피쳐들의 존재는 모델의 성능을 떨어트릴 수 있다. 따라서 우리는 여러 피쳐 상호작용 속에서 중요한 피쳐들을 구분해내는 새로운 모델, <strong>Attentional Factorization Machine (AFM)</strong>을 소개한다.</p>

<h3 id="11-introduction">1.1. Introduction</h3>
<center> (전략) </center>

<p>FM은 피쳐 상호작용의 중요성을 구분하는 능력이 부족하기 때문에(피쳐의 중요성을 파악하는 능력) suboptimal 문제에 빠질 수 있다. <strong>AFM</strong>은 이러한 문제를 해결하기 위해 도입한 모델이다.</p>

<h3 id="12-factorization-machines">1.2. Factorization Machines</h3>
<p>FM 모델에 대한 설명은 <a href="2019-12-21-FM.md">이곳</a>을 참조하길 바란다. 기호에 대해서만 설명을 추가하면, $v_i$는 피쳐 $i$에 대한 임베딩 벡터이며, $k$는 임베딩 크기를 의미한다.</p>

<h3 id="13-attentioanl-factorization-machines">1.3. Attentioanl Factorization Machines</h3>
<h4 id="131-model">1.3.1. Model</h4>
<center><img src="/public/img/Machine_Learning/2020-05-01-AFM/01.JPG" width="100%" /></center>

<p>위 그림은 <strong>AFM</strong>의 구조를 보여준다. 선명히 보여주기 위해 그림에서는 선형 회귀 부분을 생략하였다. Input Layer와 Embedding Layer의 경우 FM과 같은 구조를 지니는데, Input 피쳐들은 sparse하게 이루어져있고 이들은 dense vector로 임베딩된다. 지금부터는 본 모델의 핵심인 <code class="highlighter-rouge">pair-wise interaction layer</code>과 <code class="highlighter-rouge">attention-based pooling layer</code>를 설명할 것이다.</p>

<p><strong>Pair-wise Interaction Layer</strong><br />
상호작용을 포착하기 위해 내적을 사용하는 FM을 참고하여, 본 논문에서는 신경망 모델링에서 새로운 <code class="highlighter-rouge">Pair-wise Interaction Layer</code>를 제시한다. $m$개의 벡터를 $\frac{m(m-1)}{2}$개의 interacted 벡터로 만드는데, 이 때 각 interacted 벡터는 상호작용을 포착하기 위해 2개의 다른 벡터들의 원소곱으로 계산된다.</p>

<p>정확히 말하면, 피쳐 벡터 $x$의 0이 아닌 피쳐의 집합을 $\chi$라고 하자. 그리고 <code class="highlighter-rouge">Embedding Layer</code>의 결과물을 $\epsilon = {{v_i x_i}}_{i \in \chi} $라고 하자. 우리는 아래와 같이 <code class="highlighter-rouge">Pair-wise Interaction Layer</code>의 결과물을 아래와 같은 벡터의 집합으로 표현할 수 있다.</p>

<script type="math/tex; mode=display">f_{PI}(\epsilon) = \{ (v_i \odot v_j) x_i x_j \}_{(i, j \in R_x)}</script>

<ul>
  <li>$\odot$ 기호: 원소곱</li>
  <li>$ R_x = { (i, j) }_{i, j \in \chi, j&gt;i} $</li>
</ul>

<p>이 Layer를 정의하면서 우리는 FM을 신경망 구조로 표현할 있게 된다. 먼저 $f_{PI}(\epsilon)$를 <strong>sum pooling</strong>으로 압축한다음, <strong>Fully Connected Layer</strong>를 사용하여 prediction score에 투사(project)한다.</p>

<script type="math/tex; mode=display">\hat{y} = p^T \sum_{(i, j) \in R_x} (v_i \odot v_j) x_i x_j + b</script>

<ul>
  <li>$p \in R^k$</li>
  <li>$b \in R$</li>
</ul>

<p>위에서 등장한 <strong>p, b</strong>는 <code class="highlighter-rouge">Prediction Layer</code>의 weight과 bias이다. 물론 p=1, b=0으로 값을 고정한다면 이는 FM과 동일한 형상을 취하게 될 것이다.</p>

<p><strong>Attention-based Pooling Layer</strong><br />
Attention의 기본 아이디어는, 여러 개의 부분이 압축 과정에 있어서 각각 다르게 기여하여 하나로 표현되게 만드는 것이다. interacted 벡터들의 가중 합을 수행하여 피쳐 상호작용에 대해 Attention 메커니즘을 적용하였다.</p>

<script type="math/tex; mode=display">f_{Att}(f_{PI}(\epsilon)) = a_{i,j} \sum_{(i, j) \in R_x} (v_i \odot v_j) x_i x_j</script>

<p>여기서 $a_{i, j}$는 피쳐 상호작용 $\hat{w}_{ij}$의 <strong>Attention Score</strong>이다.</p>

<p>Prediction Loss를 최소화하여 직접적으로 학습을 진행하여 $a_{i,j}$를 추정하는 것이 기술적으로는 맞게 느껴지지만, 학습 데이터에서 한 번도 동시에 등장한 적이 없는 피쳐들의 경우, 이들의 상호작용에 대한 <strong>Attention Score</strong>는 추정될 수 없다.</p>

<p>이러한 일반화 문제를 해결하기 위해 MLP를 통해 <strong>Attention Score</strong>를 파라미터화 하는 <strong>Attention Network</strong>를 추가하였다. 이 네트워크의 Input은 2개의 피쳐의 interacted 벡터인데, 이들의 상호작용 정보는 임베딩 공간에 인코딩된다.</p>

<p><script type="math/tex">e_{ij} = h^T ReLU(W (v_i \odot v_j) x_i x_j + b)</script><br />
<script type="math/tex">a_{ij} = \frac {exp(e_{ij})} { \sum_{(i, j) \in R_x} exp(e_{ij}) }</script></p>

<ul>
  <li>$W \in R^{t*k}, b \in R^t, h \in R^t$</li>
  <li>$t$: Attention Network의 hidden layer의 크기(Attention Factor)</li>
</ul>

<p><strong>Attention Score</strong>는 softmax 함수를 통해 정규화된다. 이 <code class="highlighter-rouge">Attention-based Pooling Layer</code>의 결과물은 k 차원의 벡터로, 중요성을 구별하여 임베딩 공간에서의 모든 피쳐 상호작용을 압축한 것이다. 요약하자면, <strong>AFM</strong> 모델의 최종 공식은 아래와 같다.</p>

<script type="math/tex; mode=display">\hat{y}_{AFM}(x) = w_0 + \sum_{i=1}^n w_i x_i + p^T \sum_{i=1}^n \sum_{j=i+1}^n a_{ij} (v_i \odot v_j) x_i x_j</script>

<p>모델 파라미터들은 $ w_0, w, v, p, W, b, h $이다.</p>

<h4 id="132-learning">1.3.2. Learning</h4>
<p><strong>AFM</strong>이 데이터 모델링의 관점에서 FM을 개선함에 따라 본 모델은 예측, 회귀, 분류, 랭킹 문제 등에 다양하게 적용될 수 있다. 목적 함수를 최적화하기 위해 SGD를 사용하였다. SGD 알고리즘 적용의 핵심은, 각 파라미터를 기준으로 예측 모델 <strong>AFM</strong>의 derivative를 구하는 것이다.</p>

<p><strong>과적합 문제</strong><br />
FM보다 표현력이 뛰어난 <strong>AFM</strong>이기에 더욱 과적합 문제에 민감할 수 있다. 따라서 본 모델에서는 dropout과 L2 Regularization 테크닉이 사용되었다.</p>

<p>(후략)</p>

<hr />

<h2 id="2-tensorflow를-활용한-구현">2. Tensorflow를 활용한 구현</h2>
<h3 id="21-데이터-준비">2.1. 데이터 준비</h3>
<p>본 모델의 경우 Dataset에 대한 Domain 지식이 필요하다고 볼 수는 없지만, 학습을 진행하기에 앞서 기본적으로 직접 전처리를 해주어야 하는 부분들이 있다. One-Hot 인코딩 외에도, 본 모델은 앞서 논문 리뷰에서도 확인하였듯이 0이 아닌 값에 대해서만 Lookup을 수행하여 실제 학습 데이터를 사용하기 때문에 이에 대한 정보를 저장해야할 필요가 있다. 아래 예시를 잠시 살펴보면,</p>

<center><img src="/public/img/Machine_Learning/2020-05-01-AFM/02.JPG" width="100%" /></center>

<p>만약 연속형 변수 중에 0.0이라는 값이 존재하더라도 사실 이 값은 중요한 특성을 나타낼 수도 있다. 그러나 논문의 기본 논조대로라면, 0인 값이기 때문에 학습에서 제외되게 된다. 이렇게 0이라고 해서 중요한 값이 학습에서 제외되는 현상을 막기 위해 본 구현에서는 One-Hot 인코딩 이후의 데이터에 대하여 중요한 정보의 위치를 저장하는 masking 작업을 진행하게 된다.</p>

<p>데이터는 <a href="2020-04-07-DeepFM.md">DeepFM 구현글</a>에서 사용한 것과 동일하다. 데이터 전처리는 연속형 변수에 대해서는 MinMaxScale, 범주형 변수에 대해서는 One-Hot 인코딩만을 진행하게 된다.</p>

<h3 id="22-layer-정의">2.2. Layer 정의</h3>
<p><strong>AFM</strong> 모델에서는 크게 3개의 Layer가 필요하다. <code class="highlighter-rouge">Embedding Layer</code>, <code class="highlighter-rouge">Pairwise Interaction Layer</code>, <code class="highlighter-rouge">Attention Pooling Layer</code>가 바로 그 3가지이다. <code class="highlighter-rouge">Embedding Layer</code> 부분은 이전 글(논문)들을 읽었다면, 굉장히 익숙하게 받아들여 질 것이다. 다만 이전 <a href="2020-04-07-DeepFM.md">DeepFM 구현글</a>에서는 하나의 Field에 대해 하나의 Embedding Row가 학습되었다면, 본 글에서는 하나의 Feature에 대해 하나의 Embedding Row가 학습되도록 코드를 수정하였다.</p>

<p>앞서 언급하였듯이 One-Hot 인코딩으로 생성된 0 값을 갖는 feature를 제외한 feature들만 실제 학습에 사용되는데(예를 들어 One-Hot 인코딩 이후에 0.2, 7.4, 0, 1, … 0, 1와 같은 데이터로 변환되었다면 실제 학습에 사용되는 데이터는 0.2, 7.4, 1, … 1이라는 뜻이다.)</p>

<p>위와 같은 논리를 구현하는 방법에는 여러가지가 있을 수 있겠지만 본 구현에서는 다음과 같은 논리를 따랐다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1) 연속형 변수들은 모두 앞쪽에 배치한 후, 이들에게는 무조건 True Mask를 씌워 학습 데이터로 활용한다.  
2) 범주형 변수들에 대해서는 0이 아닌 값들에 대해서 True Mask를 씌워 학습 데이터로 활용한다.  
</code></pre></div></div>

<p>논리 자체는 간단하며, 아래 call 메서드에서 그 논리가 구현되어 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">config</span>


<span class="k">class</span> <span class="nc">Embedding_layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_field</span><span class="p">,</span> <span class="n">num_feature</span><span class="p">,</span> <span class="n">num_cont</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Embedding_layer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>    <span class="c1"># k: 임베딩 벡터의 차원(크기)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span> <span class="o">=</span> <span class="n">num_field</span>              <span class="c1"># m: 인코딩 이전 feature 수
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_feature</span> <span class="o">=</span> <span class="n">num_feature</span>          <span class="c1"># p: 인코딩 이후 feature 수, m &lt;= p
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_cont</span> <span class="o">=</span> <span class="n">num_cont</span>                <span class="c1"># 연속형 field 수
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_cat</span>  <span class="o">=</span> <span class="n">num_field</span> <span class="o">-</span> <span class="n">num_cont</span>    <span class="c1"># 범주형 field 수
</span>
        <span class="c1"># Parameters
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_feature</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">),</span>
                                              <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'V'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># inputs: (None, p, k), embeds: (None, m, k)
</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># 원핫인코딩으로 생성된 0을 제외한 값에 True를 부여한 mask(np.array): (None, m)
</span>        <span class="c1"># indices: 그 mask의 indices
</span>        <span class="n">cont_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cont</span><span class="p">),</span> <span class="n">fill_value</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">cat_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_cont</span><span class="p">:],</span> <span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">cont_mask</span><span class="p">,</span> <span class="n">cat_mask</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">flatten_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">flatten_indices</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span><span class="p">))</span>

        <span class="c1"># embedding_matrix: (None, m, k)
</span>        <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

        <span class="c1"># masked_inputs: (None, m, 1)
</span>        <span class="n">masked_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">),</span>
                                   <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="n">masked_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">masked_inputs</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">)</span>    <span class="c1"># (None, m, k)
</span>
        <span class="k">return</span> <span class="n">masked_inputs</span>
</code></pre></div></div>

<p>다음은 <code class="highlighter-rouge">Pairwise Interaction Layer</code>에 대한 설명이다. 만약 14개의 Row가 존재한다면 이에 대한 모든 조합을 구하여 91 = $14\choose2$ 개의 Row를 생성하는 Layer인데, 간단하게 생각해보면 아래와 같이 코드를 짜고 싶을 것이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">combinations</span>

<span class="n">interactions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">comb_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_field</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">comb_list</span><span class="p">,</span> <span class="mi">2</span><span class="p">)):</span>
        <span class="n">interactions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">inputs</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="p">:]))</span>

<span class="n">pairwise_interactions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">interactions</span><span class="p">),</span>
                                    <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">))</span>
</code></pre></div></div>

<p>하지만 위와 같이 loop를 돌리게 되면, 속도가 현저하게 느려져서 실 사용이 불가능하다. 따라서 이 때는 Trick이 필요한데, 그림으로 설명하면 아래와 같다.</p>

<center><img src="/public/img/Machine_Learning/2020-05-01-AFM/03.JPG" width="100%" /></center>

<p>위 그림에서 14는 <code class="highlighter-rouge">num_field</code>의 예시이고, 5는 <code class="highlighter-rouge">embedding_size</code>의 예시이다. 가장 왼쪽에 있는 그림은 <code class="highlighter-rouge">Embedding Layer</code>를 통과한 Input 행렬을 그대로 <code class="highlighter-rouge">num_field</code> 수 만큼 쌓은 형태이이고, 그 오른쪽 그림은 똑같은 행들을 <code class="highlighter-rouge">num_field</code> 수만큼 쌓은 형태이다. 이렇게 쌓은 두 행렬 집단을 그대로 원소곱을 하게 되면 마치 조합을 구해서 곱을 한 것과 같은 형태가 나온다. 여기서 필요한 행들만 masking을 통해 취하면, 제일 오른쪽과 같은 결과물을 얻을 수 있다.</p>

<p>이를 코드를 구현한 것이 아래이다. <strong>tf.tile</strong>, <strong>tf.expand_dims</strong> 함수를 잘 이용하면 이 Trick을 코드로 구현할 수 있다. 직접 해보길 바란다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Pairwise_Interaction_Layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_field</span><span class="p">,</span> <span class="n">num_feature</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Pairwise_Interaction_Layer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>    <span class="c1"># k: 임베딩 벡터의 차원(크기)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span> <span class="o">=</span> <span class="n">num_field</span>              <span class="c1"># m: 인코딩 이전 feature 수
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_feature</span> <span class="o">=</span> <span class="n">num_feature</span>          <span class="c1"># p: 인코딩 이후 feature 수, m &lt;= p
</span>
        <span class="n">masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">MASKS</span><span class="p">)</span>    <span class="c1"># (num_field**2)
</span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>             <span class="c1"># (num_field**2, 1)
</span>        <span class="n">masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">])</span>   <span class="c1"># (num_field**2, embedding_size)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>         <span class="c1"># (1, num_field**2, embedding_size)
</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># a, b shape: (batch_size, num_field^2, embedding_size)
</span>        <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">])</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># ab, mask_tensor: (batch_size, num_field^2, embedding_size)
</span>        <span class="n">ab</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">mask_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">masks</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># pairwise_interactions: (batch_size, num_field C 2, embedding_size)
</span>        <span class="n">pairwise_interactions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">ab</span><span class="p">,</span> <span class="n">mask_tensor</span><span class="p">),</span>
                                           <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">pairwise_interactions</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">config.MASKS</code>는 아래와 같이 구현되어 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MASKS</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_FIELD</span><span class="p">):</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">i</span>

    <span class="n">MASKS</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="bp">False</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">flag</span><span class="p">))</span>
    <span class="n">MASKS</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="bp">True</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">NUM_FIELD</span> <span class="o">-</span> <span class="n">flag</span><span class="p">))</span>
</code></pre></div></div>

<p>다음으로는 마지막 <code class="highlighter-rouge">Attention Pooling Layer</code>이다. 설명할 것이 많지 않은 간단한 구조이다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Attention_Pooling_Layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Attention_Pooling_Layer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>    <span class="c1"># k: 임베딩 벡터의 차원(크기)
</span>
        <span class="c1"># Parameters
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
                                              <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'h'</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">),</span>
                                              <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'W_attention'</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>


    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># 조합 수 = combinations(num_feauture, 2)
</span>        <span class="c1"># inputs: (None, 조합 수, embedding_size)
</span>        <span class="c1"># --&gt; (전치 후) (None, embedding_size, 조합 수)
</span>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># e: (None, 조합 수, 1)
</span>        <span class="n">e</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">))</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Attention Score 산출
</span>        <span class="n">attention_score</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">attention_score</span>
</code></pre></div></div>

<h3 id="23-model-build">2.3. Model Build</h3>
<p>위에서 설명한 모든 Layer들을 이어 붙이면 <strong>AFM</strong> 모델이 완성된다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model 정의
</span><span class="kn">from</span> <span class="nn">layers</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_floatx</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">AFM</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_field</span><span class="p">,</span> <span class="n">num_feature</span><span class="p">,</span> <span class="n">num_cont</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AFM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>    <span class="c1"># k: 임베딩 벡터의 차원(크기)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span> <span class="o">=</span> <span class="n">num_field</span>              <span class="c1"># m: 인코딩 이전 feature 수
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_feature</span> <span class="o">=</span> <span class="n">num_feature</span>          <span class="c1"># p: 인코딩 이후 feature 수, m &lt;= p
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_cont</span> <span class="o">=</span> <span class="n">num_cont</span>                <span class="c1"># 연속형 field 수
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>          <span class="c1"># Attention Pooling Layer Hidden Unit 수
</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding_layer</span><span class="p">(</span><span class="n">num_field</span><span class="p">,</span> <span class="n">num_feature</span><span class="p">,</span>
                                               <span class="n">num_cont</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_interaction_layer</span> <span class="o">=</span> <span class="n">Pairwise_Interaction_Layer</span><span class="p">(</span>
            <span class="n">num_field</span><span class="p">,</span> <span class="n">num_feature</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_pooling_layer</span> <span class="o">=</span> <span class="n">Attention_Pooling_Layer</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

        <span class="c1"># Parameters
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">w_0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">num_feature</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                              <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">DROPOUT_RATE</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s">"AFM Model: embedding{}, hidden{}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># 1) Linear Term: (None, )
</span>        <span class="n">linear_terms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_0</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">inputs</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 2) Interaction Term
</span>        <span class="n">masked_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">pairwise_interactions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_interaction_layer</span><span class="p">(</span><span class="n">masked_inputs</span><span class="p">)</span>

        <span class="c1"># Dropout and Attention Score
</span>        <span class="n">pairwise_interactions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">pairwise_interactions</span><span class="p">)</span>
        <span class="n">attention_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_pooling_layer</span><span class="p">(</span><span class="n">pairwise_interactions</span><span class="p">)</span>

        <span class="c1"># (None, 조합 수, embedding_size)
</span>        <span class="n">attention_interactions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">pairwise_interactions</span><span class="p">,</span> <span class="n">attention_score</span><span class="p">)</span>

        <span class="c1"># (None, embedding_size)
</span>        <span class="n">final_interactions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">attention_interactions</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 3) Final: (None, )
</span>        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">linear_terms</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">final_interactions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_pred</span>
</code></pre></div></div>

<h3 id="24-코드-전문">2.4. 코드 전문</h3>
<p>코드의 전문은 <a href="https://github.com/ocasoyy/Recommendation-Algorithms">깃헙</a>에서 확인할 수 있다.</p>


    </article>
    <div class="post-more">
      
      <a href="/machine_learning/2020/05/01/AFM/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/machine_learning/2020/05/01/AFM/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/machine_learning/2020/04/07/DeepFM/">
        추천 시스템의 기본 - 05. DeepFM 논문 리뷰 및 Tensorflow 구현
      </a>
    </h1>

    <span class="post-date">07 Apr 2020</span>
     |
    
    <a href="/blog/tags/#machine-learning" class="post-tag">Machine_Learning</a>
    
    <a href="/blog/tags/#recommendation-system" class="post-tag">Recommendation System</a>
    
    <a href="/blog/tags/#deepfm" class="post-tag">DeepFM</a>
    
    

    <article>
      <p>본 글의 전반부에서는 먼저 <strong>DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</strong> 논문을 리뷰하면서 본 모델에 대해 설명할 것이다. 후반부에서는 Tensorflow를 이용하여 직접 코딩을 하고 학습하는 과정을 소개할 것이다. 논문의 전문은 <a href="https://arxiv.org/pdf/1703.04247v1.pdf">이곳</a>에서 확인할 수 있다.</p>

<hr />

<h2 id="1-deepfm-a-factorization-machine-based-neural-network-for-ctr-prediction-논문-리뷰">1. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction 논문 리뷰</h2>
<h3 id="10-abstract">1.0. Abstract</h3>
<p>추천 시스템에서 CTR을 최대화하는 것에 있어 사용자의 행동 속에 숨어있는 복잡한 feature interactions들을 학습하는 것은 매우 중요하다. 본 논문에서는 저차원 및 고차원 feature interactions를 모두 강조하면서 end-to-end 학습을 진행하는 모델에 대해 설명할 것이다. 이 <strong>DeepFM</strong>이라는 모델은 FM과 딥러닝을 결합한 것이다. 최근(2017년 기준) 구글에서 발표한 <strong>Wide &amp; Deep model</strong>에 비해 피쳐 엔지니어링이 필요 없고, wide하고 deep한 부분에서 공통된 Input을 가진다는 점이 특징적이다.</p>

<h3 id="11-introduction">1.1. Introduction</h3>
<p>추천 시스템에서 CTR은 매우 중요하다. 많은 경우에 추천시스템의 목표는 이 클릭 수를 증대하는 것인데, 따라서 CTR 추정값에 근거하여 아이템을 정렬한 뒤 아이템(기사, 영화 등)을 사용자에게 제시할 수 있다. 온라인 광고에서는 수익을 증가시키는 것이 가장 중요하기에, 이 상황에서는 <strong>CTR * bid</strong>라는 기준 아래 랭킹 전략을 세울 수 있을 것이다. 여기서 bid는 사용자가 아이템을 클릭할 경우 시스템이 수령하는 수입을 의미한다. 어떠한 케이스든, 이 CTR을 정확히 추정하는 것은 매우 중요할 것이다.</p>

<p>CTR 예측에 있어 중요한 포인트는, 사용자의 클릭 행동 속에 숨어 있는 implicit feature interactions(암시적 피쳐 상호작용)를 학습할 줄 알아야 한다는 것이다.</p>

<p>예를 들어 사람들이 식사 시간에 음식 배달을 위한 앱을 다운로드 받는다면, 이 때 앱 카테고리와 시간이라는 요소 사이의 2차 상호작용이 바로 <strong>클릭</strong>에 대한 신호가 될 수 있다는 것이다. 10대 남자아이가 RPG게임을 좋아한다고 하자, 이 때는 앱 카테고리-사용자의 성별-사용자의 나이라는 3개 요소의 관계가 <strong>클릭</strong>을 결정하는 요인이 될 수 있다. 즉, 사용자의 클릭 뒤에 숨어있는 이러한 상호작용들은 매우 복잡하여 저/고차원 <strong>모두</strong> 잘 잡아내는 것이 매우 중요하다.</p>

<p>(중략)</p>

<p>feature representation을 학습하는 방법으로써 Deep Neural Network가 복잡한 feature interactions를 학습하는 잠재력을 갖고 있다고 판단된다. 다만 CNN-based 모델의 경우 이웃한 feature들 사이에 발생하는 상호작용에 의해 편향된 경향을 보이고, RNN-based 모델의 경우 sequential dependency를 갖고 있는 클릭 데이터에 상대적으로 적합한 모습을 보였다. 이후에 FNN, PNN, Wide &amp; Deep 등 여러 모델들이 제안되었다. 본 논문에서는 이러한 모델들의 단점을 보완한 새로운 모델을 제시한다.</p>

<p>1) <strong>DeepFM</strong>은 피쳐 엔지니어링 없이 end-to-end 학습을 진행할 수 있다. 저차원의 interaction들은 FM 구조를 통해 모델화하고, 고차원의 interaction들은 DNN을 통해 모델화한다.<br />
2) <strong>DeepFM</strong>은 같은 Input과 Embedding 벡터를 공유하기 때문에 효과적으로 학습을 진행할 수 있다.<br />
3) 본 논문에서 <strong>DeepFM</strong>은 벤치마크 데이터와 상업용 데이터 모두에서 평가될 것이다.</p>

<hr />

<h3 id="12-our-approach">1.2. Our Approach</h3>
<p>$n$개의 instance를 가진 $(\chi, y)$ 학습 데이터셋이 있다고 하자. 이 때 $\chi$는 $m$개의 <strong>field</strong>를 지니고 있고, $y$는 0과 1의 값을 가진다. (1 = 클릭함)</p>

<p>$\chi$에는 범주형 변수가 있을 수도 있고, 연속형 변수가 있을 수도 있다. 범주형 변수의 경우 원핫인코딩된 벡터로 표현되며, 연속형 변수의 경우 그 값 자체로 표현되거나 이산화되어 원핫인코딩된 벡터로 표현될 수도 있다.</p>

<p>그렇다면 이제 데이터는 $(x, y)$로 표현할 수 있을 것이다. 여기서 $x$는 $[x_{field_1}, x_{field_2}, …, x_{field_m}]$의 구조를 갖게 되며 각각의 $x_{field_j}$는 $\chi$에서의 j번째 field의 벡터 표현을 의미하게 된다. 일반적으로 $x$는 굉장히 고차원이고 희소하다. CTR의 목적은 context가 주어졌을 때 사용자가 특정 어플을 클릭할 확률을 정확히 추정하는 것이다.</p>

<h4 id="121-deepfm">1.2.1. DeepFM</h4>
<center><img src="/public/img/Machine_Learning/2020-04-07-DeepFM/01.JPG" width="70%" /></center>

<p>위 그림에서도 확인할 수 있다시피, <strong>DeepFM</strong>은 2가지 요소로 구성되어 있다. 이 요소들은 같은 Input을 공유한다.</p>

<ul>
  <li>$i$번재 피쳐에 대해 스칼라 $w_i$: 1차원 importance를 측정함</li>
  <li>latent vector $V_i$: 다른 피쳐들과의 interaction의 영향을 측정</li>
</ul>

<p>$V_i$의 경우 FM요소에서는 2차원 interaction을 모델화하며, Deep요소에서는 고차원 피쳐 interaction을 모델화한다. 모든 파라미터들은 통합 예측모델에서 함께 학습된다. 즉 모델을 아주 간단히 표현하자면 아래와 같다.</p>

<script type="math/tex; mode=display">\hat{y} = sigmoid(y_{FM} + y_{DNN})</script>

<p><strong>FM Component</strong></p>
<center><img src="/public/img/Machine_Learning/2020-04-07-DeepFM/02.JPG" width="60%" /></center>

<p>FM요소는 Factorization Machine이다. FM모델에 대한 설명은 <a href="2019-12-21-FM.md">이글</a>에서 확인할 수 있다.</p>

<p><strong>Deep Component</strong><br />
CTR 예측에 사용되는 Raw 데이터는 일반적으로 매우 희소하고, 고차원이며, 범주형/연속형 변수가 섞여 있고, 일종의 field(성별, 위치, 나이 등)로 그룹화되어 있다는 특징을 지닌다. 따라서 <strong>Embedding Layer</strong>로 이러한 정보들을 압축하여 저차원의, dense한 실수 벡터를 만들어서 Input을 재가공할 필요가 있다.</p>

<p>아래 그림은 <strong>Input Layer</strong>에서 <strong>Embedding Layer</strong>로 이어지는 보조 네트워크를 강조한 부분이다. 여기서 확인해야 할 부분은 2가지이다. 첫 번재는, Input으로 쓰이는 Input field 벡터가 각자 다른 길이를 갖고 있을 수 있기 때문에, 이들의 임베딩은 같은 크기(<strong>k</strong>)여야 한다는 것이다. 두 번재는, FM 모델에서 latent 벡터로 기능했던 $V$는 본 요소에서는 Input field 벡터를 Embedding 벡터로 압축하기 위해 사용되고 학습되는 네트워크 weight가 된다는 것이다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-07-DeepFM/06.JPG" width="60%" /></center>

<p><strong>Embedding Layer</strong>의 Output은 아래와 같다.</p>

<script type="math/tex; mode=display">a^0 = [e_1, e_2, ..., e_m]</script>

<ul>
  <li>$e_i$는 i번재 field의 Embedding</li>
  <li>$m$은 field의 수</li>
</ul>

<p>$a^{(0)}$는 DNN에 투입되며 forward process는 다음과 같다.</p>

<script type="math/tex; mode=display">a^{(l+1)} = \sigma{(W^{(l)}a^{(l)} + b^{(l)}})</script>

<ul>
  <li>$l$: layer의 깊이</li>
</ul>

<p>이렇게 Dense한 실수 피쳐 벡터가 생성되면 CTR prediction을 위해 최종적으로 sigmoid 함수에 투입되게 된다.</p>

<script type="math/tex; mode=display">y_{DNN} = \sigma{(W^{|H|+1} a^{|H|} + b^{|H| + 1}})</script>

<ul>
  <li>$ㅣHㅣ$: hidden layer의 수</li>
  <li>$ \vert H \vert $: hidden layer의 수</li>
</ul>

<center> (중략) </center>

<h4 id="15-conclusions">1.5. Conclusions</h4>
<p>DeepFM은 FM Component와 Deep Component를 함께 학습시킨다. 이러한 방식은 다음과 같은 장점을 지닌다.<br />
1) pre-training이 필요 없다.<br />
2) 저/고차원 feature를 모두 잘 학습한다.<br />
3) feature embedding을 통해 피쳐 엔지니어링이 불필요하다.</p>

<p>실험 결과를 확인하면, DeepFM이 최신 모델들을 압도하고 상당한 효율성을 지닌 것을 알 수 있다.</p>

<hr />

<h2 id="2-tensorflow-구현">2. Tensorflow 구현</h2>
<h3 id="21-데이터-설명-및-데이터-변환">2.1. 데이터 설명 및 데이터 변환</h3>
<p>구현의 핵심은 Parameter인 $w$와 $V$의 shape과 활용 방법에 대해 이해하는 것이다. 사실 구현하는 사람의 입장에서는 논문이 썩 친절하다고 느끼지는 못할 것이다. 다소 애매모호한 표현으로 읽는 사람으로 하여금 혼란을 일으키게 하는 문구나 그림 등도 존재한다. 그럼에도 침착하게 잘 생각해보면, 모델을 구축할 수 있을 것이다.</p>

<p>학습 데이터로는 연봉이 5만 달러를 상회하는지의 여부를 예측하는 데이터를 사용하였고, <a href="https://archive.ics.uci.edu/ml/datasets/Adult">여기</a>에서 다운로드 받을 수 있다.</p>

<p>데이터는 48,842개의 Instance로 구성되어 있고, 14개의 Feature를 갖고 있으며, 이 중 6개의 변수가 연속형 변수이다. 당연히 예측 과제는 <strong>Binary Classification</strong>이다. 0은 연봉 5만 달러 이하를 의미하며, 전체 데이터의 25% 정도를 차지한다. 1은 연봉 5만 달러 초과를 의미한다.</p>

<p>앞에서 설명한 데이터를 예로 들어 설명하도록 하겠다. 이 데이터에는 총 14개의 변수가 있다. 이 14개는 곧, field의 개수가 된다. 이 중 범주형 변수를 One-Hot 인코딩을 통해 변환시키면(물론 연속형 변수도 필요에 따라 구간화하여 범주형 변수화해도 된다.) 본 데이터는 총 108개의 칼럼을 갖게 된다. 이 108개는 곧, feature의 개수가 된다. 즉, One-Hot 인코딩을 통해 변환시킨 칼럼의 개수를 feature의 개수로, 인코딩 이전의 데이터의 칼럼의 개수를 field의 개수로 이해하면 쉽다. 논문에서는 임베딩 스킬을 이용하고 있는데, 여기서 Embedding Matrix인 $V$의 칼럼의 개수는 Hyperparameter이다.</p>

<p>본 프로젝트 파일은 다음과 같이 5개의 py파일로 구성되어 있다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-07-DeepFM/07.JPG" width="25%" /></center>

<p>먼저 config파일을 보자. 이 파일에는 칼럼의 목록을 연속형/범주형을 구분하여 저장한 리스트와 Hyperparameter들이 저장되어 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># config.py
</span><span class="n">ALL_FIELDS</span> <span class="o">=</span> <span class="p">[</span><span class="s">'age'</span><span class="p">,</span> <span class="s">'workclass'</span><span class="p">,</span> <span class="s">'fnlwgt'</span><span class="p">,</span> <span class="s">'education'</span><span class="p">,</span> <span class="s">'education-num'</span><span class="p">,</span>
             <span class="s">'marital-status'</span><span class="p">,</span> <span class="s">'occupation'</span><span class="p">,</span> <span class="s">'relationship'</span><span class="p">,</span> <span class="s">'race'</span><span class="p">,</span>
             <span class="s">'sex'</span><span class="p">,</span> <span class="s">'capital-gain'</span><span class="p">,</span> <span class="s">'capital-loss'</span><span class="p">,</span> <span class="s">'hours-per-week'</span><span class="p">,</span> <span class="s">'country'</span><span class="p">]</span>
<span class="n">CONT_FIELDS</span> <span class="o">=</span> <span class="p">[</span><span class="s">'age'</span><span class="p">,</span> <span class="s">'fnlwgt'</span><span class="p">,</span> <span class="s">'education-num'</span><span class="p">,</span>
               <span class="s">'capital-gain'</span><span class="p">,</span> <span class="s">'capital-loss'</span><span class="p">,</span> <span class="s">'hours-per-week'</span><span class="p">]</span>
<span class="n">CAT_FIELDS</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">ALL_FIELDS</span><span class="p">)</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="n">CONT_FIELDS</span><span class="p">))</span>

<span class="c1"># Hyper-parameters for Experiment
</span><span class="n">NUM_BIN</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">EMBEDDING_SIZE</span> <span class="o">=</span> <span class="mi">5</span>
</code></pre></div></div>

<p>이제 데이터를 가공할 시간이다. (데이터가 매우 커서 서버에서 데이터를 받아오는 상황이라면, 아래 코드를 pyspark로 짜면 좋을 것이다.) 지금부터 할 작업은 <code class="highlighter-rouge">field_index</code>와 <code class="highlighter-rouge">field_dict</code>를 만드는 것인데, 쉽게 말해서 아래와 같은 작업을 진행하는 것이다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-07-DeepFM/05.JPG" width="100%" /></center>

<p>인코딩 이후의 데이터에 대해 각 칼럼이 본래 인코딩 이전에 몇 번째 field에 속했었는지에 대한 정보를 저장한 것이 <code class="highlighter-rouge">field_index</code>와 <code class="highlighter-rouge">field_dict</code>이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Preprocess
</span><span class="kn">import</span> <span class="nn">config</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">repeat</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="k">def</span> <span class="nf">get_modified_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">all_fields</span><span class="p">,</span> <span class="n">continuous_fields</span><span class="p">,</span> <span class="n">categorical_fields</span><span class="p">,</span> <span class="n">is_bin</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">field_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">field_index</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">X_modified</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">col</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">all_fields</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"{} not included: Check your column list"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">col</span><span class="p">))</span>
            <span class="k">raise</span> <span class="nb">ValueError</span>

        <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">continuous_fields</span><span class="p">:</span>
            <span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>

            <span class="c1"># 연속형 변수도 구간화 할 것인가?
</span>            <span class="k">if</span> <span class="n">is_bin</span><span class="p">:</span>
                <span class="n">X_bin</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="n">col</span><span class="p">]])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">config</span><span class="o">.</span><span class="n">NUM_BIN</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                <span class="n">X_bin</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">X_bin</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'str'</span><span class="p">)</span>

                <span class="n">X_bin_col</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X_bin</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">prefix_sep</span><span class="o">=</span><span class="s">'-'</span><span class="p">)</span>
                <span class="n">field_dict</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_bin_col</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
                <span class="n">field_index</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">repeat</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">X_bin_col</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
                <span class="n">X_modified</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_modified</span><span class="p">,</span> <span class="n">X_bin_col</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">X_cont_col</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="n">col</span><span class="p">]]),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>
                <span class="n">field_dict</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">col</span>
                <span class="n">field_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
                <span class="n">X_modified</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_modified</span><span class="p">,</span> <span class="n">X_cont_col</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">categorical_fields</span><span class="p">:</span>
            <span class="n">X_cat_col</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">prefix_sep</span><span class="o">=</span><span class="s">'-'</span><span class="p">)</span>
            <span class="n">field_dict</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_cat_col</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
            <span class="n">field_index</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">repeat</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">X_cat_col</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="n">X_modified</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X_modified</span><span class="p">,</span> <span class="n">X_cat_col</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">'Data Prepared...'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'X shape: {}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">X_modified</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'# of Feature: {}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">field_index</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'# of Field: {}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">field_dict</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">field_dict</span><span class="p">,</span> <span class="n">field_index</span><span class="p">,</span> <span class="n">X_modified</span>
</code></pre></div></div>

<h3 id="22-모델-빌드">2.2. 모델 빌드</h3>
<p>먼저 FM Component에 대해 살펴보자. <strong>call</strong> 함수에서 y_fm을 어떤 shape으로 반환할 지는 그 task에 맞게 변환하면 된다. 아래 코드에서는 (None, 2)의 형태로 반환되어 최종적으로 Deep Component의 (None, 2)와 합쳐져 (None, 4)의 최종 Output을 반환하게 되는데, 이 수치는 성능 향상을 위해 변경이 가능하다.</p>

<p>Parameter $w$의 길이는 <code class="highlighter-rouge">num_feature(108)</code>이며, Parameter $V$의 shape은 <code class="highlighter-rouge">num_field(14), embedding_size(5)</code>이다. 그런데 아래 <strong>call</strong> 함수에서 보면 알 수 있듯이, 이 $V$행렬은 One-Hot 인코딩된 데이터에 곱해지는 구조이기 때문에 <code class="highlighter-rouge">tf.nn.embedding_lookup</code>이라는 함수를 통해 행이 복제된다. 즉, 앞서 생성한 <code class="highlighter-rouge">field_index</code>의 정보를 참조하여, 같은 field에서 나온 feature일 경우, 같은 Embedding Row($V$의 Row)를 공유하는 것이다.</p>

<p><strong>new_inputs</strong>는 Deep Component의 Input으로 쓰일 개체이다. 코드를 살펴보면, $V$라는 행렬이 FM Component에도 쓰이지만, <strong>new_inputs</strong>를 만들어내면서 Deep Component에도 영향을 미치는 것을 알 수 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FM_layer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_feature</span><span class="p">,</span> <span class="n">num_field</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">field_index</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FM_layer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>    <span class="c1"># k: 임베딩 벡터의 차원(크기)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_feature</span> <span class="o">=</span> <span class="n">num_feature</span>          <span class="c1"># f: 원래 feature 개수
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span> <span class="o">=</span> <span class="n">num_field</span>              <span class="c1"># m: grouped field 개수
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">field_index</span> <span class="o">=</span> <span class="n">field_index</span>          <span class="c1"># 인코딩된 X의 칼럼들이 본래 어디 소속이었는지
</span>
        <span class="c1"># Parameters of FM Layer
</span>        <span class="c1"># w: capture 1st order interactions
</span>        <span class="c1"># V: capture 2nd order interactions
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_feature</span><span class="p">],</span>
                                              <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'w'</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_field</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">),</span>
                                              <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'V'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x_batch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_feature</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="c1"># Parameter V를 field_index에 맞게 복사하여 num_feature에 맞게 늘림
</span>        <span class="n">embeds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">field_index</span><span class="p">)</span>

        <span class="c1"># Deep Component에서 쓸 Input
</span>        <span class="c1"># (batch_size, num_feature, embedding_size)
</span>        <span class="n">new_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">embeds</span><span class="p">)</span>

        <span class="c1"># (batch_size, )
</span>        <span class="n">linear_terms</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span> <span class="n">inputs</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># (batch_size, )
</span>        <span class="n">interactions</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">new_inputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">new_inputs</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="p">)</span>

        <span class="n">linear_terms</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">linear_terms</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">interactions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">interactions</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="n">y_fm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">linear_terms</span><span class="p">,</span> <span class="n">interactions</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_fm</span><span class="p">,</span> <span class="n">new_inputs</span>
</code></pre></div></div>

<p>아래는 메인 모델에 대한 코드이다. 성능 향상을 위해 Deep Component를 수정하는 것은 연구자의 자유이다. Task에 따라 가볍게 설계할 수도, 복잡하게 설계할 수도 있을 것이다. 본 코드에서는 Dropout만을 추가하여 다소 가볍게 설계하였다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">layers</span> <span class="kn">import</span> <span class="n">FM_layer</span>

<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_floatx</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DeepFM</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_feature</span><span class="p">,</span> <span class="n">num_field</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">field_index</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeepFM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>    <span class="c1"># k: 임베딩 벡터의 차원(크기)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_feature</span> <span class="o">=</span> <span class="n">num_feature</span>          <span class="c1"># f: 원래 feature 개수
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span> <span class="o">=</span> <span class="n">num_field</span>              <span class="c1"># m: grouped field 개수
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">field_index</span> <span class="o">=</span> <span class="n">field_index</span>          <span class="c1"># 인코딩된 X의 칼럼들이 본래 어디 소속이었는지
</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fm_layer</span> <span class="o">=</span> <span class="n">FM_layer</span><span class="p">(</span><span class="n">num_feature</span><span class="p">,</span> <span class="n">num_field</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">field_index</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers3</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">final</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s">"DeepFM Model: #Field: {}, #Feature: {}, ES: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_field</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_feature</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># 1) FM Component: (num_batch, 2)
</span>        <span class="n">y_fm</span><span class="p">,</span> <span class="n">new_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fm_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># retrieve Dense Vectors: (num_batch, num_feature*embedding_size)
</span>        <span class="n">new_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_feature</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">])</span>

        <span class="c1"># 2) Deep Component
</span>        <span class="n">y_deep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers1</span><span class="p">(</span><span class="n">new_inputs</span><span class="p">)</span>
        <span class="n">y_deep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">y_deep</span><span class="p">)</span>
        <span class="n">y_deep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers2</span><span class="p">(</span><span class="n">y_deep</span><span class="p">)</span>
        <span class="n">y_deep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">y_deep</span><span class="p">)</span>
        <span class="n">y_deep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers3</span><span class="p">(</span><span class="n">y_deep</span><span class="p">)</span>

        <span class="c1"># Concatenation
</span>        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">y_fm</span><span class="p">,</span> <span class="n">y_deep</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">])</span>

        <span class="k">return</span> <span class="n">y_pred</span>
</code></pre></div></div>

<h3 id="23-학습">2.3. 학습</h3>
<p>학습 코드는 아래와 같다. 그리 무거운 모델은 아니므로 Autograph는 사용하지 않았다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">config</span>
<span class="kn">from</span> <span class="nn">preprocess</span> <span class="kn">import</span> <span class="n">get_modified_data</span>
<span class="kn">from</span> <span class="nn">DeepFM</span> <span class="kn">import</span> <span class="n">DeepFM</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">perf_counter</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.metrics</span> <span class="kn">import</span> <span class="n">BinaryAccuracy</span><span class="p">,</span> <span class="n">AUC</span>


<span class="k">def</span> <span class="nf">get_data</span><span class="p">():</span>
    <span class="nb">file</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/adult.data'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="nb">file</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">13</span><span class="p">]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="nb">file</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="mi">14</span><span class="p">]</span><span class="o">.</span><span class="nb">map</span><span class="p">({</span><span class="s">' &lt;=50K'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">' &gt;50K'</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>

    <span class="n">X</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">ALL_FIELDS</span>
    <span class="n">field_dict</span><span class="p">,</span> <span class="n">field_index</span><span class="p">,</span> <span class="n">X_modified</span> <span class="o">=</span> \
        <span class="n">get_modified_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">ALL_FIELDS</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">CONT_FIELDS</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">CAT_FIELDS</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>

    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_modified</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>

    <span class="n">train_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
        <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span> \
        <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">30000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">)</span>

    <span class="n">test_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span>
        <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span> \
        <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_ds</span><span class="p">,</span> <span class="n">test_ds</span><span class="p">,</span> <span class="n">field_dict</span><span class="p">,</span> <span class="n">field_index</span>


<span class="c1"># Batch 단위 학습
</span><span class="k">def</span> <span class="nf">train_on_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>

    <span class="c1"># apply_gradients()를 통해 processed gradients를 적용함
</span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>

    <span class="c1"># accuracy &amp; auc
</span>    <span class="n">acc</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">auc</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span>


<span class="c1"># 반복 학습 함수
</span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">train_ds</span><span class="p">,</span> <span class="n">test_ds</span><span class="p">,</span> <span class="n">field_dict</span><span class="p">,</span> <span class="n">field_index</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">()</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">DeepFM</span><span class="p">(</span><span class="n">embedding_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">EMBEDDING_SIZE</span><span class="p">,</span> <span class="n">num_feature</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">field_index</span><span class="p">),</span>
                   <span class="n">num_field</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">field_dict</span><span class="p">),</span> <span class="n">field_index</span><span class="o">=</span><span class="n">field_index</span><span class="p">)</span>

    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"Start Training: Batch Size: {}, Embedding Size: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">EMBEDDING_SIZE</span><span class="p">))</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">auc</span> <span class="o">=</span> <span class="n">AUC</span><span class="p">()</span>
        <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">train_on_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="s">"Epoch {:03d}: 누적 Loss: {:.4f}, Acc: {:.4f}, AUC: {:.4f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
            <span class="n">i</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">loss_history</span><span class="p">),</span> <span class="n">acc</span><span class="o">.</span><span class="n">result</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">auc</span><span class="o">.</span><span class="n">result</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>

    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">test_auc</span> <span class="o">=</span> <span class="n">AUC</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_ds</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">test_acc</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
        <span class="n">test_auc</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"테스트 ACC: {:.4f}, AUC: {:.4f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">test_acc</span><span class="o">.</span><span class="n">result</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">test_auc</span><span class="o">.</span><span class="n">result</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Batch Size: {}, Embedding Size: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">EMBEDDING_SIZE</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"걸린 시간: {:.3f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s">'weights/weights-epoch({})-batch({})-embedding({}).h5'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
        <span class="n">epochs</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">EMBEDDING_SIZE</span><span class="p">))</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">train</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div></div>

<p>Embedding Size를 변환하면서 진행한 테스트 결과는 아래와 같다. (Epoch: 100)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Embedding Size</th>
      <th style="text-align: center">누적 Loss</th>
      <th style="text-align: center">Train ACC</th>
      <th style="text-align: center">Train AUC</th>
      <th style="text-align: center">Test ACC</th>
      <th style="text-align: center">Test AUC</th>
      <th style="text-align: center">시간</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">10</td>
      <td style="text-align: center">0.3243</td>
      <td style="text-align: center"><strong>0.8485</strong></td>
      <td style="text-align: center"><strong>0.9038</strong></td>
      <td style="text-align: center"><strong>0.8464</strong></td>
      <td style="text-align: center">0.8991</td>
      <td style="text-align: center">4분 0.78초</td>
    </tr>
    <tr>
      <td style="text-align: center">9</td>
      <td style="text-align: center">0.3386</td>
      <td style="text-align: center">0.8382</td>
      <td style="text-align: center">0.8954</td>
      <td style="text-align: center">0.8402</td>
      <td style="text-align: center">0.8975</td>
      <td style="text-align: center">4분 3.64초</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: center">0.3704</td>
      <td style="text-align: center">0.8240</td>
      <td style="text-align: center">0.8729</td>
      <td style="text-align: center">0.8260</td>
      <td style="text-align: center">0.8745</td>
      <td style="text-align: center">4분 2.79초</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">0.3248</td>
      <td style="text-align: center">0.8471</td>
      <td style="text-align: center">0.9033</td>
      <td style="text-align: center">0.8424</td>
      <td style="text-align: center">0.9013</td>
      <td style="text-align: center">4분 0.84초</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">0.3305</td>
      <td style="text-align: center">0.8433</td>
      <td style="text-align: center">0.9001</td>
      <td style="text-align: center">0.8416</td>
      <td style="text-align: center"><strong>0.9041</strong></td>
      <td style="text-align: center">4분 1.28초</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.3945</td>
      <td style="text-align: center">0.8169</td>
      <td style="text-align: center">0.8512</td>
      <td style="text-align: center">0.8190</td>
      <td style="text-align: center">0.8576</td>
      <td style="text-align: center">4분 8.10초</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="reference">Reference</h2>
<p>https://github.com/ChenglongChen/tensorflow-DeepFM</p>

    </article>
    <div class="post-more">
      
      <a href="/machine_learning/2020/04/07/DeepFM/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/machine_learning/2020/04/07/DeepFM/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/machine_learning/2020/04/05/FFM/">
        추천 시스템의 기본 - 04. Field-aware Factorization Machines 설명 및 xlearn 실습
      </a>
    </h1>

    <span class="post-date">05 Apr 2020</span>
     |
    
    <a href="/blog/tags/#machine-learning" class="post-tag">Machine_Learning</a>
    
    <a href="/blog/tags/#recommendation-system" class="post-tag">Recommendation System</a>
    
    <a href="/blog/tags/#field-aware-factorization-machines" class="post-tag">Field-aware Factorization Machines</a>
    
    

    <article>
      <p>본 글의 전반부에서는 먼저 <strong>Field-aware Factorization Machines for CTR prediction</strong> 논문을 리뷰하면서 본 모델에 대해 설명할 것이다. 후반부에서는 간단한 xlearn코드 역시 소개할 예정이다. 논문의 전문은 <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf">이곳</a>에서 확인할 수 있다.</p>

<hr />

<h2 id="1-field-aware-factorization-machines-for-ctr-prediction-논문-리뷰">1. Field-aware Factorization Machines for CTR prediction 논문 리뷰</h2>
<h3 id="10abstract">1.0.Abstract</h3>
<p>CTR 예측과 같은 크고 희소한 데이터셋에 대해 <strong>FFM</strong>은 효과적인 방법이다. 본 논문에서는 우리는 <strong>FFM</strong>을 학습시키는 효과적인 구현 방법을 제시할 것이다. 그리고 우리는 이 모델을 전체적으로 분석한 뒤 다른 경쟁 모델과 비교를 진행할 것이다. 실험에 따르면 <strong>FFM</strong>이 특정 분류 모델에 있어서 굉장히 뛰어난 접근 방법이라는 것을 알려준다. 마지막으로, 우리는 <strong>FFM</strong> 패키지를 공개한다.</p>

<h3 id="11-introduction">1.1. Introduction</h3>
<p>CTR 예측에 있어서 굉장히 중요한 것은, feature 간의 conjunction(결합, 연결)을 이해하는 것이다. Simple Logistic Regression과 같은 간단한 모델은 이러한 <code class="highlighter-rouge">결합</code>을 잘 이해하지 못한다. FM 모델은 2개의 Latent Vector의 곱으로 factorize하여 feature conjunction을 이해하게 된다.</p>

<p>개인화된 태그 추천을 위해 pairwise interaction tensor factorization (PITF)라는 FM의 변형 모델이 제안되었다. 이후 KDD Cup 2020에서, Team Opera Solutions라는 팀이 이 모델의 일반화된 버전을 제안하였다. 그러나 이 용어는 다소 일반적이고 혼동을 줄 수 있는 이름이므로, 본 논문에서는 이를 <strong>FFM</strong>이라고 부르도록 하겠다.</p>

<p><strong>FFM</strong>의 중요 특징은 아래와 같다.</p>
<ol>
  <li>최적화 문제를 해결하기 위해 Stochastic Gradient를 사용한다. 과적합을 막기 위해 오직 1 epoch만 학습한다.</li>
  <li>FFM은 위 팀에서 비교한 모델 6개 중 가장 뛰어난 성적을 보여주었다.</li>
</ol>

<hr />

<h3 id="12-poly2-and-fm">1.2. POLY2 and FM</h3>
<p>(중략)</p>

<hr />

<h3 id="13-ffm">1.3. FFM</h3>
<p><strong>FFM</strong>의 중요한 아이디어는 PITF로 부터 파생되었는데, 이는 바로 개인화된 태그에 관한 것이다. PIFT에서 그들은 <code class="highlighter-rouge">User, Item, Tag</code>를 포함한 3개의 가용 필드를 가정했고, 이를 분리된 latent space에서 (User, Item), (User, Tag), (Item,Tag)로 factorize하였다. 이러한 정의는 추천 시스템에 적합한 정의이고 CTR 예측에 있어서는 자세한 설명이 부족한 편이므로, 좀 더 포괄적인 논의를 진행해보도록 하겠다.</p>

<p>아래와 같은 데이터 테이블이 있을 때, <code class="highlighter-rouge">features</code>는 <code class="highlighter-rouge">fields</code>로 그룹화할 수 있다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/01.JPG" width="70%" /></center>

<p>예를 들어, Espn, Vogue, NBC는 Publisher라는 field에 속할 수 있겠다. <strong>FFM</strong>은 이러한 정보를 활용하는 FM의 변형된 버전이다. <strong>FFM</strong>의 원리를 설명하기 위해, 다음 새로운 예시에 대해 생각해보자.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/02.JPG" width="60%" /></center>

<p>FM의 상호작용 항인 $\phi_{FM}(w, x)$는 아래와 같이 표현될 수 있다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/03.JPG" width="60%" /></center>

<p>FM에서는 다른 feature들과의 latent effect를 학습하기 위해 모든 feature는 오직 하나의 latent vector를 가진다. Espn을 예로 들어보면, $w_{Espn}$은 Nike와 Male과의 latent effect를 학습하기 위해 이용되었다. 그러나 Nike와 Male은 다른 Field에 속하기 때문에 사실 (Espn, Nike)의 관계와 (Espn, Male)의 관계에서 사용되었던 $w_{Espn}$의 값은 다를 가능성이 높다. 즉, 하나의 벡터로 2개의 관계를 모두 표현하기에는 무리가 있다는 점이다.</p>

<p><strong>FFM</strong>에서는 각각의 feature는 여러 latent vector를 갖게 된다. <strong>FFM</strong>의 상호작용 항인 $\phi_{FFM}(w, x)$은 아래와 같이 표현된다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/04.JPG" width="70%" /></center>

<p>수학적으로 재표현하면 아래와 같이 표현할 수 있겠다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/05.JPG" width="60%" /></center>

<p>여기서 $f_1$과 $f_2$는 $j_1$과 $j_2$의 field를 의미한다. $j$들은 Espn, Nike 등을 의미한다. $f$를 field의 개수라고 할 때, FFM의 변수의 개수는 $nfk$이며, FFM의 계산 복잡성은 $O(\overline{n}^2 k)$이다.</p>

<p>여기서 <strong>n, f, k</strong>는 각각 feature의 개수(often called p), field의 개수, latent 변수의 개수를 의미한다.</p>

<p><strong>FFM</strong>의 경우 각각의 latent vector아 오직 특정 field와 관련한 효과에 대해서는 학습을 진행하기 때문에 잠재 변수의 수은 $k$는 FM의 경우보다 작은 경우가 많다.</p>

<script type="math/tex; mode=display">% <![CDATA[
k_{FFM} < k_{FM} %]]></script>

<hr />

<h4 id="131-solving-the-optimization-problem">1.3.1. Solving the Optimization Problem</h4>
<p>사실 FFM의 최적화 문제를 푸는 것은 Simple Logistic Regression의 최적화 문제를 푸는 식에서 $\phi_{LM}(w, x)$를 $\phi_{FFM}(w, x)$로 바꾸는 것을 제외하면 동일하다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/06.JPG" width="60%" /></center>

<p>실험 결과에 그 이유가 나오지만, Stochastic Gradient 알고리즘으로 행렬 분해에 있어 효과적인 <code class="highlighter-rouge">AdaGrad</code>를 적용하였다. 각 SG 스텝마다 data point $(y, x)$는 $\phi_{FFM}(w, x)$ 식에서 $w_{j1, f2}, w_{j2f1}$를 업데이트하기 위해 추출된다. CTR prediction과 같은 문제를 푸는 데에 있어 $x$는 굉장히 희소한 벡터임을 기억하자. 따라서 실제로는 0이 아닌 값들에 대해서만 업데이트가 진행될 것이다.</p>

<p>sub-gradient는 아래와 같다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/07.JPG" width="70%" /></center>

<p>d=1…k에 대해 gradient의 제곱합은 아래와 같이 합산된다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/08.JPG" width="50%" /></center>

<p>최종적으로 $(w_{j1, f2})<em>d$과 $(w</em>{j2, f1})_d$ 는 아래와 같이 업데이트 된다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/09.JPG" width="50%" /></center>

<p>여기서 $\eta$는 직접 정한 learning rate를 의미한다. $w$의 초깃값은 $[0, 1/\sqrt{k}]$ 사이의 Uniform Distribution 에서의 랜덤한 값으로 초기화된다. $G$는 $(G_{j1, f2})_d^{-\frac{1}{2}}$의 값이 매우 커지는 것을 막기 위해 모두 1로 세팅된다. 전체적인 과정은 아래와 같으며, 각 instance를 normalize해주는 것이 성능 향상에 도움이 되었다는 말을 남긴다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/10.JPG" width="60%" /></center>

<hr />

<h4 id="132-parallelization-on-shared-memory-systems">1.3.2. Parallelization on Shared-memory Systems</h4>
<p>본 논문에서는 Hog-WILD!라는 병렬처리 기법을 사용하였다.</p>

<hr />

<h4 id="133-adding-field-information">1.3.3. Adding Field Information</h4>
<p>널리 사용되는 LIBSVM의 데이터 포맷은 다음과 같다.</p>

<p>label feat1:val1 feat2:val2 …</p>

<p>여기서 각 (feat, val) 쌍은 feature index와 value를 의미한다. <strong>FFM</strong>을 위해 우리는 위 포맷을 아래와 같이 확장할 수 있다.</p>

<p>label field1:feat1:val1 field2:feat2:val2 …</p>

<p>이는 적합한 field를 각 feature 마다 지정해주어야 함을 의미한다. 특정 feature에 대해서는 이 지정 작업이 쉽지만, 나머지들에 대해서는 그렇지 않을 수도 있다. 이 부분에 대해서는 feature의 3가지 종류의 관점에서 논의해보도록 하자.</p>

<p><strong>Categorical Features</strong><br />
선형 모델에서 categorical feature는 여러 개의 binary feature로 변환하는 것이 일반적이다. 우리는 다음과 같이 데이터 instance를 변형할 수 있다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/11.JPG" width="55%" /></center>

<p>LIBSVM 포맷에서는 0의 값은 저장되지 않기 때문에 이렇게 모든 categorical feature들을 binary feature로 변형할 수 있는 것이다. 이제 위 데이터는 최종적으로 아래와 같은 형상을 갖게 된다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/12.JPG" width="45%" /></center>

<p><strong>Numerical Features</strong><br />
conference에서 논문이 통과될지에 대한 데이터가 있다고 하자. 칼럼의 의미는 아래와 같다.</p>
<ul>
  <li>AR: accept rate of the conference</li>
  <li>Hidx: h-index of the author</li>
  <li>Cite: # citations of the author</li>
</ul>

<p>각 feature를 dummy field로 취급하여 아래와 같은 데이터 형상을 만들 수도 있지만, 이는 딱히 도움이 되지 않는 방법 같다.</p>

<p>Yes AR:AR:45.73 Hidx:Hidx:2 Cite:Cite:3</p>

<p>또 하나의 방법은, feature는 field에 넣고, 기존의 실수 값을 이산화하여 feature로 만든 후, binary하게 1과 0의 값을 넣어주는 방식이다.</p>

<p>Yes AR:45:1 Hidx:2:1 Cite:3:1</p>

<p>이산화 방법에 대해서는 여러가지 방식이 존재할 수 있다. 어떠한 방법이든 일정 수준의 정보 손실은 감수해야 한다.</p>

<p><strong>Single-field Features</strong><br />
일부 데이터 셋에 대해서 모든 feature가 단일 field에 속하여 각 feature에 대해 field를 지정해주는 것이 무의미한 경우도 있다. 특히 NLP와 같은 분야에서는 이러한 현상이 두드러진다.</p>

<center><img src="/public/img/Machine_Learning/2020-04-05-FFM/13.JPG" width="55%" /></center>

<p>위 경우에서 유일한 field는 “sentence”가 될 것이다. 일부 사람들은 numerical features의 경우처럼 dummy field를 만들면 어떨까 하고 의문을 가지지만, 사실 그렇게 되면 n(feature의 수)이 너무 커지기 때문에 굉장히 비효율적이다.</p>

<p>(<strong>FFM</strong>의 모델 크기가 $O(nfk)$임을 기억해보자. 이 경우에는 $f=n$이 될 것이다. (field의 수 = feature의 수))</p>

<hr />

<h4 id="14-experiments">1.4. Experiments</h4>
<p>(후략)</p>

<hr />

<h3 id="2-xlearn">2. xlearn</h3>
<h4 id="21-설치">2.1. 설치</h4>
<p>여러 가지 방법으로 설치를 진행할 수 있지만, <a href="https://github.com/aksnzhy/xlearn/releases">여기</a>에서 whl파일을 통해 설치하는 것이 가장 간단하다.</p>

<h4 id="22-코드">2.2. 코드</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_convert_to_ffm</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="nb">type</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">numerics</span><span class="p">,</span> <span class="n">categories</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">encoder</span><span class="p">):</span>
    <span class="c1"># Flagging categorical and numerical fields
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'convert_to_ffm - START'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">numerics</span><span class="p">:</span>
        <span class="k">if</span><span class="p">(</span><span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoder</span><span class="p">[</span><span class="s">'catdict'</span><span class="p">]):</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'UPDATING CATDICT: numeric field - {x}'</span><span class="p">)</span>
            <span class="n">encoder</span><span class="p">[</span><span class="s">'catdict'</span><span class="p">][</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">categories</span><span class="p">:</span>
        <span class="k">if</span><span class="p">(</span><span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoder</span><span class="p">[</span><span class="s">'catdict'</span><span class="p">]):</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'UPDATING CATDICT: categorical field - {x}'</span><span class="p">)</span>
            <span class="n">encoder</span><span class="p">[</span><span class="s">'catdict'</span><span class="p">][</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">nrows</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">)</span> <span class="o">+</span> <span class="s">"_ffm.txt"</span><span class="p">,</span> <span class="s">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">text_file</span><span class="p">:</span>

        <span class="c1"># Looping over rows to convert each row to libffm format
</span>        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nrows</span><span class="p">)):</span>
            <span class="n">datastring</span> <span class="o">=</span> <span class="s">""</span>
            <span class="n">datarow</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
            <span class="n">datastring</span> <span class="o">+=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">datarow</span><span class="p">[</span><span class="n">target</span><span class="p">]))</span>  <span class="c1"># Set Target Variable here
</span>
            <span class="c1"># For numerical fields, we are creating a dummy field here
</span>            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">encoder</span><span class="p">[</span><span class="s">'catdict'</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                <span class="k">if</span><span class="p">(</span><span class="n">encoder</span><span class="p">[</span><span class="s">'catdict'</span><span class="p">][</span><span class="n">x</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="c1"># Not adding numerical values that are nan
</span>                    <span class="k">if</span> <span class="n">math</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">datarow</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">True</span><span class="p">:</span>
                        <span class="n">datastring</span> <span class="o">=</span> <span class="n">datastring</span> <span class="o">+</span> <span class="s">" "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s">":"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s">":"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">datarow</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>

                    <span class="c1"># For a new field appearing in a training example
</span>                    <span class="k">if</span><span class="p">(</span><span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoder</span><span class="p">[</span><span class="s">'catcodes'</span><span class="p">]):</span>
                        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'UPDATING CATCODES: categorical field - {x}'</span><span class="p">)</span>
                        <span class="n">encoder</span><span class="p">[</span><span class="s">'catcodes'</span><span class="p">][</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
                        <span class="n">encoder</span><span class="p">[</span><span class="s">'currentcode'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'UPDATING CATCODES: categorical value for field {x} - {datarow[x]}'</span><span class="p">)</span>
                        <span class="n">encoder</span><span class="p">[</span><span class="s">'catcodes'</span><span class="p">][</span><span class="n">x</span><span class="p">][</span><span class="n">datarow</span><span class="p">[</span><span class="n">x</span><span class="p">]]</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">[</span><span class="s">'currentcode'</span><span class="p">]</span>  <span class="c1"># encoding the feature
</span>
                    <span class="c1"># For already encoded fields
</span>                    <span class="k">elif</span><span class="p">(</span><span class="n">datarow</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoder</span><span class="p">[</span><span class="s">'catcodes'</span><span class="p">][</span><span class="n">x</span><span class="p">]):</span>
                        <span class="n">encoder</span><span class="p">[</span><span class="s">'currentcode'</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'UPDATING CATCODES: categorical value for field {x} - {datarow[x]}'</span><span class="p">)</span>
                        <span class="n">encoder</span><span class="p">[</span><span class="s">'catcodes'</span><span class="p">][</span><span class="n">x</span><span class="p">][</span><span class="n">datarow</span><span class="p">[</span><span class="n">x</span><span class="p">]]</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">[</span><span class="s">'currentcode'</span><span class="p">]</span>  <span class="c1"># encoding the feature
</span>
                    <span class="n">code</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">[</span><span class="s">'catcodes'</span><span class="p">][</span><span class="n">x</span><span class="p">][</span><span class="n">datarow</span><span class="p">[</span><span class="n">x</span><span class="p">]]</span>
                    <span class="n">datastring</span> <span class="o">=</span> <span class="n">datastring</span> <span class="o">+</span> <span class="s">" "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s">":"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">code</span><span class="p">))</span><span class="o">+</span><span class="s">":1"</span>

            <span class="n">datastring</span> <span class="o">+=</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span>
            <span class="n">text_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">datastring</span><span class="p">)</span>

    <span class="c1"># print('Encoder Summary:')
</span>    <span class="c1"># print(json.dumps(encoder, indent=4))
</span>    <span class="k">return</span> <span class="n">encoder</span>
</code></pre></div></div>

<p>위와 같이 LIBSVM 데이터 포맷으로 데이터를 변경한 후에,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">xlearn</span> <span class="k">as</span> <span class="n">xl</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">xl</span><span class="o">.</span><span class="n">create_ffm</span><span class="p">()</span>

<span class="c1"># 학습/테스트 데이터 path 연결
</span><span class="n">model</span><span class="o">.</span><span class="n">setTrain</span><span class="p">(</span><span class="s">"data/train_ffm.txt"</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">setValidate</span><span class="p">(</span><span class="s">"data/test_ffm.txt"</span><span class="p">)</span>

<span class="c1"># Early Stopping 불가
</span><span class="n">model</span><span class="o">.</span><span class="n">disableEarlyStop</span><span class="p">()</span>

<span class="c1"># param 선언
</span><span class="n">param</span> <span class="o">=</span> <span class="p">{</span><span class="s">'task'</span><span class="p">:</span> <span class="s">'binary'</span><span class="p">,</span> <span class="s">'lr'</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span> <span class="s">'lambda'</span><span class="p">:</span> <span class="mf">0.00002</span><span class="p">,</span>
         <span class="s">'k'</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">'epoch'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span> <span class="s">'metric'</span><span class="p">:</span> <span class="s">'auc'</span><span class="p">,</span> <span class="s">'opt'</span><span class="p">:</span> <span class="s">'adagrad'</span><span class="p">,</span>
         <span class="s">'num_threads'</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>

<span class="c1"># 학습
# model.fit(param=param, model_path="model/model.out")
</span>
<span class="c1"># Cross-Validation 학습
</span><span class="n">model</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

<span class="c1"># Predict
</span><span class="n">model</span><span class="o">.</span><span class="n">setTest</span><span class="p">(</span><span class="s">"data/test_ffm.txt"</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">setSigmoid</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s">"model/model.out"</span><span class="p">,</span> <span class="s">"output/predictions.txt"</span><span class="p">)</span>
</code></pre></div></div>

<p>위와 같이 학습을 진행하면 된다. 간단하다.</p>

<hr />

<h2 id="reference">Reference</h2>
<p>https://wngaw.github.io/field-aware-factorization-machines-with-xlearn/</p>

    </article>
    <div class="post-more">
      
      <a href="/machine_learning/2020/04/05/FFM/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/machine_learning/2020/04/05/FFM/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>


  </div>
</div>

<label for="sidebar-checkbox" class="sidebar-toggle"></label>

<script>
  (function (document) {
    let toggle = document.querySelector('.sidebar-toggle');
    let sidebar = document.querySelector('#sidebar');
    let checkbox = document.querySelector('#sidebar-checkbox');

    document.addEventListener('click', function (e) {
      let target = e.target;

      if (target === toggle) {
        checkbox.checked = !checkbox.checked;
        e.preventDefault();
      } else if (checkbox.checked && !sidebar.contains(target)) {
        /* click outside the sidebar when sidebar is open */
        checkbox.checked = false;
      }
    }, false);
  })(document);
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    };
    i[r].l = 1 * new Date();
    a = s.createElement(o);
    m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-00000000-1', 'auto');
  ga('send', 'pageview');
</script>


<!-- Naver Analytics -->	
<script type="text/javascript" src="//wcs.naver.net/wcslog.js"></script>
<script type="text/javascript">
  if(!wcs_add) var wcs_add = {};
    wcs_add["wa"] = "18cbce78e94161";
  wcs_do();
</script>

</body>

<script id="dsq-count-scr" src="//greeksharifa-github-io.disqus.com/count.js" async></script>

</html>
