<!DOCTYPE html>
<html lang="en-us">
<head>
  <head>
  <!-- Description of Blog -->
  <meta name="description" content="Python, Machine & Deep Learning">
  <link rel="canonical" href="https://greeksharifa.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Python, Machine & Deep Learning">
  <meta property="og:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta property="og:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta property="og:url" content="https://greeksharifa.github.io/">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python, Machine & Deep Learning">
  <meta name="twitter:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta name="twitter:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta name="twitter:domain" content="https://greeksharifa.github.io/">

  <!-- link -->
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="icon-144x144" sizes="144x144" href="/public/img/icon-144x144.png">
  <link rel="shortcut icon" href="/public/img/icon_32x32.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  

  <!-- Ads -->
  <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
  </script>
</head>

  <!-- for Google AdSense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9951774327887666",
    enable_page_level_ads: true
  });
</script>

  <style>blockquote {
    font-size: 1em;
    line-height: 1.4
  }</style>
  <link href='http://fonts.googleapis.com/css?family=Gill+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Consolas' rel='stylesheet' type='text/css'>
</head>
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/3c2986ad7ac1f2230ea3596f44563328">
          <img src="/public/img/maple_tree.jpg" title="Cover Photo" alt="Maple tree" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>Developer and Analyst</strong>, YW & YY.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/greeksharifa">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:greeksharifa@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://greeksharifa.github.io/">
          Github Project
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 YW & YY. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://greeksharifa.github.io">YW & YY</a>
    </p>
  </div>
</div>


<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
     content to avoid any CSS collisions with our real content. -->
<div class="wrap">
  <div class="masthead">
    <div class="container">
      <h3 class="masthead-title" align="center">
        <a href="/" title="Home" title="YW & YY">
          <img class="masthead-logo" src="/public/img/logo.png"/>
        </a>
        <small>YW & YY's Python, Machine & Deep Learning</small>
        <!-- HTML elements for search -->
        <a href="/search/" id="search_icon">
          <img src="/public/img/search.png" width="25" height="25"
               align="right" style="margin-top:5px; margin-bottom:0;"
               onmouseover="this.style.opacity=0.7" onmouseout="this.style.opacity=0.5"
               alt="search">
        </a>
      </h3>
    </div>
  </div>

  <div class="container content">
    <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/references/2019/01/26/Jupyter-usage/">
        Jupyter Notebook 사용법
      </a>
    </h1>

    <span class="post-date">26 Jan 2019</span>
     |
    
    <a href="/blog/tags/#jupyter" class="post-tag">Jupyter</a>
    
    <a href="/blog/tags/#miniconda" class="post-tag">Miniconda</a>
    
    <a href="/blog/tags/#usage" class="post-tag">usage</a>
    
    

    <article>
      <p>Jupyter notebook은 대화형 파이썬 인터프리터(Interpreter)로서 웹 브라우저 환경에서 파이썬 코드를 작성 및 실행할 수 있는 툴이다.<br />
서버에 Jupyter notebook을 설치하여 포트를 개방한 후 해당 url에 접속하여 원격으로 사용하거나, 로컬 환경에서 브라우저를 띄워 대화형 환경에서 코드를 작성 및 실행할 수 있다.</p>

<h2 id="설치-및-실행">설치 및 실행</h2>

<h3 id="설치">설치</h3>

<p>설치는 두 가지 방법이 있는데, 첫 번째는 <a href="https://www.anaconda.com/distribution/">Anaconda</a>와 함께 설치하는 방법이 있다. Anaconda를 설치할 때 Jupyter Notebook도 같이 설치하면 된다.<br />
Anaconda와 같은 역할을 하는 Miniconda 사용법은 <a href="https://greeksharifa.github.io/references/2019/02/01/Miniconda-usage/">여기</a>를 참조하도록 한다.</p>

<p>Anadonda를 설치하는 방법 외에 기본적으로 pip은 Jupyter 패키지 설치를 지원한다. 설치 방법은 다른 패키지 설치 방법과 똑같다.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install jupyter
</code></pre></div></div>
<p>파이썬3과 2를 구분지어야 한다면 pip 대신 pip3를 사용한다.</p>

<h3 id="실행-및-종료">실행 및 종료</h3>

<script data-ad-client="ca-pub-9951774327887666" async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook
</code></pre></div></div>
<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/01.PNG" alt="01_jupyter_notebook" /></p>

<p>위 명령을 입력하면 자동으로 어떤 html 파일을 열면서 브라우저가 실행된다. 만약 실행되지 않는다면 http://localhost:8888 으로 접속하거나 위 그림의 맨 마지막 줄에 있는 url을 복사하여 브라우저에서 접속한다.</p>

<p>그러면 위 명령을 실행한 디렉토리 위치(위 그림에서 <code class="highlighter-rouge">jupyter notebook</code>을 실행한 줄에서 볼 수 있다. 필자의 경우 <code class="highlighter-rouge">C:\JupyterTest</code>)의 파일들이 브라우저에 보이게 된다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/02.PNG" alt="02_broswer" /></p>

<p>Jupyter의 실행을 종료하려면 명령창에서 <code class="highlighter-rouge">Ctrl + C</code>를 입력한다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/03.PNG" alt="03_terminate" /></p>

<h3 id="고급-실행-옵션">고급: 실행 옵션</h3>

<p>명령 옵션의 도움말을 표시한다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook --help
</code></pre></div></div>

<p>실행 속도 상승을 위해 MathJax를 무효화할 수 있다. MathJax는 수식 입력을 위해 필요한 JavaScript 라이브러리이다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook --no-mathjax
</code></pre></div></div>

<p>웹 브라우저를 지정하거나 실행시키지 않을 수 있다. 포트 번호 지정도 가능하다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook --browser="safari"
jupyter notebook --no-browser
jupyter notebook --port=8889
</code></pre></div></div>

<p>노트북 실행 시 실행 디렉토리를 지정할 수 있다. 기본값은 현재 밍령창에서의 실행 위치이다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook --notebook-dir=/user/define/directory
</code></pre></div></div>

<h3 id="고급-설정-파일-수정">고급: 설정 파일 수정</h3>

<p>매번 옵션을 지정해서 실행하기가 귀찮다면, Jupyter Notebook의 기본 설정을 변경하기 위해 다음 명령을 입력한다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook --generate-config
</code></pre></div></div>
<p>그러면 Jupyter가 실행되는 대신 설정 파일이 열린다.<br />
Linux에서는 기본적으로 <code class="highlighter-rouge">/home/&lt;username&gt;/.jupyter/jupyter_notebook_config.py</code> 파일로 생성되며, 윈도우에서는 <code class="highlighter-rouge">C:\Users\&lt;username&gt;\.jupyter\jupyter_notebook_config.py</code>로 생성된다.</p>

<p>설정 파일에서 필요한 옵션을 변경하여 사용하면 된다. 기본적으로 사용하지 않는 옵션은 모두 주석 처리되어 있다.</p>

<p>기본 설정 파일을 재지정하고 싶으면 다음과 같이 입력한다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook --config=custom_config.py
</code></pre></div></div>

<p>임시로 설정 파일을 변경해서 실행하고 싶다면 일반 옵션 주듯이 하면 된다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter notebook --config custom_config.py
</code></pre></div></div>

<p>Jupyter notebook을 단순히 로컬 환경에서 실행하는 것이 아니라 서버로 띄워 놓고 원격 접속을 하려면, 위 방법으로 허용 포트나 접속 주소 등 설정 파일을 수정해야 한다.</p>

<h3 id="고급-원격-접속-설정">고급: 원격 접속 설정</h3>

<p>localhost(127.0.0.1) 말고 다른 컴퓨터에서 (서버로) 원격접속하고 싶을 때가 있다. 그럴 때는 다음 과정을 따른다.</p>

<ol>
  <li>명령창(cmd or terminal)에 python 또는 ipython을 입력하여 대화창을 연다.
    <ul>
      <li>다음을 입력한다:
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  &gt;&gt;&gt; from notebook.auth import passwd
  &gt;&gt;&gt; passwd()
  Enter password: 
  Verity password: 
  'sha1:c5b493745105:0d26dcd6e9cf868d3b49f43d'
</code></pre></div>        </div>
      </li>
      <li>출력으로 나온 암호화된 비밀번호를 기억해 둔다.</li>
      <li>참고로 linux에서나 윈도우에서나 passwd() 등으로 비밀번호를 입력할 때에는 명령창에 입력하고 있는 문자가 전혀 표시되지 않는다. 별표(<code class="highlighter-rouge">*</code>)로도 표시되지 않으니 참고.</li>
      <li>대화창을 종료한다.</li>
    </ul>
  </li>
  <li>이제 조금 전에 생성한 <code class="highlighter-rouge">jupyter_notebook_config.py</code>를 편집기로 연다.
    <ul>
      <li>아래처럼 주석처리된 부분을 다음과 같이 바꾼다. 물론 비밀번호는 조금 전 여러분이 생성한 문자열로 입력해야 한다.
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  #c.NotebookApp.password = '' 
</code></pre></div>        </div>
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  c = get_config()
  c.NotebookApp.password = 'sha1:c5b493745105:0d26dcd6e9cf868d3b49f43d'
</code></pre></div>        </div>
      </li>
      <li><strong>필수:</strong> 비슷하게 다음 설정들을 바꿔주어야 한다. 모든 설정을 변경할 때에는 앞의 주석(<code class="highlighter-rouge">#</code>)을 지우도록 한다.
        <ul>
          <li>외부접속 허용: <code class="highlighter-rouge">c.NotebookApp.allow_origin = '*'</code></li>
          <li>IP 설정: <code class="highlighter-rouge">c.NotebookApp.ip = &lt;여러분의 IP&gt;</code></li>
        </ul>
      </li>
      <li><strong>옵션:</strong> 다음은 하고 싶으면 하도록 한다.
        <ul>
          <li>작업경로 설정: <code class="highlighter-rouge">c.NotebookApp.notebook_dir = &lt;원하는 경로&gt;</code></li>
          <li>포트 설정: <code class="highlighter-rouge">c.NotebookApp.port = &lt;원하는 port&gt;</code></li>
          <li>jupyter notebook으로 실행 시 브라우저 실행 여부: <code class="highlighter-rouge">c.NotebookApp.open_browser = False</code></li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p>이제 외부접속을 할 때는 서버에서</p>
<ul>
  <li>jupyter notebook을 실행시킨 다음</li>
  <li>
    <여러분의 IP="">:<원하는 port=""> 형식을 브라우저의 주소창에 입력하면 된다.
</원하는></여러분의>
    <ul>
      <li>예시: <code class="highlighter-rouge">123.212.321.14:8888</code></li>
    </ul>
  </li>
  <li>여러분이 설정한 비밀번호를 입력한다. 암호화된 문자열이 아니라 <code class="highlighter-rouge">passwd()</code> 에서 입력한 비밀번호면 된다.</li>
  <li>물론 일반 가정집에서는 그냥 ip를 할당할 수 없기 때문에 공유기 설정을 해주거나, 회사 컴퓨터 등이라면 따로 접속 허용하는 절차를 거쳐야 한다. 이 부분은 여기서는 ~pass~
    <ul>
      <li>그냥 되는 경우도 있다. 안 되는 경우에만 검색해서 해 보기 바람.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="jupyter의-기본-사용법">Jupyter의 기본 사용법</h2>

<h3 id="새-파일-생성">새 파일 생성</h3>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/04.PNG" alt="04_new" /></p>

<p>오른쪽 부분의 <code class="highlighter-rouge">New</code> 버튼을 클릭하면 Python 3, Text File, Folder, Terminal 등의 옵션이 있다(파이썬 버전에 따라 Python 2가 있을 수 있다). 우선 Python 3을 클릭하여 Python 3 코드를 입력할 수 있는 창을 열도록 한다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/05.PNG" alt="05_python3" /></p>

<p>생성하면 맨 위에 기본적으로 Untitled라는 제목으로 생성이 된다. 파일 탐색기나 Finder 등에서도 <code class="highlighter-rouge">Untitled.ipynb</code>라는 파일을 확인할 수 있다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/06.PNG" alt="06_ipynb" /></p>

<p>위의 checkpoints 디렉토리는 자동으로 생성된다. Jupyter는 자동저장이 되고(맨 위의 autosaved), 체크포인트를 따로 설정할 수 있다.</p>

<p>제목은 Untitled 부분을 클릭하면 수정할 수 있다.</p>

<h3 id="편집--명령-모드">편집 / 명령 모드</h3>

<p>편집 모드에서는 셀의 내용을 편집할 수 있고(셀의 테두리가 초록색), 명령 모드는 편집중이 아닌 상태 또는 셀 자체에 조작을 가하는 상태(셀의 테두리가 파란색)이다.<br />
명령 모드에서 편집 모드로 들어가려면 <code class="highlighter-rouge">Enter</code>키를, 반대로는 <code class="highlighter-rouge">Esc</code> 키를 누르면 된다.</p>

<h3 id="셀의-타입">셀의 타입</h3>

<p>Code 타입, Markdown 타입이 있다.<br />
Code 타입은 일반 코드를 실행할 수 있는 셀이다. 기본적으로 셀을 생성하면 Code 타입으로 생성된다.<br />
Markdown 타입은 <a href="https://greeksharifa.github.io/references/2018/06/29/markdown-usage/">Markdown</a>으로 셀의 내용을 작성할 수 있다. 코드로 실행되지는 않으며, 수식을 작성할 수 있다. 수식은 MathJax에 의해 지원된다. 수식 작성 방법은 <a href="https://greeksharifa.github.io/references/2018/06/29/equation-usage/">여기</a>를 참고한다.</p>

<p>Markdown 타입으로 변경하면 Markdown 코드를 작성할 수 있다. <code class="highlighter-rouge">Shift + Enter</code> 키를 누르면 마크다운이 실제 보여지는 방식으로 변경되며, 다시 수정하려면 <code class="highlighter-rouge">Enter</code> 또는 <code class="highlighter-rouge">더블 클릭</code>하면 편집 가능하다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/07.PNG" alt="07_markdown" /></p>

<h3 id="셀-실행">셀 실행</h3>

<p>실행하고 싶은 셀의 아무 곳에나 커서를 위치시킨 후 <code class="highlighter-rouge">Shift + Enter</code> 키를 누른다.<br />
실행하면 셀 아래쪽에는 실행 결과가 표시되고, 셀 옆의 ‘In [ ]’과 ‘Out [ ]’에 몇 번째로 실행시켰는지를 나타내는 숫자가 표시된다. 여러 번 실행하면 계속 숫자가 올라간다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/08.PNG" alt="08_run" /></p>

<h3 id="강제-중단--재실행">강제 중단 / 재실행</h3>

<p>제목 아래 줄의 탭에 Kernel 탭이 있다. 커널은 IPython 대화창 아래에서 백그라운드 비슷하게 실행되는 일종의 운영체제 같은 개념이다. IPython 대화창을 관리한다고 보면 된다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/09.PNG" alt="09_interrupt" /></p>

<p>Kernel 탭의 모든 버튼은 코드를 삭제하지는 않는다. 각 버튼의 기능을 설명하면,</p>

<ul>
  <li>Interrupt: 실행 중인 코드를 강제 중지한다. 중지하면 위 그림과 같은 에러가 뜨며 실행이 중지된다.</li>
  <li>Restart: 실행 중인 코드가 중지되며 재시작된다. 코드나 실행 결과는 삭제되지 않는다.</li>
  <li>Restart &amp; Clear Output: 코드는 중지되며 실행 결과도 삭제한다.</li>
  <li>Restart &amp; Run All: 재시작 후 모든 셀의 코드를 위에서부터 순차적으로 한 번씩 실행한다.</li>
  <li>Reconnect: 인터넷 연결이 끊어졌을 때 연결을 재시도한다.</li>
  <li>Shutdown: 커널을 종료한다. 이 버튼을 누르면 실행 결과는 삭제되지 않으나 완전 종료된 상태로 더 이상 메모리를 잡아먹지 않는다.</li>
</ul>

<p>Shutdown되었거나, 인터넷 연결이 끊어졌거나, 기타 문제가 있으면 아래와 같이 탭 옆에 알림이 표시된다. Shutdown 된 경우 No kernel이라고 뜬다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/10.PNG" alt="10_shutdowned" /></p>

<p>현재 실행중인 커널이 있는지 확인하는 방법은 두 가지다. 첫 번째는 Home 화면에서 <code class="highlighter-rouge">ipynb</code> 파일의 아이콘이 초록색이면 실행중, 회색이면 중단된 또는 시작되지 않은 상태이다. 여기서는 해당 디렉토리에서 실행중인 것만 확인할 수 있다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/11_shutdowned.PNG" alt="11_shutdowned" /></p>

<p>또 하나는 Home 화면에서 Files 탭 대신 Running 탭을 클릭하면 실행 중인 IPython과 터미널의 목록을 확인할 수 있다. 이 탭에서는 전체 디렉토리에서 실행중인 파일 또는 터미널을 전부 볼 수 있다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/12.PNG" alt="12_list" /></p>

<h3 id="text-file-생성">Text File 생성</h3>

<p>New 버튼에서 Text File을 생성하면 <code class="highlighter-rouge">.txt</code> 파일이나 <code class="highlighter-rouge">.py</code> 파일 등을 만들 수 있다. 이렇게 만든 파일은 대화 형식으로 실행되지 않고, 터미널에서 실행시켜야 한다. 읽는 것은 IPython 창에서도 가능하다.</p>

<h3 id="folder-생성">Folder 생성</h3>

<p>디렉토리를 생성할 때 사용한다. 폴더랑 같은 것이다.</p>

<h3 id="터미널">터미널</h3>

<p>New 버튼으로 Terminal을 클릭하면, 터미널을 하나 새로 연다. 이것은 윈도우나 맥 등의 명령창(cmd 또는 terminal)과 같다. 여기서 .py 파일을 실행시킬 수 있고, 파일의 목록을 보거나 삭제하는 등의 명령이 모두 가능하다. Running 탭에서 중지시킬 수 있다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/13.PNG" alt="13_terminal" /></p>

<h3 id="파일-이름-변경-또는-삭제">파일 이름 변경 또는 삭제</h3>

<p>파일 맨 왼쪽의 체크박스를 클릭하면 복제, 수정, 삭제 등이 가능하다. 물론 로컬 파일 탐색기에서 수정이나 삭제를 해도 되며, 서버가 연결에 문제가 없으면 바로 반영된다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/14.PNG" alt="14_name" /></p>

<h3 id="자동완성">자동완성</h3>

<p>웬만한 IDE에는 다 있는 자동완성 기능이다. 변수나 함수 등을 일부만 입력하고 <code class="highlighter-rouge">Tab</code> 키를 누르면 된다. 따로 설명할 필요는 없을 듯 하다.</p>

<hr />

<h2 id="단축키">단축키</h2>

<p>단축키 정보는 [Help] - [Keyboard Shortcuts] 또는 명령 모드에서 <code class="highlighter-rouge">H</code>를 눌러서 표시할 수 있다.</p>

<table>
  <thead>
    <tr>
      <th>공용 단축키</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Shift + Enter</td>
      <td>액티브 셀을 실행하고 아래 셀을 선택한다.</td>
    </tr>
    <tr>
      <td>Ctrl + Enter</td>
      <td>액티브 셀을 실행한다.</td>
    </tr>
    <tr>
      <td>Alt + Enter</td>
      <td>액티브 셀을 실행하고 아래에 셀을 하나 생성한다.</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>편집 모드 단축키</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Ctrl + Z</td>
      <td>Undo 명령이다.</td>
    </tr>
    <tr>
      <td>Ctrl + Shift + Z</td>
      <td>Redo 명령이다.</td>
    </tr>
    <tr>
      <td>Tab</td>
      <td>자동완성 또는 Indent를 추가한다.</td>
    </tr>
    <tr>
      <td>Shift + Tab</td>
      <td>툴팁 또는 변수의 상태를 표시한다.</td>
    </tr>
    <tr>
      <td>Ctrl + Shift + -</td>
      <td>커서의 위치에서 셀을 잘라 두 개로 만든다.</td>
    </tr>
  </tbody>
</table>

<p>참고로 명령 모드 단축키 중 콤마(<code class="highlighter-rouge">,</code>)로 되어 있는 것은 연속해서 누르라는 의미이다. 예로 <code class="highlighter-rouge">D</code>를 두 번 누르면 액티브 코드 셀을 삭제한다.</p>

<table>
  <thead>
    <tr>
      <th>명령 모드 단축키</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>↑, ↓</td>
      <td>셀 선택</td>
    </tr>
    <tr>
      <td>A</td>
      <td>액티브 코드 셀의 위(Above)에 셀을 하나 생성한다.</td>
    </tr>
    <tr>
      <td>B</td>
      <td>액티브 코드 셀의 위(Below)에 셀을 하나 생성한다.</td>
    </tr>
    <tr>
      <td>Ctrl + S</td>
      <td>Notebook 파일을 저장한다.</td>
    </tr>
    <tr>
      <td>Shift + L</td>
      <td>줄 번호 표시를 토글한다.</td>
    </tr>
    <tr>
      <td>D, D</td>
      <td>(D 두번 연속으로 타이핑)액티브 코드 셀을 삭제한다.</td>
    </tr>
    <tr>
      <td>Z</td>
      <td>삭제한 셀을 하나 복원한다.</td>
    </tr>
    <tr>
      <td>Y</td>
      <td>액티브 코드 셀을 Code 타입(코드를 기술하는 타입)으로 한다.</td>
    </tr>
    <tr>
      <td>M</td>
      <td>액티브 코드 셀을 Markdown 타입으로 한다.</td>
    </tr>
    <tr>
      <td>O, O</td>
      <td>커널을 재시작한다.</td>
    </tr>
    <tr>
      <td>P</td>
      <td>명령 팔레트를 연다.</td>
    </tr>
    <tr>
      <td>H</td>
      <td>단축키 목록을 표시한다. <code class="highlighter-rouge">Enter</code> 키로 숨긴다.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="jupyter의-기능">Jupyter의 기능</h2>

<h3 id="docstring의-표시">DocString의 표시</h3>

<p>선언한 변수 뒤에 <code class="highlighter-rouge">?</code>를 붙여서 셀을 실행하는 것으로 해당 변수의 상태를 확인할 수 있다.</p>

<p>약간 다른 방법으로 변수를 타이핑한 후 <code class="highlighter-rouge">Shift + Tab</code>을 누르면 툴팁이 표시된다.<br />
툴팁에는 DocString의 일부 내용이 표시된다.</p>

<h3 id="이미지-첨부하기">이미지 첨부하기</h3>

<p>Drag &amp; Drop으로 첨부할 수 있다.</p>

<h3 id="shell명령-프롬프트의-이용">shell(명령 프롬프트)의 이용</h3>

<p>명령창에서 쓰는 명령을 그대로 쓰되, 맨 앞에 <code class="highlighter-rouge">!</code>를 입력하여 사용 가능하다.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!cd Documents
</code></pre></div></div>

<h3 id="매직-명령어-이용">매직 명령어 이용</h3>

<p>맨 앞에 <code class="highlighter-rouge">%</code>를 붙이고 특정 명령을 수행할 수 있다. 이는 파이썬 문법에는 포함되지 않은, Jupyter notebook만의 기능이다.</p>

<p><img src="/public/img/Jupyter/2019-01-26-Jupyter-usage/15.PNG" alt="15_magic" /></p>

<table>
  <thead>
    <tr>
      <th>매직 명령어</th>
      <th>설명</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>%pwd</td>
      <td>현재 디렉토리 경로 출력</td>
    </tr>
    <tr>
      <td>%time <code class="highlighter-rouge">코드</code></td>
      <td><code class="highlighter-rouge">코드</code>의 실행 시간을 측정하여 표시</td>
    </tr>
    <tr>
      <td>%timeit <code class="highlighter-rouge">코드</code></td>
      <td><code class="highlighter-rouge">코드</code>를 여러 번 실행한 결과를 요약하여 표시</td>
    </tr>
    <tr>
      <td>%history -l 3</td>
      <td>최근 3개의 코드 실행 이력 취득</td>
    </tr>
    <tr>
      <td>%ls</td>
      <td>윈도우의 dir, Linux의 ls 명령과 같음</td>
    </tr>
    <tr>
      <td>%autosave <code class="highlighter-rouge">n</code></td>
      <td>자동저장 주기를 설정한다. 초 단위이며, 0이면 무효로 한다.</td>
    </tr>
    <tr>
      <td>%matplotlib</td>
      <td>그래프를 그리는 코드 위에 따로 설정한다. <code class="highlighter-rouge">%matplotlib inline</code>으로 설정하면 코드 셀의 바로 아래에, <code class="highlighter-rouge">%matplotlib tk</code>로 설정하면 별도 창에 그래프가 출력된다. <code class="highlighter-rouge">%matplotlib notebook</code>으로 하면 코드 셀 바로 아래에 동적으로 그래프를 조작할 수 있는 그래프가 생성된다.</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 코드 실행 시간 측정
</span><span class="o">%</span><span class="n">time</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">))</span>
<span class="c1"># 결과:
# CPU times: user 225 us, sys: 0 ns, total: 225 us
# Wall time: 228 us
# 499950000
</span>
<span class="c1"># 1000회 반복, 3회 실행
</span><span class="o">%</span><span class="n">timeit</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">))</span>
<span class="c1"># 결과: 
# 1000 loops, best of 3: 238 us for loop
</span>
<span class="c1"># 옵션 지정하기
</span><span class="o">%</span><span class="n">timeit</span> <span class="o">-</span><span class="n">n</span> <span class="mi">2000</span> <span class="o">-</span><span class="n">r</span> <span class="mi">5</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">))</span>

<span class="c1"># 셀 전체의 시간 측정
</span><span class="o">%%</span><span class="n">timeit</span> <span class="o">-</span><span class="n">n</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">r</span> <span class="mi">3</span>
<span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">+=</span> <span class="n">i</span>
</code></pre></div></div>


    </article>
    <div class="post-more">
      
      <a href="/references/2019/01/26/Jupyter-usage/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/references/2019/01/26/Jupyter-usage/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/">
        PyTorch 사용법 - 03. How to Use PyTorch
      </a>
    </h1>

    <span class="post-date">10 Nov 2018</span>
     |
    
    <a href="/blog/tags/#pytorch" class="post-tag">PyTorch</a>
    
    

    <article>
      <hr />

<p><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-00-references/">PyTorch 사용법 - 00. References</a><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-01-introduction/">PyTorch 사용법 - 01. 소개 및 설치</a><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/">PyTorch 사용법 - 02. Linear Regression Model</a><br />
<strong><a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/">PyTorch 사용법 - 03. How to Use PyTorch</a></strong><br />
<a href="https://greeksharifa.github.io/pytorch/2019/06/12/pytorch-usage-04-RNN-Model/">PyTorch 사용법 - 04. Recurrent Neural Network Model</a></p>

<hr />

<p><em>2020.02.04 Updated</em></p>

<p>이 글에서는 PyTorch 프로젝트를 만드는 방법에 대해서 알아본다.</p>

<p>사용되는 torch 함수들의 사용법은 <a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-00-references/">여기</a>에서 확인할 수 있다.</p>

<p>Pytorch의 학습 방법(loss function, optimizer, autograd, backward 등이 어떻게 돌아가는지)을 알고 싶다면 <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#train-model">여기</a>로 바로 넘어가면 된다.</p>

<p>Pytorch 사용법이 헷갈리는 부분이 있으면 <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#q--a">Q&amp;A 절</a>을 참고하면 된다.</p>

<p>예시 코드의 많은 부분은 링크와 함께 공식 Pytorch 홈페이지(pytorch.org/docs)에서 가져왔음을 밝힌다.</p>

<p><em>주의: 이 글은 좀 길다. ㅎ</em></p>

<hr />

<h1 id="import">Import</h1>

<script data-ad-client="ca-pub-9951774327887666" async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># preprocess, set hyperparameter
</span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># load data
</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="c1"># train
</span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># visualization
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<hr />

<h1 id="argparse">argparse</h1>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train.py --epochs 50 --batch-size 64 --save-dir weights
</code></pre></div></div>
<p>Machine Learning을 포함해서, 위와 같은 실행 옵션은 많은 코드에서 볼 수 있었을 것이다. 학습 과정을 포함하여 대부분은 명령창 또는 콘솔에서 <code class="highlighter-rouge">python 파일 옵션들...</code>으로 실행시키기 때문에, argparse에 대한 이해는 필요하다.</p>

<p>argparse에 대한 내용은 <a href="https://greeksharifa.github.io/references/2019/02/12/argparse-usage/">여기</a>를 참조하도록 한다.</p>

<hr />

<h1 id="load-data">Load Data</h1>

<p>전처리하는 과정을 설명할 수는 없다. 데이터가 어떻게 생겼는지는 직접 봐야 알 수 있다.<br />
다만 한 번 쓰고 말 것이 아니라면, 데이터가 추가되거나 변경점이 있더라도 전처리 코드의 대대적인 수정이 발생하도록 짜는 것은 본인 손해이다.</p>

<h2 id="단순한-방법">단순한 방법</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/02_Linear_Regression_Model_Data.csv'</span><span class="p">)</span>
<span class="c1"># Avoid copy data, just refer
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'x'</span><span class="p">].</span><span class="n">values</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
</code></pre></div></div>
<p><code class="highlighter-rouge">pandas</code>나 <code class="highlighter-rouge">csv</code> 패키지 등으로 그냥 불러오는 방법이다. 데이터가 복잡하지 않은 형태라면 단순하고 유용하게 쓸 수 있다. 그러나 이 글에서 중요한 부분은 아니다.</p>

<h2 id="torchutilsdatadataloader">torch.utils.data.DataLoader</h2>

<p>참조: <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">torch.utils.data.DataLoader</a></p>

<p>Pytorch는 <code class="highlighter-rouge">DataLoader</code>라고 하는 괜찮은 utility를 제공한다. 간단하게 생각하면 DataLoader 객체는 학습에 쓰일 데이터 전체를 보관했다가, train 함수가 batch 하나를 요구하면 batch size 개수만큼 데이터를 꺼내서 준다고 보면 된다.</p>
<ul>
  <li>실제로 <code class="highlighter-rouge">[batch size, num]</code>처럼 미리 잘라놓는 것은 아니고, 내부적으로 Iterator에 포함된 Index가 존재한다. train() 함수가 데이터를 요구하면 사전에 저장된 batch size만큼 return하는 형태이다.</li>
</ul>

<p>사용할 <code class="highlighter-rouge">torch.utils.data.Dataset</code>에 따라 반환하는 데이터(자연어, 이미지, 정답 label 등)는 조금씩 다르지만, 일반적으로 실제 DataLoader를 쓸 때는 다음과 같이 쓰기만 하면 된다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>DataLoader 안에 데이터가 어떻게 들어있는지 확인하기 위해, MNIST 데이터를 가져와 보자. DataLoader는 <code class="highlighter-rouge">torchvision.datasets</code> 및 <code class="highlighter-rouge">torchvision.transforms</code>와 함께 자주 쓰이는데, 각각 Pytorch가 공식적으로 지원하는 <a href="https://pytorch.org/docs/stable/torchvision/datasets.html">dataset</a>, <a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=torchvision%20transforms">데이터 transformation 및 augmentation 함수들</a>(주로 이미지 데이터에 사용)를 포함한다.<br />
각각의 사용법은 아래 절을 참조한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_size</span> <span class="o">=</span> <span class="mi">28</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">input_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)),</span>
                                <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()])</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s">'data/mnist'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'type:'</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">data_loader</span><span class="p">),</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>

<span class="n">first_batch</span> <span class="o">=</span> <span class="n">data_loader</span><span class="p">.</span><span class="n">__iter__</span><span class="p">().</span><span class="n">__next__</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'{:15s} | {:&lt;25s} | {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">,</span> <span class="s">'size'</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'{:15s} | {:&lt;25s} | {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">'Num of Batch'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'{:15s} | {:&lt;25s} | {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">'first_batch'</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">first_batch</span><span class="p">)),</span> <span class="nb">len</span><span class="p">(</span><span class="n">first_batch</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'{:15s} | {:&lt;25s} | {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">'first_batch[0]'</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">first_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">first_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'{:15s} | {:&lt;25s} | {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">'first_batch[1]'</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">first_batch</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">first_batch</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">shape</span><span class="p">))</span>
</code></pre></div></div>

<p>결과:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>type: &lt;class 'torch.utils.data.dataloader.DataLoader'&gt; 

name            | type                      | size
Num of Batch    |                           | 938
first_batch     | &lt;class 'list'&gt;            | 2
first_batch[0]  | &lt;class 'torch.Tensor'&gt;    | torch.Size([64, 1, 28, 28])
first_batch[1]  | &lt;class 'torch.Tensor'&gt;    | torch.Size([64])
# 총 데이터의 개수는 938 * 28 ~= 60000(마지막 batch는 32)이다.
</code></pre></div></div>

<h3 id="custom-dataset-만들기">Custom Dataset 만들기</h3>

<p><strong>nn.Module</strong>을 상속하는 Custom Model처럼, Custom DataSet은 <code class="highlighter-rouge">torch.utils.data.Dataset</code>를 상속해야 한다. 또한 override해야 하는 것은 다음 두 가지다. <code class="highlighter-rouge">python dunder</code>를 모른다면 먼저 구글링해보도록 한다.</p>
<ul>
  <li><code class="highlighter-rouge">__len__(self)</code>: dataset의 전체 개수를 알려준다.</li>
  <li><code class="highlighter-rouge">__getitem__(self, idx)</code>: parameter로 idx를 넘겨주면 idx번째의 데이터를 반환한다.</li>
</ul>

<p>위의 두 가지만 기억하면 된다. 전체 데이터 개수와, i번째 데이터를 반환하는 함수만 구현하면 Custom DataSet이 완성된다.<br />
다음에는 완성된 DataSet을 <code class="highlighter-rouge">torch.utils.data.DataLoader</code>에 인자로 전달해주면 끝이다.</p>

<p>완전 필수는 아니지만 <code class="highlighter-rouge">__init__()</code>도 구현하는 것이 좋다.</p>

<p>1차함수 선형회귀(Linear Regression)의 <a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/#load-data">예</a>를 들면 다음과 같다.<br />
데이터는 <a href="https://drive.google.com/file/d/1gVxV5eD5NfyEO4aHSyAGmsDgUco8FQPb/view?usp=sharing">여기</a>에서 받을 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LinearRegressionDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">csv_file</span><span class="p">):</span>
        <span class="s">"""
        Args:
            csv_file (string): Path to the csv file. 
        """</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'x'</span><span class="p">].</span><span class="n">values</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">LinearRegressionDataset</span><span class="p">(</span><span class="s">'02_Linear_Regression_Model_Data.csv'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="torchvisiondatasets">torchvision.datasets</h2>

<p>참조: <a href="https://pytorch.org/docs/stable/torchvision/datasets.html">torchvision.datasets</a></p>

<p>Pytorch가 공식적으로 다운로드 및 사용을 지원하는 datasets이다. 2020.02.04 기준 dataset 목록은 다음과 같다.</p>

<ul>
  <li>MNIST
    <ul>
      <li>MNIST(숫자 0~9에 해당하는 손글씨 이미지 6만(train) + 1만(test))</li>
      <li>Fashion-MNIST(간소화된 의류 이미지),</li>
      <li>KMNIST(일본어=히라가나, 간지 손글씨),</li>
      <li>EMNIST(영문자 손글씨),</li>
      <li>QMNIST(MNIST를 재구성한 것)</li>
    </ul>
  </li>
  <li>MS COCO
    <ul>
      <li>Captions(이미지 한 장과 이를 설명하는 한 영문장),</li>
      <li>Detection(이미지 한 장과 여기에 있는 object들을 segmantation한 정보)</li>
    </ul>
  </li>
  <li>LSUN(https://www.yf.io/p/lsun),</li>
  <li><em>ImageFolder</em>, <em>DatasetFolder</em></li>
  <li>Image:
    <ul>
      <li>ImageNet 2012,</li>
      <li>CIFAR10 &amp; CIFAR100,</li>
      <li>STL10, SVHN, PhotoTour, SBU</li>
    </ul>
  </li>
  <li>Flickr8k &amp; Flickr30k, VOC Segmantation &amp; Detection,</li>
  <li>Cityscapes, SBD, USPS, Kinetics-400, HMDB51, UCF101</li>
</ul>

<p>각각의 dataset마다 필요한 parameter가 조금씩 다르기 때문에, <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#mnist">MNIST</a>만 간단히 설명하도록 하겠다. 사실 공식 홈페이지를 참조하면 어렵지 않게 사용 가능하다.</p>

<p><img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/01.PNG" alt="01_MNIST" /></p>

<ul>
  <li>root: 데이터를 저장할 루트 폴더이다. 보통 <code class="highlighter-rouge">data/</code>나 <code class="highlighter-rouge">data/mnist/</code>를 많이 쓰는 것 같지만, 상관없다.</li>
  <li>train: 학습 데이터를 받을지, 테스트 데이터를 받을지를 결정한다.</li>
  <li>download: true로 지정하면 알아서 다운로드해 준다. 이미 다운로드했다면 재실행해도 다시 받지 않는다.</li>
  <li>transform: 지정하면 이미지 데이터에 어떤 변형을 가할지를 transform function의 묶음(Compose)로 전달한다.</li>
  <li>target_transform: 보통 위의 transform까지만 쓰는 것 같다. 쓰고 싶다면 이것도 쓰자.</li>
</ul>

<h2 id="torchvisiontransforms">torchvision.transforms</h2>

<p>참조: <a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=torchvision%20transforms">torchvision.transforms</a></p>

<ol>
  <li>
    <p>이미지 변환 함수들을 포함한다. 상대적으로 자주 쓰이는 함수는 다음과 같은 것들이 있다. 더 많은 목록은 홈페이지를 참조하면 된다. 참고로 parameter 중 <code class="highlighter-rouge">transforms</code>는 변환 함수들의 list 또는 tuple이다.</p>

    <ul>
      <li>transforms.CenterCrop(size): 이미지의 중앙 부분을 크롭하여 [size, size] 크기로 만든다.</li>
      <li>transforms.Resize(size, interpolation=2): 이미지를 지정한 크기로 변환한다. 직사각형으로 자를 수 있다.
        <ul>
          <li>참고: transforms.Scale는 Resize에 의해 deprecated되었다.</li>
        </ul>
      </li>
      <li>transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=’constant’): 이미지의 랜덤한 부분을 [size, size] 크기로 잘라낸다. input 이미지가 output 크기보다 작으면 padding을 추가할 수 있다.</li>
      <li>transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 3/4), interpolation=2): 이미지를 랜덤한 크기 및 비율로 자른다.
        <ul>
          <li>참고: transforms.RandomSizedCrop는 RandomResizedCrop에 의해 deprecated되었다.</li>
        </ul>
      </li>
      <li>transforms.RandomRotation(degrees, resample=False, expand=False, center=None): 이미지를 랜덤한 각도로 회전시킨다.</li>
      <li>transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0): brightness, contrast 등을 변화시킨다.</li>
    </ul>
  </li>
  <li>이미지를 torch.Tensor 또는 PILImage로 변환시킬 수 있다. 사용자 정의 변환도 가능하다.
    <ul>
      <li>transforms.ToPILImage(mode=None): PILImage로 변환시킨다.</li>
      <li>transforms.ToTensor(): torch.Tensor로 변환시킨다.</li>
      <li>transforms.Lambda(lambd): 사용자 정의 lambda function을 적용시킨다.</li>
    </ul>
  </li>
  <li>torch.Tensor에 적용해야 하는 변환 함수들도 있다.
    <ul>
      <li>transforms.LinearTransformation(transformation_matrix): tensor로 표현된 이미지에 선형 변환을 시킨다.</li>
      <li>transforms.Normalize(mean, std, inplace=False): tensor의 데이터 수치(또는 범위)를 정규화한다.</li>
    </ul>
  </li>
  <li>brightness나 contrast 등을 바꿀 수도 있다.
    <ul>
      <li>transforms.functional.adjust_contrast(img, contrast_factor) 등</li>
    </ul>
  </li>
  <li>
    <p>위의 변환 함수들을 랜덤으로 적용할지 말지 결정할 수도 있다.</p>

    <ul>
      <li>transforms.RandomChoice(transforms): <code class="highlighter-rouge">transforms</code> 리스트에 포함된 변환 함수 중 랜덤으로 1개 적용한다.</li>
      <li>transforms.RandomApply(transforms, p=0.5): <code class="highlighter-rouge">transforms</code> 리스트에 포함된 변환 함수들을 p의 확률로 적용한다.</li>
    </ul>
  </li>
  <li>
    <p>위의 모든 변환 함수들을 하나로 조합하는 함수는 다음과 같다. 이 함수를 <code class="highlighter-rouge">dataloader</code>에 넘기면 이미지 변환 작업이 간단하게 완료된다.</p>

    <ul>
      <li>transforms.Compose(transforms)
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
 <span class="n">transforms</span><span class="p">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">14</span><span class="p">),</span>
 <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
 <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">std</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="p">])</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<p>변환 순서는 보통 resize/crop, toTensor, Normalize 순서를 거친다. Normalize는 tensor에만 사용 가능하므로 이 부분은 순서를 지켜야 한다.</p>

<h2 id="torchtext">torchtext</h2>

<p>자연어처리(NLP)를 다룰 때 쓸 수 있는 좋은 라이브러리가 있다. 이는 자연어처리 데이터셋을 다루는 데 있어서 매우 편리한 기능을 제공한다.</p>
<ul>
  <li>데이터셋 로드</li>
  <li>토큰화(Tokenization)</li>
  <li>단어장(Vocabulary) 생성</li>
  <li>Index mapping: 각 단어를 해당하는 인덱스로 매핑</li>
  <li>단어 벡터(Word Vector): word embedding을 만들어준다. 0이나 랜덤 값 및 사전학습된 값으로 초기화할 수 있다.</li>
  <li>Batch 생성 및 (자동) padding 수행</li>
</ul>

<p>설치는 다음과 같다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install torchtext
# conda 환경에선 다음과 같다.
conda install -c pytorch torchtext
</code></pre></div></div>

<hr />

<h1 id="define-and-load-model">Define and Load Model</h1>

<h2 id="pytorch-model">Pytorch Model</h2>

<p>gradient 계산 방식 등 Pytorch model의 작동 방식은 <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#set-loss-functioncreterion-and-optimizer">Set Loss function(creterion) and Optimizer 절</a>을 보면 된다.</p>

<p>Pytorch에서 쓰는 용어는 Module 하나에 가깝지만, 많은 경우 layer나 model 등의 용어도 같이 사용되므로 굳이 구분하여 적어 보았다.</p>

<p><strong>Layer</strong> : Model 또는 Module을 구성하는 한 개의 층, Convolutional Layer, Linear Layer 등이 있다.<br />
<strong>Module</strong> : 1개 이상의 Layer가 모여서 구성된 것. Module이 모여 새로운 Module을 만들 수도 있다.<br />
<strong>Model</strong> : 여러분이 최종적으로 원하는 것. 당연히 한 개의 Module일 수도 있다.</p>

<p>예를 들어 <strong>nn.Linear</strong>는 한 개의 layer이기도 하며, 이것 하나만으로도 module이나 Model을 구성할 수 있다. 단순 Linear Model이 필요하다면, <code class="highlighter-rouge"><span class="k">model</span> <span class="p">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="nb">True</span><span class="p">)</span></code>처럼 사용해도 무방하다.</p>

<p>PyTorch의 모든 모델은 기본적으로 다음 구조를 갖는다. PyTorch 내장 모델뿐 아니라 사용자 정의 모델도 반드시 이 정의를 따라야 한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">Model_Name</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model_Name</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">module1</span> <span class="o">=</span> <span class="p">...</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">module2</span> <span class="o">=</span> <span class="p">...</span>
        <span class="s">"""
        ex)
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)
        """</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">some_function1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">some_function2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="s">"""
        ex)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        """</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>PyTorch 모델로 쓰기 위해서는 다음 조건을 따라야 한다. 내장된 모델들(<strong>nn.Linear</strong> 등)은 당연히 이 조건들을 만족한다.</p>

<ol>
  <li><strong>torch.nn.Module</strong>을 상속해야 한다.</li>
  <li><code class="highlighter-rouge">__init__()</code>과 <code class="highlighter-rouge">forward()</code>를 override해야 한다.
    <ul>
      <li>사용자 정의 모델의 경우 init과 forward의 인자는 자유롭게 바꿀 수 있다. 이름이 x일 필요도 없으며, 인자의 개수 또한 달라질 수 있다.</li>
    </ul>
  </li>
</ol>

<p>이 두 가지 조건은 PyTorch의 기능들을 이용하기 위해 필수적이다.</p>

<p>따르지 않는다고 해서 에러를 내뱉진 않지만, 다음 규칙들은 따르는 것이 좋다:</p>

<ol>
  <li><code class="highlighter-rouge">__init__()</code>에서는 모델에서 사용될 module을 정의한다. module만 정의할 수도, activation function 등을 전부 정의할 수도 있다.
    <ul>
      <li>아래에서 설명하겠지만 module은 <strong>nn.Linear</strong>, <strong>nn.Conv2d</strong> 등을 포함한다.</li>
      <li>activation function은 <strong>nn.functional.relu</strong>, <strong>nn.functional.sigmoid</strong> 등을 포함한다.</li>
    </ul>
  </li>
  <li><code class="highlighter-rouge">forward()</code>에서는 모델에서 행해져야 하는 계산을 정의한다(대개 train할 때). 모델에서 forward 계산과 backward gradient 계산이 있는데, 그 중 forward 부분을 정의한다. input을 네트워크에 통과시켜 어떤 output이 나오는지를 정의한다고 보면 된다.
    <ul>
      <li><code class="highlighter-rouge">__init__()</code>에서 정의한 module들을 그대로 갖다 쓴다.</li>
      <li>위의 예시에서는 <code class="highlighter-rouge">__init__()</code>에서 정의한 <code class="highlighter-rouge">self.conv1</code>과 <code class="highlighter-rouge">self.conv2</code>를 가져다 썼고, activation은 미리 정의한 것을 쓰지 않고 즉석에서 불러와 사용했다.</li>
      <li>backward 계산은 PyTorch가 알아서 해 준다. <code class="highlighter-rouge">backward()</code> 함수를 호출하기만 한다면.</li>
    </ul>
  </li>
</ol>

<h3 id="nnmodule">nn.Module</h3>

<p><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/#import">여기</a>를 참고한다. 요약하면 <strong>nn.Module</strong>은 모든 PyTorch 모델의 base class이다.</p>

<h3 id="nnmodule-내장-함수">nn.Module 내장 함수</h3>

<p><a href="https://pytorch.org/docs/stable/nn.html#module">nn.Module</a>에 내장된 method들은 모델을 추가 구성/설정하거나, train/eval(test) 모드 변경, cpu/gpu 변경, 포함된 module 목록을 얻는 등의 활동에 초점이 맞춰져 있다.</p>

<p>모델을 추가로 구성하려면,</p>
<ul>
  <li><code class="highlighter-rouge">add_module(name, module)</code>: 현재 module에 새로운 module을 추가한다.</li>
  <li><code class="highlighter-rouge">apply(fn)</code>: 현재 module의 모든 submodule에 해당 함수(fn)을 적용한다. 주로 model parameter를 초기화할 때 자주 쓴다.</li>
</ul>

<p>모델이 어떻게 생겼는지 보려면,</p>
<ul>
  <li><code class="highlighter-rouge">children()</code>, <code class="highlighter-rouge">modules()</code>: 자식 또는 모델 전체의 모든 module에 대한 iterator를 반환한다.</li>
  <li><code class="highlighter-rouge">named_buffers(), named_children(), named_modules(), named_parameters()</code>: 위 함수와 비슷하지만 이름도 같이 반환한다.</li>
</ul>

<p>모델을 통째로 저장 혹은 불러오려면,</p>
<ul>
  <li><code class="highlighter-rouge">state_dict(destination=None, prefix='', keep_vars=False)</code>: 모델의 모든 상태(parameter, running averages 등 buffer)를 딕셔너리 형태로 반환한다.</li>
  <li><code class="highlighter-rouge">load_state_dict(state_dict, strict=True)</code>: parameter와 buffer 등 모델의 상태를 현 모델로 복사한다. <code class="highlighter-rouge">strict=True</code>이면 모든 module의 이름이 <em>정확히</em> 같아야 한다.</li>
</ul>

<p>학습 시에 필요한 함수들을 살펴보면,</p>
<ul>
  <li><code class="highlighter-rouge">cuda(device=None)</code>: 모든 model parameter를 GPU 버퍼에 옮기는 것으로 GPU를 쓰고 싶다면 이를 활성화해주어야 한다.
    <ul>
      <li>GPU를 쓰려면 두 가지에 대해서만 <code class="highlighter-rouge">.cuda()</code>를 call하면 된다. 그 두 개는 모든 input batch 또는 tensor, 그리고 모델이다.</li>
      <li><code class="highlighter-rouge">.cuda()</code>는 optimizer를 설정하기 전에 실행되어야 한다. 잊어버리지 않으려면 모델을 생성하자마자 쓰는 것이 좋다.</li>
    </ul>
  </li>
  <li><code class="highlighter-rouge">eval()</code>, <code class="highlighter-rouge">train()</code>: 모델을 train mode 또는 eval(test) mode로 변경한다. Dropout이나 BatchNormalization을 쓰는 모델은 학습시킬 때와 평가할 때 구조/역할이 다르기 때문에 반드시 이를 명시하도록 한다.</li>
  <li><code class="highlighter-rouge">parameters(recurse=True)</code>: module parameter에 대한 iterator를 반환한다. 보통 optimizer에 넘겨줄 때 말고는 쓰지 않는다.</li>
  <li><code class="highlighter-rouge">zero_grad()</code>: 모든 model parameter의 gradient를 0으로 설정한다.</li>
</ul>

<p>사용하는 법은 매우 간단히 나타내었다. Optimizer에 대한 설명은 <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#set-loss-functioncreterion-and-optimizer">여기</a>를 참조하면 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">def</span> <span class="nf">user_defined_initialize_function</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">vgg16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># 예시는 예시일 뿐
</span><span class="n">last_module</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">add_module</span><span class="p">(</span><span class="s">'last_module'</span><span class="p">,</span> <span class="n">last_module</span><span class="p">)</span>
<span class="n">last_module</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">user_defined_initialize_function</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>

<span class="c1"># set optimizer. model.parameter를 넘겨준다.
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>

<span class="c1"># train
</span><span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">[</span><span class="s">'train'</span><span class="p">]:</span>
    <span class="p">...</span>

<span class="c1"># test
</span><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">[</span><span class="s">'test'</span><span class="p">]:</span>
    <span class="p">...</span>
</code></pre></div></div>

<hr />

<h2 id="pytorch-layer의-종류">Pytorch Layer의 종류</h2>

<p>참조: <a href="https://pytorch.org/docs/stable/nn.html#module">nn.module</a></p>

<p>참고만 하도록 한다. 좀 많다. 쓰고자 하는 것과 이름이 비슷하다 싶으면 홈페이지를 참조해서 쓰면 된다.</p>

<ol>
  <li>Linear layers
    <ul>
      <li>nn.Linear</li>
      <li>nn.Bilinear</li>
    </ul>
  </li>
  <li>Convolution layers
    <ul>
      <li>nn.Conv1d, nn.Conv2d, nn.Conv3d</li>
      <li>nn.ConvTranspose1d, nn.ConvTranspose2d, nn.ConvTranspose3d</li>
      <li>nn.Unfold, nn.Fold</li>
    </ul>
  </li>
  <li>Pooling layers
    <ul>
      <li>nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d</li>
      <li>nn.MaxUnpool1d, nn.MaxUnpool2d, nn.MaxUnpool3d</li>
      <li>nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d</li>
      <li>nn.FractionalMaxPool2d</li>
      <li>nn.LPPool1d, nn.LPPool2d</li>
      <li>nn.AdaptiveMaxPool1d, nn.AdaptiveMaxPool2d, nn.AdaptiveMaxPool3d</li>
      <li>nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d</li>
    </ul>
  </li>
  <li>Padding layers
    <ul>
      <li>nn.ReflectionPad1d, nn.ReflectionPad2d</li>
      <li>nn.ReplicationPad1d, nn.ReplicationPad2d, nn.ReplicationPad3d</li>
      <li>nn.ZeroPad2d</li>
      <li>nn.ConstantPad1d, nn.ConstantPad2d, nn.ConstantPad3d</li>
    </ul>
  </li>
  <li>Normalization layers
    <ul>
      <li>nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d</li>
      <li>nn.GroupNorm</li>
      <li>nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d</li>
      <li>nn.LayerNorm</li>
      <li>nn.LocalResponseNorm</li>
    </ul>
  </li>
  <li>Recurrent layers
    <ul>
      <li>nn.RNN, nn.RNNCell</li>
      <li>nn.LSTM, nn.LSTMCell</li>
      <li>nn.GRU, nn.GRUCell</li>
    </ul>
  </li>
  <li>Dropout layers
    <ul>
      <li>nn.Dropout, nn.Dropout2d, nn.Dropout3d</li>
      <li>nn.AlphaDropout</li>
    </ul>
  </li>
  <li>Sparse layers
    <ul>
      <li>nn.Embedding</li>
      <li>nn.EmbeddingBag</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="pytorch-activation-function의-종류">Pytorch Activation function의 종류</h2>

<p>참조: <a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">Activation functions</a></p>

<ol>
  <li>Non-linear activations
    <ul>
      <li>nn.ELU, nn.SELU</li>
      <li>nn.Hardshrink, nn.Hardtanh</li>
      <li>nn.LeakyReLU, nn.PReLU, nn.ReLU, nn.ReLU6, nn.RReLU</li>
      <li>nn.Sigmoid, nn.LogSigmoid</li>
      <li>nn.Softplus, nn.Softshrink, nn.Softsign</li>
      <li>nn.Tanh, nn.Tanhshrink</li>
      <li>nn.Threshold</li>
    </ul>
  </li>
  <li>Non-linear activations (other)
    <ul>
      <li>nn.Softmin</li>
      <li>nn.Softmax, nn.Softmax2d, nn.LogSoftmax</li>
      <li>nn.AdaptiveLogSoftmaxWithLoss</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="containers">Containers</h2>

<p>참조: <a href="https://pytorch.org/docs/stable/nn.html#containers">Containers</a></p>

<p>여러 layer들을 하나로 묶는 데 쓰인다.<br />
종류는 다음과 같은 것들이 있는데, Module 설계 시 자주 쓰는 것으로 <strong>nn.Sequential</strong>이 있다.</p>
<ul>
  <li>nn.Module</li>
  <li>nn.Sequential</li>
  <li>nn.ModuleList</li>
  <li>nn.ModuleDict</li>
  <li>nn.ParameterList</li>
  <li>nn.ParameterDict</li>
</ul>

<h3 id="nnsequential">nn.Sequential</h3>

<p>참조: <a href="https://pytorch.org/docs/stable/nn.html#sequential">nn.Sequential</a></p>

<p>이름에서 알 수 있듯 여러 module들을 연속적으로 연결하는 모델이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of using Sequential
</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
          <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
          <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
<span class="s">"""
이 경우 model(x)는 nn.ReLU(nn.Conv2d(20,64,5)(nn.ReLU(nn.Conv2d(1,20,5)(x))))와 같음.
"""</span>

<span class="c1"># Example of using Sequential with OrderedDict
</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
          <span class="p">(</span><span class="s">'conv1'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s">'relu1'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()),</span>
          <span class="p">(</span><span class="s">'conv2'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">)),</span>
          <span class="p">(</span><span class="s">'relu2'</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="p">]))</span>
</code></pre></div></div>

<p>조금 다르지만 비슷한 역할을 할 수 있는 것으로는 nn.ModuleList, nn.ModuleDict가 있다.</p>

<hr />

<h2 id="모델-구성-방법">모델 구성 방법</h2>

<p>크게 6가지 정도의 방법이 있다. <strong>nn</strong> 라이브러리를 잘 써서 직접 만들거나, 함수 또는 클래스로 정의, cfg파일 정의 또는 <a href="https://pytorch.org/docs/stable/torchvision/models.html">torchvision.models</a>에 미리 정의된 모델을 쓰는 방법이 있다.</p>

<h3 id="단순한-방법-1">단순한 방법</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/#define-and-load-model">이전 글</a>에서 썼던 방식이다. <em>매우</em> 단순한 모델을 만들 때는 굳이 nn.Module을 상속하는 클래스를 만들 필요 없이 바로 사용 가능하며, 단순하다는 장점이 있다.</p>

<h3 id="nnsequential을-사용하는-방법">nn.Sequential을 사용하는 방법</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sequential_model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div>

<p>여러 <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#pytorch-layer%EC%9D%98-%EC%A2%85%EB%A5%98">Layer</a>와 <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#pytorch-activation-function%EC%9D%98-%EC%A2%85%EB%A5%98">Activation function</a>들을 조합하여 하나의 sequential model을 만들 수 있다. 역시 상대적으로 복잡하지 않은 모델 중 모델의 구조가 sequential한 모델에만 사용할 수 있다.</p>

<h3 id="함수로-정의하는-방법">함수로 정의하는 방법</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">TwoLayerNet</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">net</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>바로 위의 모델과 완전히 동일한 모델이다. 함수로 선언할 경우 변수에 저장해 놓은 layer들을 재사용하거나, skip-connection을 구현할 수도 있다. 하지만 그 정도로 복잡한 모델은 아래 방법을 쓰는 것이 낫다.</p>

<h3 id="nnmodule을-상속한-클래스를-정의하는-방법">nn.Module을 상속한 클래스를 정의하는 방법</h3>

<p>가장 정석이 되는 방법이다. 또한, 복잡한 모델을 구현하는 데 적합하다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">class</span> <span class="nc">TwoLinearLayerNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TwoLinearLayerNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TwoLinearLayerNet</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>역시 동일한 모델을 구현하였다. 여러분의 코딩 스타일에 따라, <a href="https://pytorch.org/docs/stable/nn.html#relu">ReLU</a> 등의 Activation function을 <code class="highlighter-rouge">forward()</code>에서 바로 정의해서 쓰거나, <code class="highlighter-rouge">__init__()</code>에 정의한 후 forward에서 갖다 쓰는 방법을 선택할 수 있다. 후자의 방법은 아래와 같다.<br />
물론 변수명은 전적으로 여러분의 선택이지만, activation1, relu1 등의 이름을 보통 쓰는 것 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">TwoLinearLayerNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TwoLinearLayerNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">activation1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">activation1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TwoLinearLayerNet</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>두 코딩 스타일의 차이점 중 하나는 import하는 것이 다르다(F.relu와 nn.ReLU는 사실 거의 같다). Activation function 부분에서 <code class="highlighter-rouge">torch.nn.functional</code>은 <code class="highlighter-rouge">torch.nn</code>의 Module에 거의 포함되는데, <code class="highlighter-rouge">forward()</code>에서 정의해서 쓰느냐 마느냐에 따라 다르게 선택하면 되는 정도이다.</p>

<h3 id="cfgconfig를-정의한-후-모델을-생성하는-방법">cfg(config)를 정의한 후 모델을 생성하는 방법</h3>

<p>처음 보면 알아보기 까다로운 방법이지만, <em>매우</em> 복잡한 모델의 경우 <code class="highlighter-rouge">.cfg</code> 파일을 따로 만들어 모델의 구조를 정의하는 방법이 존재한다. 많이 쓰이는 방법은 대략 두 가지 정도인 것 같다.</p>

<p>먼저 PyTorch documentation에서 찾을 수 있는 방법이 있다. 예로는 <a href="https://arxiv.org/abs/1409.1556">VGG</a>를 가져왔다. 코드는 <a href="https://pytorch.org/docs/0.4.0/_modules/torchvision/models/vgg.html">여기</a>에서 찾을 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VGG</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">init_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VGG</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">features</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(...)</span>
        <span class="k">if</span> <span class="n">init_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_initialize_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):...</span>

    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):...</span>

<span class="k">def</span> <span class="nf">make_layers</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">batch_norm</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">in_channels</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="s">'M'</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">batch_norm</span><span class="p">:</span>
                <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">conv2d</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">conv2d</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="n">v</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

<span class="n">cfg</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'A'</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">],</span>
    <span class="s">'B'</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">],</span>
    <span class="s">'D'</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">],</span>
    <span class="s">'E'</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="s">'M'</span><span class="p">],</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">vgg16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="s">"""VGG 16-layer model (configuration "D")"""</span>
    <span class="k">if</span> <span class="n">pretrained</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s">'init_weights'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">VGG</span><span class="p">(</span><span class="n">make_layers</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s">'D'</span><span class="p">]),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pretrained</span><span class="p">:</span>
        <span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_zoo</span><span class="p">.</span><span class="n">load_url</span><span class="p">(</span><span class="n">model_urls</span><span class="p">[</span><span class="s">'vgg16'</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p>여기서는 <code class="highlighter-rouge">.cfg</code> 파일이 사용되지는 않았으나, <code class="highlighter-rouge">cfg</code>라는 변수가 configuration을 담당하고 있다. VGG16 모델을 구성하기 위해 cfg 변수의 해당하는 부분을 읽어 <code class="highlighter-rouge">make_layer</code> 함수를 통해 모델을 구성한다.</p>

<p>더 복잡한 모델은 아예 따로 <code class="highlighter-rouge">.cfg</code> 파일을 빼놓는다. <a href="https://greeksharifa.github.io/paper_review/2018/10/26/YOLOv2/">YOLO</a>의 경우 수백 라인이 넘기도 한다.</p>

<p><code class="highlighter-rouge">.cfg</code> 파일은 대략 <a href="https://github.com/marvis/pytorch-yolo2/blob/master/cfg/yolo.cfg">다음</a>과 같이 생겼다.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[net]
# Testing
batch=1
subdivisions=1
# Training
# batch=64
# subdivisions=8
...

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2
...
</code></pre></div></div>

<p>이를 파싱하는 <a href="https://github.com/marvis/pytorch-yolo2/blob/master/cfg.py">코드</a>도 있어야 한다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">parse_cfg</span><span class="p">(</span><span class="n">cfgfile</span><span class="p">):</span>
    <span class="n">blocks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">fp</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">cfgfile</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>
    <span class="n">block</span> <span class="o">=</span>  <span class="bp">None</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">fp</span><span class="p">.</span><span class="n">readline</span><span class="p">()</span>
    <span class="k">while</span> <span class="n">line</span> <span class="o">!=</span> <span class="s">''</span><span class="p">:</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="n">rstrip</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">line</span> <span class="o">==</span> <span class="s">''</span> <span class="ow">or</span> <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s">'#'</span><span class="p">:</span>
            <span class="n">line</span> <span class="o">=</span> <span class="n">fp</span><span class="p">.</span><span class="n">readline</span><span class="p">()</span>
            <span class="k">continue</span>        
        <span class="k">elif</span> <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s">'['</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">block</span><span class="p">:</span>
                <span class="n">blocks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
            <span class="n">block</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="n">block</span><span class="p">[</span><span class="s">'type'</span><span class="p">]</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="n">lstrip</span><span class="p">(</span><span class="s">'['</span><span class="p">).</span><span class="n">rstrip</span><span class="p">(</span><span class="s">']'</span><span class="p">)</span>
            <span class="c1"># set default value
</span>            <span class="k">if</span> <span class="n">block</span><span class="p">[</span><span class="s">'type'</span><span class="p">]</span> <span class="o">==</span> <span class="s">'convolutional'</span><span class="p">:</span>
                <span class="n">block</span><span class="p">[</span><span class="s">'batch_normalize'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">key</span><span class="p">,</span><span class="n">value</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'='</span><span class="p">)</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s">'type'</span><span class="p">:</span>
                <span class="n">key</span> <span class="o">=</span> <span class="s">'_type'</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">block</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">fp</span><span class="p">.</span><span class="n">readline</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">block</span><span class="p">:</span>
        <span class="n">blocks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
    <span class="n">fp</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">blocks</span>
</code></pre></div></div>

<p>이 방법의 경우 대개 depth가 수십~수백에 이르는 아주 거대한 모델을 구성할 때 사용되는 방법이다. 많은 수의 github 코드들이 이런 방식을 사용하고 있는데, 그러면 그 모델은 굉장히 복잡하게 생겼다는 뜻이 된다.</p>

<h3 id="torchvisionmodels의-모델을-사용하는-방법">torchvision.models의 모델을 사용하는 방법</h3>

<p><a href="https://pytorch.org/docs/stable/torchvision/models.html">torchvision.models</a>에서는 미리 정의되어 있는 모델들을 사용할 수 있다. 이 모델들은 그 구조뿐 아니라 <code class="highlighter-rouge">pretrained=True</code> 인자를 넘김으로써 pretrained weights를 가져올 수도 있다.</p>

<p>2019.02.12 시점에서 사용 가능한 모델 종류는 다음과 같다.</p>
<ul>
  <li>AlexNet</li>
  <li>VGG-11, VGG-13, VGG-16, VGG-19</li>
  <li>VGG-11, VGG-13, VGG-16, VGG-19 (with batch normalization)</li>
  <li>ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152</li>
  <li>SqueezeNet 1.0, SqueezeNet 1.1</li>
  <li>Densenet-121, Densenet-169, Densenet-201, Densenet-161</li>
  <li>Inception v3</li>
</ul>

<p>모델에 따라 train mode와 eval mode가 정해진 경우가 있으므로 이는 주의해서 사용하도록 한다.</p>

<p>모든 pretrained model을 쓸 때 이미지 데이터는 [3, W, H] 형식이어야 하고, W, H는 224 이상이어야 한다. 또 아래 코드처럼 정규화된 이미지 데이터로 학습된 것이기 때문에, 이 모델들을 사용할 때에는 데이터셋을 이와 같이 정규화시켜주어야 한다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                     <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
</code></pre></div></div>

<p>사용법은 대략 다음과 같다. 사실 이게 거의 끝이고, 나머지는 다른 일반 모델처럼 사용하면 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>

<span class="c1"># model load
</span><span class="n">alexnet</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">alexnet</span><span class="p">()</span>
<span class="n">vgg16</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">vgg16</span><span class="p">()</span>
<span class="n">vgg16_bn</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">vgg16_bn</span><span class="p">()</span>
<span class="n">resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">resnet18</span><span class="p">()</span>
<span class="n">squeezenet</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">squeezenet1_0</span><span class="p">()</span>
<span class="n">densenet</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">densenet161</span><span class="p">()</span>
<span class="n">inception</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">inception_v3</span><span class="p">()</span>

<span class="c1"># pretrained model load
</span><span class="n">resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">vgg16</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">vgg16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="p">...</span>
</code></pre></div></div>

<hr />

<h1 id="set-loss-functioncreterion-and-optimizer">Set Loss function(creterion) and Optimizer</h1>

<h2 id="pytorch-loss-function의-종류">Pytorch Loss function의 종류</h2>

<p>참조: <a href="https://pytorch.org/docs/stable/nn.html#loss-functions">Loss functions</a></p>

<p>Loss function은 모델이 추측한 결과(prediction 또는 output)과 실제 정답(label 또는 y 등)의 <em>loss</em>를 계산한다. 이는 loss function을 어떤 것을 쓰느냐에 따라 달라진다. 예를 들어 regression model에서 MSE(Mean Squared Error)를 쓸 경우 평균 제곱오차를 계산한다.</p>

<p>사용법은 다른 함수들도 아래와 똑같다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="n">criterion</span>  <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">12</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">52</span><span class="p">])</span> <span class="c1"># 예측값
</span><span class="n">target</span>     <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span> <span class="c1"># 정답
</span><span class="n">loss</span>       <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="c1"># tensor(2.)
# loss = (2^2 + 1^2 + 0^2 + 1^2 + 2^2) / 5 = 2
</span>
<span class="n">criterion_reduction_none</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion_reduction_none</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="c1"># tensor([4., 1., 0., 1., 4.])
</span></code></pre></div></div>

<p>여러 코드들을 살펴보면, loss function을 정의할 때는 보통 <code class="highlighter-rouge">creterion</code>, <code class="highlighter-rouge">loss_fn</code>, <code class="highlighter-rouge">loss_function</code>등의 이름을 사용하니 참고하자.</p>

<p>홈페이지를 참조하면 각 함수별 설명에 ‘Creates a criterion that measures…‘라 설명이 되어 있다. 위의 예시를 보면 알겠지만 해당 함수들이 당장 loss를 계산하는 것이 아니라 loss를 계산하는 기준을 정의한다는 뜻이다.<br />
또 많은 함수들은 <code class="highlighter-rouge">reduce</code>와 <code class="highlighter-rouge">size_average</code> argument를 갖는다. loss를 계산하여 평균을 내는 것이 아니라 각 원소별로 따로 계산할 수 있게 해 준다. 그러나 2019.02.16 기준으로 다음과 비슷한 경고가 뜬다.</p>

<blockquote>
  <p>reduce args will be deprecated, please use reduction=’none’ instead.</p>
</blockquote>

<p>따라서 <code class="highlighter-rouge">reduction</code> argument를 쓰도록 하자. 지정할 수 있는 종류는 ‘none’ | ‘mean’ | ‘sum’ 세 가지이다. 기본값은 mean으로 되어 있다.</p>

<ul>
  <li><strong>nn.L1Loss</strong>: 각 원소별 차이의 절댓값을 계산한다.
<img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/02.PNG" alt="L1" /></li>
  <li><strong>nn.MSELoss</strong>: Mean Squared Error(평균제곱오차) 또는 squared L2 norm을 계산한다.
<img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/03.PNG" alt="MSE" /></li>
  <li><strong>nn.CrossEntropyLoss</strong>: Cross Entropy Loss를 계산한다. nn.LogSoftmax() and nn.NLLLoss()를 포함한다. weight argument를 지정할 수 있다.
<img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/04.PNG" alt="CE" /></li>
  <li><strong>nn.CTCLoss</strong>: Connectionist Temporal Classification loss를 계산한다.</li>
  <li><strong>nn.NLLLoss</strong>: Negative log likelihood loss를 계산한다.
<img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/05.PNG" alt="NLL" /></li>
  <li><strong>nn.PoissonNLLLoss</strong>: target이 poission 분포를 가진 경우 Negative log likelihood loss를 계산한다.
<img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/06.PNG" alt="PNLL" /></li>
  <li><strong>nn.KLDivLoss</strong>: Kullback-Leibler divergence Loss를 계산한다.
<img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/07.PNG" alt="KLDiv" /></li>
  <li><strong>nn.BCELoss</strong>: Binary Cross Entropy를 계산한다. 
<img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/08.PNG" alt="BCE" /></li>
  <li><strong>nn.BCEWithLogitsLoss</strong>: Sigmoid 레이어와 BCELoss를 하나로 합친 것인데, 홈페이지의 설명에 따르면 두 개를 따로 쓰는 것보다 이 함수를 쓰는 것이 조금 더 수치 안정성을 가진다고 한다.
<img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/09.PNG" alt="BCE" /></li>
  <li>이외에 <strong>MarginRankingLoss, HingeEmbeddingLoss, MultiLabelMarginLoss, SmoothL1Loss, SoftMarginLoss, MultiLabelSoftMarginLoss, CosineEmbeddingLoss, MultiMarginLoss, TripletMarginLoss</strong>를 계산하는 함수들이 있다. 필요하면 찾아보자.</li>
</ul>

<h2 id="pytorch-optimizer의-종류">Pytorch Optimizer의 종류</h2>

<p>참조: <a href="https://pytorch.org/docs/stable/optim.html">torch.optim</a></p>

<p><a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#nnmodule-%EB%82%B4%EC%9E%A5-%ED%95%A8%EC%88%98">여기</a>에도 간략하게 언급했었지만, GPU CUDA를 사용할 계획이라면 optimizer를 정의하기 전에 미리 해놓아야 한다(<code class="highlighter-rouge">model.cuda()</code>). 공식 홈페이지에 따르면,</p>

<blockquote>
  <p>If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.
In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.</p>
</blockquote>

<p>이유를 설명하자면</p>

<ol>
  <li>optimizer는 argument로 model의 parameter를 입력받는다.</li>
  <li><code class="highlighter-rouge">.cuda()</code>를 쓰면 모델의 parameter가 cpu 대신 gpu에 올라가는 것이므로 다른 object가 된다.</li>
  <li>따라서 optimizer에 model parameter의 위치를 전달한 후 <code class="highlighter-rouge">.cuda()</code>를 실행하면, 학습시켜야 할 parameter는 GPU에 올라가 있는데 optimizer는 cpu에 올라간 엉뚱한 parameter 위치를 참조하고 있는 것이 된다.</li>
</ol>

<p>그러니 순서를 지키자.</p>

<p>optimizer 정의는 다음과 같이 할 수 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">var1</span><span class="p">,</span> <span class="n">var2</span><span class="p">],</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">)</span>
</code></pre></div></div>

<p>optimizer에 대해 알아 두어야 할 것이 조금 있다.</p>
<ol>
  <li>optimizer는 <code class="highlighter-rouge">step()</code> method를 통해 argument로 전달받은 parameter를 업데이트한다.</li>
  <li>모델의 parameter별로(per-parameter) 다른 기준(learning rate 등)을 적용시킬 수 있다. <a href="https://pytorch.org/docs/stable/optim.html#per-parameter-options">참고</a></li>
  <li><code class="highlighter-rouge">torch.optim.Optimizer(params, defaults)</code>는 모든 optimizer의 base class이다.</li>
  <li><code class="highlighter-rouge">nn.Module</code>과 같이 <code class="highlighter-rouge">state_dict()</code>와 <code class="highlighter-rouge">load_state_dict()</code>를 지원하여 optimizer의 상태를 저장하고 불러올 수 있다.</li>
  <li><code class="highlighter-rouge">zero_grad()</code> method는 optimizer에 연결된 parameter들의 gradient를 0으로 만든다.</li>
  <li><code class="highlighter-rouge">torch.optim.lr_scheduler</code>는 epoch에 따라 learning rate를 조절할 수 있다.</li>
</ol>

<p><strong>Optimizer의 종류:</strong></p>
<ul>
  <li>optim.Adadelta, optim.Adagrad, optim.Adam, optim.SparseAdam, optim.Adamax</li>
  <li>optim.ASGD, <em>optim.LBFGS</em></li>
  <li>optim.RMSprop, optim.Rprop</li>
  <li>optim.SGD</li>
</ul>

<p>LBFGS는 per-parameter 옵션이 지원되지 않는다. 또한 memory를 다른 optimizer에 비해 많이 잡아먹는다고 한다.</p>

<h2 id="pytorch-lrlearning-rate-scheduler의-종류">Pytorch LR(Learning Rate) Scheduler의 종류</h2>

<p>LR(Learning Rate) Scheduler는 미리 지정한 횟수의 epoch이 지날 때마다 lr을 감소(decay)시켜준다.<br />
이는 학습 초기에는 빠르게 학습을 진행시키다가 minimum 근처에 다다른 것 같으면 lr을 줄여서 더 최적점을 잘 찾아갈 수 있게 해주는 것이다.</p>

<p>종류는 여러 개가 있는데, 마음에 드는 것을 선택하면 된다. 아래쪽에 어떻게 lr이 변화하는지 그림을 그려 놓았다.</p>

<p><strong>lr Scheduler의 종류:</strong></p>
<ul>
  <li>optim.lr_scheduler.LambdaLR: lambda 함수를 하나 받아 그 함수의 결과를 lr로 설정한다.</li>
  <li>optim.lr_scheduler.StepLR: 특정 step마다 lr을 gamma 비율만큼 감소시킨다.</li>
  <li>optim.lr_scheduler.MultiStepLR: StepLR과 비슷한데 매 step마다가 아닌 지정된 epoch에만 gamma 비율로 감소시킨다.</li>
  <li>optim.lr_scheduler.ExponentialLR: lr을 지수함수적으로 감소시킨다.</li>
  <li>optim.lr_scheduler.CosineAnnealingLR: lr을 cosine 함수의 형태처럼 변화시킨다. lr이 커졌다가 작아졌다가 한다.</li>
  <li><strong>optim.lr_scheduler.ReduceLROnPlateau</strong>: 이 scheduler는 다른 것들과는 달리 학습이 잘 되고 있는지 아닌지에 따라 동적으로 lr을 변화시킬 수 있다. 보통 validation set의 loss를 인자로 주어서 사전에 지정한 epoch동안 loss가 줄어들지 않으면 lr을 감소시키는 방식이다.</li>
</ul>

<p>각 scheduler는 공통적으로 <code class="highlighter-rouge">last_epoch</code> argument를 갖는다. Default value로 -1을 가지며, 이는 초기 lr을 optimizer에서 지정된 lr로 설정할 수 있도록 한다.</p>

<center><img src="/public/img/PyTorch/2018-11-10-pytorch-usage-03-How-to-Use-PyTorch/10.PNG" width="100%" alt="10_Scheduler" /></center>

<p>코드는 아래와 같이 작성하였다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>


<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>


<span class="n">scheduler_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                         <span class="n">mode</span><span class="o">=</span><span class="s">'min'</span><span class="p">,</span>
                                         <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                                         <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="p">),</span> <span class="c1"># 이외에도 인자가 많다. 찾아보자.
</span>    <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                <span class="n">lr_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)),</span>
    <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                              <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                              <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                   <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">28</span><span class="p">],</span>
                                   <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                     <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                         <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                         <span class="n">eta_min</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">reObj</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="s">r'&lt;torch\.optim\.lr_scheduler\.(.+) object.*&gt;'</span><span class="p">)</span>


<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">scheduler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">scheduler_list</span><span class="p">):</span>
    <span class="n">scheduler_name</span> <span class="o">=</span> <span class="n">reObj</span><span class="p">.</span><span class="n">match</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)).</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">scheduler_name</span><span class="p">)</span>

    <span class="n">lr_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">str</span><span class="p">(</span><span class="n">scheduler_name</span><span class="p">)</span> <span class="o">==</span> <span class="s">'ReduceLROnPlateau'</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">lr</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">'lr'</span><span class="p">]</span>
        <span class="c1"># print('epoch: {:3d}, lr={:.6f}'.format(epoch, lr))
</span>        <span class="n">lr_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">scheduler_name</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr_list</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># plt.savefig('scheduler')
</span></code></pre></div></div>

<p>조금 더 자세한 설명은 <a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">홈페이지</a>를 참조하자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="c1"># 1.0은 보통 너무 크다. 하지만 예시이므로 1을 주었다.
</span>
<span class="c1"># Learning Rate가 scheduler에 따라 어떻게 변하는지 보려면 이곳을 바꾸면 된다.
</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                                        <span class="n">lr_lambda</span><span class="o">=</span><span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mf">0.95</span> <span class="o">**</span> <span class="n">epoch</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">param_group</span><span class="p">[</span><span class="s">'lr'</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'epoch: {:3d}, lr={:.6f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">lr</span><span class="p">))</span>
    <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>결과:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch:   1, lr=1.000000
epoch:   2, lr=1.000000
epoch:   3, lr=0.950000
epoch:   4, lr=0.902500
epoch:   5, lr=0.857375
epoch:   6, lr=0.814506
epoch:   7, lr=0.773781
epoch:   8, lr=0.735092
epoch:   9, lr=0.698337
epoch:  10, lr=0.663420
...
</code></pre></div></div>

<hr />

<h1 id="train-model">Train Model</h1>

<p>일반적인 machine learning의 학습 방법은 다음과 같다. 입력은 input, 모델의 출력은 output, 정답은 target이라고 하자.</p>

<ol>
  <li>model structure, loss function, optimizer 등을 정한다.</li>
  <li><strong>forward-propagation</strong>: input을 모델에 통과시켜 output을 계산한다.</li>
  <li>loss function으로 output과 target 간 <strong>loss</strong>를 계산한다.</li>
  <li><strong>back-propagation</strong>: loss와 chain rule을 활용하여 모델의 각 레이어에서 gradient($\Delta w$)를 계산한다.</li>
  <li><strong>update</strong>: $ w \leftarrow w - \alpha\Delta w $식에 의해 모델의 parameter를 update한다.</li>
</ol>

<p>Pytorch의 학습 방법은 다음과 같다.</p>
<ol>
  <li>model structure, loss function, optimizer 등을 정한다.</li>
  <li><code class="highlighter-rouge">optimizer.zero_grad()</code>: 이전 epoch에서 계산되어 있는 parameter의 gradient를 0으로 초기화한다.</li>
  <li><code class="highlighter-rouge">output = model(input)</code>: input을 모델에 통과시켜 output을 계산한다.</li>
  <li><code class="highlighter-rouge">loss = loss_fn(output, target)</code>: output과 target 간 <strong>loss</strong>를 계산한다.</li>
  <li><code class="highlighter-rouge">loss.backward()</code>: loss와 chain rule을 활용하여 모델의 각 레이어에서 gradient($\Delta w$)를 계산한다.</li>
  <li><code class="highlighter-rouge">optimizer.step()</code>: $w \leftarrow w - \alpha\Delta w$식에 의해 모델의 parameter를 update한다.</li>
</ol>

<p>거의 일대일 대응되지만 다른 점이 하나 있다.</p>
<ul>
  <li><code class="highlighter-rouge">optimizer.zero_grad()</code>: Pytorch는 gradient를 <code class="highlighter-rouge">loss.backward()</code>를 통해 계산하지만, 이 함수는 이전 gradient를 덮어쓴 뒤 새로 계산하는 것이 아니라, 이전 gradient에 <strong><em>누적하여</em></strong> 계산한다.
    <ul>
      <li><em>귀찮은데?</em> 라고 생각할 수는 있다. 그러나 이러한 누적 계산 방식은 RNN 모델을 구현할 때는 오히려 훨씬 편하게 코드를 작성할 수 있도록 도와준다.</li>
      <li>그러니 gradient가 누적될 필요 없는 모델에서는 model에 input를 통과시키기 전 <code class="highlighter-rouge">optimizer.zero_grad()</code>를 한번 호출해 주기만 하면 된다고 생각하면 끝이다.</li>
    </ul>
  </li>
</ul>

<p>Pytorch가 대체 어떻게 <code class="highlighter-rouge">loss.backward()</code> 단 한번에 gradient를 자동 계산하는지에 대한 설명도 하면,</p>

<ul>
  <li>모든 Pytorch Tensor는 <code class="highlighter-rouge">requires_grad</code> argument를 가진다. 일반적으로 생성하는 Tensor는 기본적으로 해당 argument 값이 <code class="highlighter-rouge">False</code>이며, 따로 <code class="highlighter-rouge">True</code>로 설정해 주면 gradient를 계산해 주어야 한다. <code class="highlighter-rouge">nn.Linear</code> 등의 module은 생성할 때 기본적으로 <code class="highlighter-rouge">requires_grad=True</code>이기 때문에, 일반적으로 모델의 parameter는 gradient를 계산하게 된다.
    <ul>
      <li><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/#import">참고(3번 항목)</a>: Pytorch 0.4.0 버전 이전에는 <code class="highlighter-rouge">Variable</code> class가 해당 역할을 수행하였지만, deprecated되었다.</li>
    </ul>
  </li>
  <li>마지막 레이어만 원하는 것으로 바꿔서 그 레이어만 학습을 수행하는 형태의 transfer learning을 <code class="highlighter-rouge">requires_grad</code>를 이용해 손쉽게 구현할 수 있다. 이외에도 특정 레이어만 gradient를 계산하지 않게 하는 데에도 쓸 수 있다. 아래 예시는 512개의 class 대신 100개의 class를 구별하고자 할 때 resnet18을 기반으로 transfer learning을 수행하는 방식이다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
<span class="c1"># Replace the last fully-connected layer
# Parameters of newly constructed modules have requires_grad=True by default
</span><span class="n">model</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Optimize only the classifier
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">fc</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="highlighter-rouge">requires_grad=True</code>인 Tensor로부터 연산을 통해 생성된 Tensor도 <code class="highlighter-rouge">requires_grad=True</code>이다.</li>
  <li><code class="highlighter-rouge">with torch.no_grad():</code> 범위 안에서는 gradient 계산을 하지 않는다.</li>
  <li><code class="highlighter-rouge">with torch.no_grad():</code> 안에서 선언된 <code class="highlighter-rouge">with torch.enable_grad():</code> 범위 안에서는 다시 gradient 계산을 한다. 이 두 가지 기능을 통해 국지적으로 gradient 계산을 수행하거나 수행하지 않을 수 있다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'x.requires_grad:'</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y.requires_grad:'</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="n">z</span> <span class="o">=</span> <span class="mi">172</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">3</span>
<span class="k">print</span><span class="p">(</span><span class="s">'z.requires_grad:'</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'z.requires_grad:'</span><span class="p">,</span> <span class="p">(</span><span class="n">z</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">enable_grad</span><span class="p">():</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'z.requires_grad:'</span><span class="p">,</span> <span class="p">(</span><span class="n">z</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'z.grad_fn:'</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">grad_fn</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'x:'</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s">'</span><span class="se">\n</span><span class="s">y:'</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">'</span><span class="se">\n</span><span class="s">z:'</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

<span class="n">z</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y.grad:'</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'z.grad:'</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x.requires_grad: False
y.requires_grad: True
z.requires_grad: True
z.requires_grad: False
z.requires_grad: True
z.grad_fn: &lt;AddBackward0 object at 0x0000028634614780&gt;
x: tensor([1.4013e-45]) 
y: tensor([1.], requires_grad=True) 
z: tensor([175.], grad_fn=&lt;AddBackward0&gt;)
y.grad: tensor([172.])
z.grad: None
</code></pre></div></div>

<p>튜토리얼이 조금 더 궁금하다면 <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">여기</a>를 참고해도 좋다.</p>

<p>학습할 때 알아두면 괜찮은 것들을 대략 정리해보았다. 어떤 식으로 학습하는 것이 좋은지(learning rate 선택 기준 등)는 양이 너무 방대하기에 여기에는 적지 않는다.</p>

<h2 id="cuda-use-gpu">CUDA: use GPU</h2>

<p><strong><a href="https://pytorch.org/docs/stable/cuda.html">CUDA</a></strong></p>

<ul>
  <li><code class="highlighter-rouge">torch.cuda.is_available()</code>: 학습을 시킬 때는 GPU를 많이 사용한다. GPU가 사용가능한지 알 수 있다.</li>
  <li><code class="highlighter-rouge">torch.cuda.device(device)</code>: 어느 device(GPU나 CPU)를 쓸 지 선택한다.</li>
  <li><code class="highlighter-rouge">torch.cuda.device_count()</code>: 현재 선택된 device의 수를 반환한다.</li>
  <li><code class="highlighter-rouge">torch.cuda.init()</code>: C API를 쓰는 경우 명시적으로 호출해야 한다.</li>
  <li><code class="highlighter-rouge">torch.cuda.set_device(device)</code>: 현재 device를 설정한다.</li>
  <li><code class="highlighter-rouge">torch.cuda.manual_seed(seed)</code>: 랜덤 숫자를 생성할 시드를 정한다. multi-gpu 환경에서는 <code class="highlighter-rouge">manual_seed_all</code> 함수를 사용한다.</li>
  <li><code class="highlighter-rouge">torch.cuda.empty_cache()</code>: 사용되지 않는 cache를 release하나, 가용 메모리를 늘려 주지는 않는다.</li>
</ul>

<p>간단한 학습 과정은 다음 구조를 따른다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 변수명으로 input을 사용하는 것은 비추천. python 내장 함수 이름이다.
</span><span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span> 
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># RNN에서는 생략될 수 있음
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<hr />

<h1 id="visualize-and-save-results">Visualize and save results</h1>

<h2 id="visualization-library">Visualization Library</h2>

<p>Visualization은 이 글에서 설명하지 않겠다. 기본적으로 python의 그래프 패키지인 <code class="highlighter-rouge">matplotlib</code>을 많이 쓰며, <code class="highlighter-rouge">graphviz</code>, <code class="highlighter-rouge">seaborn</code> 등의 다른 라이브러리도 잘 보이는 편이다.</p>

<h2 id="save--load-model">Save &amp; Load Model</h2>

<p>모델을 저장하는 방법은 여러 가지가 있지만, pytorch를 사용할 때는 다음 방법이 가장 권장된다. 아주 유연하고 또 간단하기 때문이다.</p>

<p><strong>Save</strong>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Load</strong>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="c1"># model.eval() # 테스트 시
</span>
<span class="c1"># 참고로 model.load_state_dict(PATH)와 같이 쓸 수는 없다.
</span></code></pre></div></div>

<p>epoch별로 checkpoint를 쓰면서 저장할 때는 다음과 같이 혹은 비슷하게 쓰면 좋다. checkpoint를 쓸 때는 단순히 모델의 parameter뿐만 아니라 epoch, loss, optimizer 등을 저장할 필요가 있다.</p>

<p><strong>Save</strong>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">({</span>
            <span class="s">'epoch'</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
            <span class="s">'model_state_dict'</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s">'optimizer_state_dict'</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
            <span class="p">...</span>
            <span class="p">},</span> <span class="n">PATH</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Load</strong>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">TheOptimizerClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'model_state_dict'</span><span class="p">])</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'optimizer_state_dict'</span><span class="p">])</span>
<span class="n">epoch</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'epoch'</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span>
<span class="c1"># model.train() or model.eval()
</span></code></pre></div></div>

<p>일반적으로 저장한 모델 파일명은 <code class="highlighter-rouge">.pt</code>나 <code class="highlighter-rouge">.pth</code> 확장자를 쓴다. 모델을 포함하여 여러 가지를 같이 저장할 때는 <code class="highlighter-rouge">.tar</code> 확장자를 자주 쓰는 편이다.</p>

<p>모델을 불러오고 나서 계속 학습시킬 것이라면 <code class="highlighter-rouge">model.train()</code>, 테스트를 할 것이라면 <code class="highlighter-rouge">model.eval()</code>으로 모드를 설정하도록 한다. 이유는 이 글에 설명이 있다.</p>

<p>모델이 여러 개라면</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">({</span>
            <span class="s">'modelA_state_dict'</span><span class="p">:</span> <span class="n">modelA</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s">'modelB_state_dict'</span><span class="p">:</span> <span class="n">modelB</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="p">...</span>
</code></pre></div></div>
<p>처럼 쓰면 그만이다.</p>

<p>구조가 조금 다른 모델에다가 parameter를 load하고 싶을 경우 load할 때 다음처럼 쓴다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">),</span> <span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">load_state_dict</code> 함수는 기본적으로 <code class="highlighter-rouge">strict=True</code> 옵션을 갖고 있으며, 이는 불러올 모델과 저장된 모델의 레이어의 개수와 이름 등이 <em>같아야만</em> 오류 없이 불러온다.</p>

<p>따라서 transfer learning이나, 복잡한 모델을 새로 학습시키고 싶을 때 모델의 일부라도 parameter를 불러오고 싶다면 <code class="highlighter-rouge">strict=False</code> argument를 설정하면 된다.<br />
이는 레이어들이 정확히 일치하지 않아도 매칭이 되는 레이어가 일부라도 있다면 그 레이어들에 한해서 parameter를 load한다.<br />
또 parameter 개수는 같지만 이름은 다른 레이어에 parameter를 불러오고 싶을 때는, <code class="highlighter-rouge">state_dict</code>는 딕셔너리이기 때문에 그냥 해당 딕셔너리의 이름만 바꿔서 load하면 그만이다.</p>

<p><code class="highlighter-rouge">pickle</code> 또는 <code class="highlighter-rouge">torch.save</code>를 통해 model 전체를 통째로 저장하는 방법은 간편하기는 하지만 이후 불러올 때는 해당 모델과 완전히 똑같이 생긴 모델에만 사용 가능하기 때문에 확장성과 재사용성이 떨어진다.<br />
layer 이름과 parameter를 mapping하여 저장하는 <code class="highlighter-rouge">state_dict</code>를 쓰는 것이 transfer learinng을 쉽게 할 수 있는 등 범용성이 더 좋다.</p>

<p>device를 바꿔서 저장하고 싶다면, <code class="highlighter-rouge">load_state_dict</code>에서 <code class="highlighter-rouge">map_location</code> argument를 설정하거나, <code class="highlighter-rouge">model.to(device)</code> 함수를 사용하면 된다. 자세한 것은 <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices">홈페이지</a>를 참조한다. GPU를 사용할 때 바꿔줘야 하는 부분은 <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#nnmodule-%EB%82%B4%EC%9E%A5-%ED%95%A8%EC%88%98">여기</a>의 cuda 부분을 참고한다.</p>

<h3 id="torchsave--torchload">torch.save &amp; torch.load</h3>

<p>내부적으로 <code class="highlighter-rouge">pickle</code>을 사용하며, 따라서 모델뿐 아니라 일반 tensor, 기타 다른 모든 python 객체를 저장할 수 있다.</p>

<h3 id="nnmodulestate_dict--nnmoduleload_state_dict">nn.Module.state_dict &amp; nn.Module.load_state_dict</h3>

<p>우선 <code class="highlighter-rouge">state_dict</code>는 간단히 말해 모델의 상태를 딕셔너리 형태로 표현하는 것이다. 그러면 모델의 상태는 어떻게 정의되는가?<br />
<code class="highlighter-rouge">state_dict</code>로 저장되는 모델의 상태는 learnable parameters이며, <code class="highlighter-rouge">state_dict</code>는 <code class="highlighter-rouge">{레이어 이름: parameter tensor}</code>의 형태를 갖는 딕셔너리이다.<br />
딱 그뿐이다. 간단하지 않은가?</p>

<p>Optimizer도 <code class="highlighter-rouge">state_dict</code>를 갖고 있는데, 이 경우는 사용된 hyperparameter 등의 상태가 저장된다.</p>

<p><a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">공식 홈페이지</a>의 예시를 일부 가져오면 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 모델이 이렇게 생겼으면, 
</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>

<span class="c1"># 이 코드에 의해
</span><span class="k">for</span> <span class="n">param_tensor</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">param_tensor</span><span class="p">,</span> <span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="n">param_tensor</span><span class="p">].</span><span class="n">size</span><span class="p">())</span>

<span class="c1"># 이렇게 출력된다.
</span><span class="n">conv1</span><span class="p">.</span><span class="n">weight</span>     <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">conv1</span><span class="p">.</span><span class="n">bias</span>       <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">])</span>
<span class="n">conv2</span><span class="p">.</span><span class="n">weight</span>     <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">conv2</span><span class="p">.</span><span class="n">bias</span>       <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">16</span><span class="p">])</span>
<span class="n">fc1</span><span class="p">.</span><span class="n">weight</span>       <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">120</span><span class="p">,</span> <span class="mi">400</span><span class="p">])</span>
<span class="n">fc1</span><span class="p">.</span><span class="n">bias</span>         <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">120</span><span class="p">])</span>

<span class="c1"># optimizer는
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="k">for</span> <span class="n">var_name</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">()[</span><span class="n">var_name</span><span class="p">])</span>

<span class="c1"># 이렇다.
</span><span class="n">state</span>    <span class="p">{}</span>
<span class="n">param_groups</span>     <span class="p">[{</span><span class="s">'lr'</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span> <span class="s">'momentum'</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s">'dampening'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'weight_decay'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> 
<span class="s">'nesterov'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span> <span class="s">'params'</span><span class="p">:</span> <span class="p">[</span><span class="mi">4675713712</span><span class="p">,</span> <span class="mi">4675713784</span><span class="p">,</span> <span class="p">...,</span> <span class="mi">4675714720</span><span class="p">]}]</span>
</code></pre></div></div>

<hr />

<h1 id="q--a">Q &amp; A</h1>

<ul>
  <li><code class="highlighter-rouge">model.train()</code>과 <code class="highlighter-rouge">model.eval()</code>은 모델이 학습 모드인지, 테스트 모드인지를 정하는 것이다. 이는 dropout이나 batchnorm이 있는 모델의 경우 학습할 때와 테스트할 때 모델이 달라지기 때문에 세팅하는 것이다(또한 필수이다). <code class="highlighter-rouge">torch.no_grad()</code>는 (대개 일시적으로) 해당 범위 안에서 gradient 계산을 중지시킴으로써 메모리 사용량을 줄이고 계산 속도를 빨리 하는 것이다. <a href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615">참고</a></li>
  <li><code class="highlighter-rouge">optimizer.zero_grad()</code>를 사용하는 이유. <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#train-model">참고</a></li>
  <li>Pytorch 코드들 중에는 <code class="highlighter-rouge">torch.autograd.Variable</code>을 사용한 경우가 많다. Pytorch 0.4.0 버전 이후로는 Tensor 클래스에 통합되어 더 이상 쓸 필요가 없다. <a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/#import">참고(3번 항목)</a></li>
  <li>역시 Pytorch 코드들 중에는 loss를 tensor가 아닌 그 값을 가져올 때 <code class="highlighter-rouge">loss.data[0]</code> 등의 표현식은 에러를 뱉는 경우가 많다. 이는 0.4 이후 버전의 PyTorch에서는 <code class="highlighter-rouge">loss.item()</code>으로 그 값을 가져오도록 변경되었기 때문이다.
    <ul>
      <li>Pytorch의 loss는 이전에는 <code class="highlighter-rouge">Variable</code>에 할당된 <code class="highlighter-rouge">size=(1, )</code>의 tensor였지만 이제는 scalar 형태이다.</li>
    </ul>
  </li>
</ul>

<p>댓글로 문의하시면 확인 후 포스팅에 추가 가능합니다.</p>


    </article>
    <div class="post-more">
      
      <a href="/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/machine_learning/2018/11/06/TripleB/">
        Bootstrap, Bagging, Boosting
      </a>
    </h1>

    <span class="post-date">06 Nov 2018</span>
     |
    
    <a href="/blog/tags/#machine-learning" class="post-tag">Machine Learning</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <h2 id="bootstrap-부트스트랩의-개념">Bootstrap: 부트스트랩의 개념</h2>
<blockquote>
  <p>통계학에서의 부트스트랩과 기계학습에서의 부트스트랩은 그 의미가 다른 점도 있지만 본질적으로는 같다고 할 수 있다. 통계학적으로는 정확한 분포를 모르는 데이터의 통계치의 분포를 알아내기 위하여 Random Sampling을 하는 경우를 말하며, 종종 측정된 샘플이 부족한 경우에도 사용된다.<br />
기계학습에서는 기본적으로 Random Sampling을 통해 데이터의 수를 늘리는 것을 말한다.</p>
</blockquote>

<hr />

<h2 id="decision-tree-의사-결정-나무">Decision Tree: 의사 결정 나무</h2>
<p>내용을 입력합시당</p>

<hr />

<h2 id="bagging-배깅">Bagging: 배깅</h2>
<p>Bagging은 Bootstrap Aggregatint의 줄임말이다. 특별히 부트스트랩이 over-fitting을 줄이는 데에 사용될 때를 말한다. 주어진 데이터에 대해 여러 번의 Random Sampling을 통해 Training Data를 추출하고 (여러 개의 부트스트랩을 생성), 독립된 모델로서 각각의 자료를 학습시키고 이를 앙상블로서 결합하여 최종적으로 하나의 예측 모형을 산출하는 방법이라고 할 수 있다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/01.jpg" width="60%" /></center>

<p>예를 들어 단일 Decision Tree는 변동성이 매우 크다. 이러한 단일 Decision Tree를 여러 개 결합하여 모델을 형성한다면 과적합을 방지할 수도 있고 안정된 결과를 산출할 수 있을 것이다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/02.jpg" width="50%" /></center>

<p>대표적인 예가 Random Forest이며, Sample의 예측변수들의 결합 시 Target Variable이 연속형일 때는 평균을, 범주형일 때는 다중 투표를 사용한다.</p>

<hr />

<h2 id="boosting-부스팅">Boosting: 부스팅</h2>
<p>배깅이 독립적으로 모델을 학습시킨다면, 부스팅은 이전의 잘못을 파악하고 이를 이용하여 다음 번에는 더 나은 모델을 만들어 내자는 목표를 추구하면서 학습하는 방법이다. 분류 문제로 예를 들면, 잘못 분류된 개체들을 다음 번에는 더 잘 분류하고 싶은 것이 당연하다. 부스팅은 잘못 본류된 개체들에 집중하여 새로운 분류 규칙을 만드는 것을 반복하는 방법이며, 이는 결국 약한 예측모형들을 결합하여 강한 예측모형을 만드는 과정으로 서술할 수 있다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/03.jpg" width="20%" /></center>

<hr />

<h2 id="xgboost-이론">XGBoost 이론</h2>
<p>XGBoost는 Extreme Gradient Boosting의 줄임말로, 2014년에 등장하여 이후 지금까지 널리 쓰이고 있는 강력한 기계학습 알고리즘이다.<br />
본 글에서는 XGBoost의 창시자인 Tianqi Chen과 Carlos Guestrin이 2016년 publish한<br />
[XGBoost: A Scalable Tree Boosting System] 논문과 Chen의 관련 강연을 기초로 하여<br />
알고리즘에 대해 설명하도록 하겠다.</p>

<p>알고리즘에 대한 설명이 끝난 이후에는 XGBoost Python의 메서드와 패키지의 주요 기능에 대해 알아본 뒤, Hyperparameter들을 튜닝하는 법에 대해 설명할 것이다.</p>

<p><strong>XGBoost의 강점</strong></p>
<ol>
  <li>Regularization: 복잡한 모델에 대하여 페널티를 주는 Regularization 항이 있기 때문에 과적합을 방지할 수 있다.</li>
  <li>Handling Sparse Data: XGB는 원핫인코딩이나 결측값 등에 의해 발생한 Sparse Data(0이 많은 데이터) 또한 무리 없이 다룰 수 있다.</li>
  <li>Weighted Quantile Sketch: 가중치가 부여된 데이터 또한 Weighted Percentile Sketch 알고리즘을 통해 다룰 수 있다.</li>
  <li>Block Structure for parallel learning: 데이터는 정렬되어 in-memory units (blocks)에 저장된다. 이 데이터는 이후에 계속 반복적으로 재사용이 가능하기 때문에 다시 계산할 필요가 없다. 이를 통해 빠르게 Split Point를 찾아낼 수 있고 Column Sub-sampling을 진행할 수 있다.</li>
  <li>Cache Awarness: 하드웨어를 최적으로 사용하도록 고안되었다.</li>
  <li>Out-of-core computing: 거대한 데이터를 다룰 때 디스크 공간을 최적화하고 사용 가능 범위를 최대화한다.</li>
</ol>

<p><strong>[1] Regularized Learning Objective</strong><br />
n개의 example과 m개의 feature(변수)로 이루어진 데이터셋이 있다고 할 때,</p>

<script type="math/tex; mode=display">D = {(x_i, y_i)} (|D| = n, x_i \in \mathbb{R^m}, y_i \in \mathbb{R})</script>

<p>앙상블 모델은 output을 예측하기 위해 K개의 additive functions(가법 함수)를 이용한다. 
즉, f(x)는 q(x)라는 Tree 구조의 weight을 의미하는데,</p>

<p>$ \vec{x_i} $라는 i번째 데이터가 Input으로 들어왔을 때, 각각의 Tree가 Decision Rule을 통해 산출한 <strong>score = output = $ f_k(x_i) $</strong> 을 모두 더한 값을 아래의 식과 같이 <strong>최종 output = $ \hat{y_i} $</strong> 으로 출력하게 된다.</p>

<script type="math/tex; mode=display">\hat{y_i} = \phi(\vec{x_i}) = \sum_{k=1}^K f_k(\vec{x_i}), f_k \in \mathbb{F}</script>

<script type="math/tex; mode=display">\mathbb{F} = \{ f(\vec{x}) = w_{q(x)} \} (q:\mathbb{R} \rightarrow T, w \in \mathbb{R}^T)</script>

<p>여기서 K는 Tree의 개수를, T는 Tree 안에 있는 leaf의 개수를, w는 leaf weights를, $ w_i $는 i번째 leaf의 score를 의미한다.<br />
q는 example을 leaf index에 매핑하는 Tree 구조를 말하는데 이 안에는 물론 Tree 내부의 수많은 Decision Rule을 포함한다.<br />
F는 모든 Regression Trees를 포함하는 space of functions를 의미하며,<br />
여기서 Classification and Regression Trees의 경우 CART라고도 한다.</p>

<p>이러한 함수들을 학습하기 위해서는 다음과 같은 <strong>Objective Function</strong>을 상정할 필요가 있다.<br />
아래의 Regularized Objective는 예측 값과 실제 값 사이의 차이와 Regularized Term으로 구성된다.</p>

<script type="math/tex; mode=display">L(\phi) = \sum_{i}^{n} l(\hat{y_i}, y_i) + \sum_{k=1}^{K} \Omega(f_k)</script>

<p>여기서 $ \Omega(f) = \gamma T + \frac{1}{2} \lambda \Vert{w}\Vert^2  $</p>

<p>물론 위의 $ l $은 미분 가능한 convex loss function이 될 것이며,<br />
간단한 예로는 Square Loss나 Log Loss를 생각할 수 있을 것이다.</p>

<p>오른쪽 부분인 $ \Omega $의 역할은 모델이 너무 복잡해지는 것을 막는 페널티 항이다.<br />
이 항은 과적합을 방지하기 위해 final learnt weights을 부드럽게 만들어줄 것이다.
(<strong>Smoothing</strong>)</p>

<p>항을 자세히 보면, Tree 개수가 너무 많아지거나 leaf weights의 L2 norm이 너무 커지면 전체 Loss를 증가시키는 것을 알 수 있다.</p>

<p><strong>[2] Gradient Tree Boosting</strong><br />
위에서 본 전체 Loss는 각각의 Tree 구조 자체( f(x) )를 포함하고 있기 때문에 최적화하기가 까다롭다. 따라서 아래의 방법으로 최적화 과정에 논의해볼 것이다.</p>

<p>일단 $ \hat{y_i}^{(t)} $를 t번 째 iteration(t번 째 Tree)에서의 i번 째 Instance(실제 개체)의 예측 값이라고 해보자,</p>

<p>이 값은 아래의 과정에 의해 표현될 수 있다.</p>

<p><script type="math/tex">\hat{y_i}^{(0)} = 0</script><br />
<script type="math/tex">\hat{y_i}^{(1)} = f_1(x_i) + \hat{y_i}^{(0)}</script><br />
<script type="math/tex">...</script><br />
<script type="math/tex">\hat{y_i}^{(t)} = \sum_{k=1}^{t} f_k(x_i)</script></p>

<p>따라서 전체 Loss를 아래와 같이 표현할 수 있다.</p>

<script type="math/tex; mode=display">L^{(t)} = \sum_{i=1}^{n} l({y_i}, \hat{y_i}^{(t-1)} + f_t(\vec{x_i})) + \Omega(f_t)</script>

<p>이 단계에서 위의 $ l $ 부분을 2차항까지 사용한 테일러 전개에 의해 근사적으로 구하면, 다시 아래와 같이 표현할 수 있다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/04.jpg" width="70%" /></center>

<p>예를 들어 $ l $을 Square Loss로 사용하였다면, 아래와 같은 전개가 가능할 것이다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/06.jpg" width="70%" /></center>

<p>일반화된 식으로 다시 보면 상수항을 제거하고 남은 step t에서의 근사한 전체 Loss는 아래와 같다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/05.jpg" width="50%" /></center>

<blockquote>
  <p>여기서 잠시 $ I_j = {i|q(\vec{x_i} = j)} $를<br />
instance set of leaf j (leaf j의 할당 결과물)이라고 정의하겠다.</p>
</blockquote>

<p>위의 식에서 정규화 항을 확장하여 정리해보면,</p>

<script type="math/tex; mode=display">\tilde{L}^{(t)} = \sum_{i=1}^{n} [g_i f_i(\vec{x_i}) + \frac{1}{2}h_i f_t^2(\vec{x_i})] + \gamma T + \frac{1}{2} \sum_{j=1}^{T} w_j^2</script>

<script type="math/tex; mode=display">= \sum_{i=1}^{n} [g_i w_q(\vec{x_i}) + \frac{1}{2}h_i w_q^2(\vec{x_i})] + \gamma T + \frac{1}{2} \sum_{j=1}^{T} w_j^2</script>

<p>example 단위에서 leaf 단위로 식을 재표현해주면,</p>

<script type="math/tex; mode=display">= \sum_{j=1}^{T} [ (\sum_{i \in I_j} g_i)w_j + \frac{1}{2} (\sum_{i \in I_j} h_i + \lambda) w_j^2 ] + \gamma T</script>

<p>식을 보기 좋게 표현하기 위해 아래와 같은 정의를 사용하겠다.</p>

<script type="math/tex; mode=display">G_j = \sum_{i \in I_j} g_i, H_j = \sum_{i \in I_j} h_i</script>

<p>고정된 $ q(\vec{x}) $에 대하여 위의 식 = 0으로 놓고 계산하면,<br />
leaf j의 최적 weight을 계산할 수 있다.</p>

<script type="math/tex; mode=display">w_j^* = - \frac{G_j} {H_j + \lambda}</script>

<p>이 때의 전체 Loss는 아래와 같다.</p>

<script type="math/tex; mode=display">\tilde{L}^{(t)}(q) = - \frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^2} {H_j + \lambda} + \gamma T</script>

<p>정리하자면, 위의 식은 사실상 q라는 Tree 구조의 성능(quality)을 측정하는 Scoring Function의 역할을 수행하게 된다. 이 Score는 Decision Tree에서의 불순도와 같은 역할을 한다.</p>

<p>그런데 다만 여기서 생각해야 할 점은, 발생가능한 수많은 Tree의 구조를 일일히 다 평가할 수는 없다는 것이다. 이를 위해 <strong>Greedy 알고리즘</strong>이 사용되는데, 이 알고리즘은 단일 Leaf에서 시작하여 가지를 반복적으로 확장해 나가는 방법을 말한다.</p>

<p>$ I_L, I_R $을 각각 split 이후의 좌측, 우측 노드의 Instance Sets라고 할 때,<br />
<strong>Gain</strong> 혹은 <strong>Loss reduction</strong>이라고 불리는 아래의 식은,</p>

<script type="math/tex; mode=display">Gain = L_{split} = \frac{1}{2} [ \frac{G_{L}^2} {H_L + \lambda} + \frac{G_{R}^2} {H_R + \lambda} - \frac{ (G_{L} + G_{R})^2 } {H_L + H_R + \lambda} ] - \gamma</script>

<p>Left Child의 스코어 + Right Child의 스코어 - Split 안했을 때의 스코어 - Complexity cost by introducing additional leaf로 표현된다.</p>

<p>이는, split을 했을 때의 이득 (loss reduction)이 $ \gamma $로 표현되는 어떤 상수보다 작으면, split을 하지 말라는 뜻이다.<br />
즉, <strong>Training Loss Reduction &lt; Regularization Constant</strong>라면 split을 중지하게 되며,<br />
이는 <strong>Pruning</strong> 시스템이라고 할 수 있다.</p>

<p><strong>Pruning</strong>에는 2가지 방법이 있다.</p>
<ol>
  <li>Pre-Stopping: Best Split이 음수 Gain을 가지면 Stop한다. 다만 Future Split에서의 이득을 고려하지 못하므로 주의가 요구된다.</li>
  <li>Post-Pruning: Max_depth까지 확자한 후에 Negative Gain을 가진 Split 모두를 가지치기 한다.</li>
</ol>

<p><strong>[3] Efficient Findings of the Best Split</strong></p>

<p><strong>Exact Greedy Algorithm</strong>에서는 데이터를 Feature Value에 따라 정렬한 후 이와 같은 Gain을 반복적으로 계산하여 가장 높은 Gain을 바탕으로 Split을 결정하게 된다.<br />
(In order to do so efficiently, the algorithm must first sort the data according to feature values and visit the data in sorted order to accumulate the gradient statistics for the structure score)<br />
(자세한 내용은 논문을 참고할 것)</p>

<p><strong>Approximate Algorithm</strong>에서는 적절한 Split 후보들을 선정한 후 그 중에서만 찾게 된다.</p>

<p>직관적으로는 다음과 같은 과정을 거친다고 말할 수 있다.</p>
<ol>
  <li>Split할 양 쪽의 g, h의 합을 계산하는 것</li>
  <li>정렬된 Instances(개체)를 left -&gt; right 방향으로 스캔하면 feature 속에서 best split을 결정하기에 충분하다.</li>
</ol>

<p><strong>[3] Shrinkage and Colums Subsampling</strong><br />
위에서 설명된 정규화 과정 외에도 추가적으로 과적합을 막기 위한 방법이 도입된다.</p>

<p><strong>Shrinkage</strong>는 Tree Boosting의 각 단계를 실행한 이후 $ \eta $라는 factor를 도입하여 새로 추가된 weight을 스케일링해주는 기법이다. tochastic optimization과 유사한 방법인데, shrinkage는 모델을 향상시키기 위해 각각의 개별 Tree와 미래의 Tree의 leaves space의 영향력을 감소시킨다.</p>

<p><strong>Column(Feature) Subsampling</strong>은 Random Forest에서도 사용된 기법이다. 이 기법은 전통적인 Row- subsampling에 비해 더욱 효과적이고 빠르다고 알려져 있다.</p>

<p><strong>[4] Weighted Quantile Sketch</strong><br />
위에서 언급하였듯이 근사 알고리즘에서는 후보 Split을 제안하는데, 보통 이 때 feature의 percentil은 후보들이 데이터 상에서 고르게 분포하도록 만든다.<br />
그런데 XGBoost는 가중치가 부여된 데이터에 대해서도 효과적인 Handling이 가능하다.<br />
(Weighted quantile sketch algorithm can handle weighted data with a provable theoretical guarantee)</p>

<p>논문의 4페이지를 살펴보면, Rank Function과 전체 Loss 식의 재표현을 통해서 위의 설명을 간단히 증명하고 있다.</p>

<p><strong>[5] Sparsity-aware Split Finding</strong><br />
현실에서 데이터를 다룰 때 직면하게 되는 가장 큰 문제는 input인 $ \vec{x} $가 매우 sparse하다는 것이다. 이 현상에는 대표적으로 3가지 원인이 있다.</p>
<ol>
  <li>결측값</li>
  <li>통계학에서의 빈번한 zero entries</li>
  <li>원 핫 인코딩</li>
</ol>

<p>XGB는 내재적으로 이러한 현상을 효과적으로 Handling할 수 있다.<br />
왜냐하면 데이터를 통해 Optimal Default Direction이 학습되기 때문이다.</p>

<p><strong>[6] System Design</strong><br />
글의 서두에서 언급하였는데, 하드웨어 측면에서도 XGB는 우수한 성능을 보인다.</p>

<ul>
  <li>Block Structure for parallel learning: 데이터는 정렬되어 in-memory units (blocks)에 저장된다. 이 데이터는 이후에 계속 반복적으로 재사용이 가능하기 때문에 다시 계산할 필요가 없다. 이를 통해 빠르게 Split Point를 찾아낼 수 있고 Column Sub-sampling을 진행할 수 있다.</li>
  <li>Cache Awarness: 하드웨어를 최적으로 사용하도록 고안되었다.</li>
  <li>Out-of-core computing: 거대한 데이터를 다룰 때 디스크 공간을 최적화하고 사용 가능 범위를 최대화한다.</li>
</ul>

<p>XGB는 또한 Early Stopping 기능도 갖고 있다.<br />
XGb는 참고로 Feature Engineering이나 Hyper Parameter 자동 튜닝 등의 기능은 갖고 있지 못하다.</p>

<p>이로써 XGBoost의 이론적 배경에 대해 살펴보았다.</p>

<hr />

<h2 id="xgboost-패키지-methods">XGBoost 패키지 Methods</h2>
<blockquote>
  <p>지금부터는 XGBoost Python을 효과적을 Implement하는 방법에 대해 설명한다.</p>
</blockquote>

<hr />

<h2 id="xgboost-parameter-tuning">XGBoost Parameter Tuning</h2>
<p>파라미터 튜닝의 세부사항을 설명하기 전에, 가장 전반적인 2가지 사항에 대해 설명한다.</p>

<ol>
  <li>Control Overfitting<br />
일차적으로는 모델 Complexity를 직접적으로 조절할 수 있는데, 이는 max_depth, min_child_weight, gamma 파라미터 조정에 해당한다.</li>
</ol>

<p>이후에 학습 과정을 Noise에 Robust하게 만들기 위해 Randomness를 추가하는 방법이 있는데, 이는 subsample, colsample_bytree 파라미터 조정에 해당한다.<br />
또는 stepsize eta를 줄일 수도 있는데, 이 때는 num_round를 늘려야만 한다.</p>

<blockquote>
  <p>link: [https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html]</p>
</blockquote>

<ol>
  <li>Handle Imbalanced Dataset<br />
불균형 데이터를 효과적으로 다루기 위해서는 scale_pos_weight 파라미터 조정을 통해 positive &amp; negative weights를 균형적으로 맞출 수 있다.</li>
</ol>

<p>만약 오직 right probability를 예측하는 것에만 관심이 있다면,<br />
Dataset을 균형적으로 맞추기 힘드므로, max_delta_step 파라미터를 1과 같은 유한 실수로 세팅하면 효과적인 convergence(수렴)을 가능하게 할 수 있다.</p>

<p><strong>XGBoost Parameters</strong></p>
<blockquote>
  <p>XGB 파라미터는 크게 3가지로 구분된다.</p>
  <ul>
    <li>General, Booster, Task paramers</li>
  </ul>
</blockquote>

<p><strong>General Parameters</strong>는 부스팅을 위해 어떤 부스터를 쓰는지와 관련이 있다.<br />
<strong>Booster Parameters</strong>는 선택한 Booster에 의존한다.<br />
<strong>Task Paramters</strong>는 학습 시나리오를 결정한다. 예를 들어, Regression tasks는 ranking tasks와 관련하여 다른 파라미터를 사용할 수 있다.</p>

<p>참고로 R에서는 _대신 .를 사용하면 된다.</p>

<p><strong>1. General Parameters</strong></p>
<ul>
  <li>
    <p><strong>booster [default=gbtree]</strong><br />
gbtree(기본값), gblinear(선형), dart(tee based model 사용)</p>
  </li>
  <li>
    <p><strong>silent [default=0]</strong><br />
0은 학습 과정을 출력해라, 1은 출력하지 마라.</p>
  </li>
  <li>
    <p><strong>nthread [default=최대치]</strong><br />
XGB를 돌리기 위해 사용될 병렬 스레드의 개수</p>
  </li>
  <li>
    <p><strong>disable_default_eval_metric [default=0]</strong><br />
flag to disable default metric. Set to &gt;0 to disable</p>
  </li>
  <li>
    <p>num_pbuffer, num_feature는 자동적으로 설정됨</p>
  </li>
</ul>

<p><strong>2. Booster Parameters</strong></p>
<ul>
  <li>
    <p><strong>eta [default=0.3, alias=learning_rate]</strong><br />
과적합을 방지하기 위해 업데이트 과정에서 사용되는 shrinkage의 step size이다. 각 부스팅 단계이후 우리는 새로운 features에 대한 weights를 얻을 수 있는데, eta는 부스팅 과정을 더욱 보수적으로 만들기 위해 feature weights를 축소한다. 결론적으로 과적합을 방지하는 파라미터다!</p>
  </li>
  <li>
    <p><strong>gamma [default=0, alias=min_split_loss]</strong><br />
Tree의 leaf split을 진행하기 위해 필요한 최소 Loss Reduction을 뜻한다.<br />
gamma가 커질수록, 알고리즘은 더욱 보수적으로 만들어질 것이다.<br />
min_loss_reduction이라는 다른 이름을 생각해볼 때, 이 파라미터는 아래의 식에서 $ \gamma $를 뜻한다.</p>
  </li>
</ul>

<script type="math/tex; mode=display">Gain = L_{split} = \frac{1}{2} [ \frac{G_{L}^2} {H_L + \lambda} + \frac{G_{R}^2} {H_R + \lambda} - \frac{ (G_{L} + G_{R})^2 } {H_L + H_R + \lambda} ] - \gamma</script>

<ul>
  <li>
    <p><strong>max_depth [default=6]</strong><br />
Tree구조의 최대 깊이이다. 0을 입력하면 한계치를 설정하지 않음을 뜻한다.</p>
  </li>
  <li>
    <p><strong>min_child_weight [default=1]</strong><br />
Child Node에 필요한 Instance weight(hessian)의 최소합.<br />
만약 Tree의 Split 과정이 진행되면서 instance weight의 합이 min_child_weight보다 작은 leaf node가 나타난다면, Tree는 계속해서 Split을 진행하도록 설정하는 것이다.<br />
결론적으로 min_child_weight가 커질수록, 알고리즘은 더욱 보수적으로 변화한다.</p>
  </li>
  <li>
    <p><strong>max_delta_step [default=0]</strong><br />
Maximum delta step we allow each leaf output to be.<br />
디폴트로 설정된 0은 제한이 없음을 뜻한다.<br />
양수로 설정이 되면, update step을 더욱 보수적으로 만들어준다.<br />
일반적으로 이 파라미터는 불필요한데, Logistic Regression에서 데이터셋이 심각하게 불균형한 경우 [1-10]에 해당하는 값을 설정한다면 도움이 될 수도 있다.</p>
  </li>
  <li>
    <p><strong>subsample [default=1]</strong><br />
Training Instances의 Subsample 비율을 말한다.<br />
예를 들어 0.5로 설정될 경우, XGB가 학습 데이터의 절반을 랜덤하게 샘플링한다는 것을 뜻한다. Dropout과 유사한 측면이 있다.<br />
수치가 작아질 수록 과적합을 방지하지만 학습이 더뎌질 수 있다.</p>
  </li>
  <li>
    <p><strong>colsample_bytree [default=1]</strong><br />
Subsample ratio of columns when constructing each tree.</p>
  </li>
  <li>
    <p><strong>colsample_bylevel [default=1]</strong><br />
Subsample ratio of columns for each split, in each level.</p>
  </li>
  <li>
    <p><strong>lambda [default=1, alias:reg_lambda]</strong><br />
Weight에 대한 L2 정규화항. 커질수록 모델을 보수적으로 만든다.</p>
  </li>
</ul>

<script type="math/tex; mode=display">L(\phi) = \sum_{i}^{n} l(\hat{y_i}, y_i) + \sum_{k=1}^{K} \Omega(f_k) \leftarrow  \Omega(f) = \gamma T + \frac{1}{2} \lambda \Vert{w}\Vert^2</script>

<ul>
  <li>
    <p><strong>alpha [default=0, alias: reg_alpha]</strong><br />
Weight에 대한 L1 정규화항.</p>
  </li>
  <li>
    <p><strong>tree_method [defaul=auto]</strong><br />
Tree 구성 구조 방법을 말한다. 단, Distributed and external memory 버전은 오직 approx만 지원한다.<br />
auto외에는 exact(Exact Greedy 알고리즘), approx(Approximate Greedy 알고리즘), hist(Fast Histo Optimized Approxmate Greedy 알고리즘), gpu_exact, gpu_hist 등이 있다.<br />
auto로 두면 적당한 크기의 데이터셋에 대해서는 exact를 선택하고, 데이터셋이 매우 커지면 approx를 자동으로 선택한다.<br />
이 방법들에 대한 간략한 설명은 위 논문 리뷰에서 다루었다.</p>
  </li>
  <li>
    <p><strong>scale_pos_weight [default=1]</strong><br />
positive &amp; negative weights의 밸런스를 조정하므로, 불균형 데이터에 대해 유용한 파라미터이다.</p>
  </li>
</ul>

<p><strong>3. Task Parameters</strong></p>
<ul>
  <li>
    <p><strong>objective [default=reg:linear]</strong><br />
reg:linear, reg:logistic, binary:logistic, binary:logitraw, binary:hinge
위의 것에서 gpu:추가 가능<br />
count:poisson, survival:cos, multi:softmax, multi:softprob, rank:pairwise, rank:ndcg, rank:map, reg:gamma</p>
  </li>
  <li>
    <p><strong>base_score [default=0.5]</strong><br />
The initial prediction score of all instances, global bias.<br />
바꿀 필요 없다.</p>
  </li>
  <li>
    <p><strong>eval_metric</strong><br />
rmse, mae, logloss, error, auc, mlogloss, …</p>
  </li>
  <li>
    <p><strong>seed</strong></p>
  </li>
</ul>

<blockquote>
  <p>Dart Booster이나 Linear Booster를 선택하였을 때 따라오는 추가적인 파라미터 조정은 Documentation을 참조할 것.<br />
Console에서만 사용가능한 Line Parameter들도 있다.<br />
Link: [https://xgboost.readthedocs.io/en/latest/parameter.html]</p>
</blockquote>

<hr />

<h2 id="adaboost">AdaBoost</h2>
<p>AdaBoost는 Additive Boosting의 줄임말로, 1995년에 등장하였지만 빠르고 정확한 성능으로 좋은 평가를 받고 있는 알고리즘이다.<br />
간결한 설명을 위해 본 논문의 설명은 m개의 training data에 대하여 Y는 = {-1, +1}로,<br />
Binary Classification 문제로 범위를 제한한다.</p>

<p>AdaBoost는 t개의 weak(base) learning algorithm을 반복적으로 호출하여 학습을 진행한다. (t = 1 ~ T)</p>

<p>여기서 t번 째 round에서의 training example i에 대한 weight distribution을 $ D_t(i) $이라고 하자.</p>

<p>초기에 weight은 균일 분포로서 초기화되어 모두 동일하게 설정되지만,<br />
잘못 분류된 example에 대한 weights는 증가하게 된다. 이렇게 되면 weak learner가 다음 round에서 학습을 진행할 때 이러한 example에 대해 더욱 집중하게 하는 효과를 낼 수 있다.</p>

<p><strong>Weak Learner</strong>의 일은 $ D_t $ 분포에 적합한 weak hypothesis $ h_t : X \rightarrow {-1, +1} $을 찾는 것이다.<br />
Weak Learner는 $ D_t $를 다시 학습할 때 사용하거나 $ D_t $에 따라 다시 표본이 추출될 수 있다.<br />
그 weak hypothesis의 성능은 다음과 같이 Error를 계산하여 평가할 수 있다.</p>

<script type="math/tex; mode=display">\epsilon_t = P_{i \sim D_t} [ h_t(x_i) \neq y_i] = \sum_{i:h_t(x_i) \neq y_i} D_t(i)</script>

<p>Adaboost의 부스팅 알고리즘은 아래와 같다. 사실 그리 어렵지는 않다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/07.jpg" width="90%" /></center>

<p>$ \alpha_t $는 결국 $ h_t $에 배정된 가중치라고 볼 수 있다.</p>

<p><strong>Analyzing the training error</strong><br />
$ \gamma_t $를 모델의 예측이 Random Guess보다 얼마나 나은지를 나타낸다고 하면,<br />
$ h_t $의 Error인 $ \epsilon_t $은 $ \frac{1}{2} - \gamma_t $로 표현할 수 있다.</p>

<p>아래 식은, 최종 hypothesis H의 Training Error는 일정 수치보다 작을 수 밖에 없음을 나타내는데,<br />
이는 만약 Weak Hypothesis가 적어도 Random Guess보다는 낫다면, 결과적으로 Training Error는 지수적으로 빠르게 감소할 수 밖에 없음을 나타낸다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/08.jpg" width="50%" /></center>

<p>이전 알고리즘들도 이와 유사한 과정을 거쳤지만 이들은 $ \gamma_t $의 하한선인 $ \gamma $라는 상수에 대한 사전 정의가 필요했다. AdaBoost는 그러한 과정이 필요 없으며, 각각의 Weak Hypothesis의 Error rates에 adapt하는 모습을 보여준다.<br />
이 때문에 AdaBoost는 Adaptive Boosting이다.</p>

<p><strong>Generalization Error</strong><br />
기존의 연구는 최종 Hypothesis의 Generalization에러를 Training Error의 관점에서 설명할 때,<br />
아래와 같은 식으로 나타냈었는데, 이는 T가 커질 때, boosting 모델은 결국 과적합한다는 것을 의미한다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/09.jpg" width="35%" /></center>

<table>
  <thead>
    <tr>
      <th>Sign</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>T</td>
      <td>boosting round 수</td>
    </tr>
    <tr>
      <td>d</td>
      <td>hypotheseis의 공간의 compexity의 standard measure인 VC-차원</td>
    </tr>
    <tr>
      <td>m</td>
      <td>example 수</td>
    </tr>
    <tr>
      <td>$ \hat{Pr(.)} $</td>
      <td>empirical probability on the training example</td>
    </tr>
  </tbody>
</table>

<p>그런데 이후의 연구를 보면 이는 종종 사실이 아닌 것으로 나타났다.<br />
특히 AdaBoost의 경우 Training Error가 0에 도달한 이후에 지속적인 학습을 진행한 결과,<br />
(Generalization Error)Test Error가 점차적으로 감소한 것을 알 수 있었다.</p>

<p>이를 설명하기 위해 다른 개념이 도입되었는데, 아래를 Maring of exmaple(x, y)라고 한다.</p>

<script type="math/tex; mode=display">{Margin} = \frac{y * \sum_t \alpha_t h_t(x)} {\sum_t \alpha_t}</script>

<p>이 식은 [-1, +1]에 속하며 H가 example을 적절히 분류했을 때 0의 값을 가진다.<br />
이 Margin의 Magnitude는 prediction의 confidence를 측정한다고 해석할 수 있다.</p>

<p>이후에 증명된 바에 따르면,<br />
Training Set에서 Margin이 더욱 증가하면 이는 Generalization Error의 상위의 상한선으로 변환된다고 한다.</p>

<p>식으로 표현하면 아래와 같은데, $ \theta $로 표현되는 상한선(Upper Bound)가 클 수록 Prediction에 자신이 있다는 뜻이고, 이 $ \theta $는 T에 독립적이기 때문에 반복 횟수가 증가해도 Error가 증가하지 않는다.</p>

<center><img src="/public/img/Machine_Learning/2018-11-06-TripleB/10.jpg" width="50%" /></center>

<p>AdaBoost의 이와 같은 기재는 game-theoretic setting과 같은 방식으로도 이해될 수 있다.<br />
이는 Boosting이 어떤 특정 게임의 반복 play라고 할 때,<br />
AdaBoost는 이 게임에 반복적으로 참여하여 근사적으로 게임을 푸는 General한 알고리즘의 특별한 케이스라고 해석하는 것이다.</p>

<p><strong>Experimetns and Applications</strong></p>
<ol>
  <li>AdaBoost는 간단하고 사용하기 쉽다. weak learner에 대한 사전지식이 필요없으며 여러 method와 결합하여 사용이 가능하다.</li>
  <li>T 빼고는 튜닝할 Hyperparameter가 없다.</li>
  <li>Noise에 민감하다.</li>
  <li>AdaBoost의 operation은 선형 번류기의 coordinate-wise gradient descent로 해석할 수 있다.</li>
  <li>AdaBoost는 Outlier를 찾아내는 데에 뛰어난 성능을 보인다. (높은 Weight은 Outlier일 확률이 높다.)</li>
</ol>

<hr />

<h2 id="light-gbm">Light GBM</h2>
<p>Light GBM은</p>

    </article>
    <div class="post-more">
      
      <a href="/machine_learning/2018/11/06/TripleB/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/machine_learning/2018/11/06/TripleB/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/">
        PyTorch 사용법 - 02. Linear Regression Model
      </a>
    </h1>

    <span class="post-date">02 Nov 2018</span>
     |
    
    <a href="/blog/tags/#pytorch" class="post-tag">PyTorch</a>
    
    

    <article>
      <hr />

<p><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-00-references/">PyTorch 사용법 - 00. References</a><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-01-introduction/">PyTorch 사용법 - 01. 소개 및 설치</a><br />
<strong><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/">PyTorch 사용법 - 02. Linear Regression Model</a></strong><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/">PyTorch 사용법 - 03. How to Use PyTorch</a><br />
<a href="https://greeksharifa.github.io/pytorch/2019/06/12/pytorch-usage-04-RNN-Model/">PyTorch 사용법 - 04. Recurrent Neural Network Model</a></p>

<hr />

<p>이 글에서는 가장 기본 모델인 Linear Regression Model의 Pytorch 프로젝트를 살펴본다.</p>

<p>사용되는 torch 함수들의 사용법은 <a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-00-references/">여기</a>에서 확인할 수 있다.</p>

<hr />

<h2 id="프로젝트-구조">프로젝트 구조</h2>

<ul>
  <li>02_Linear_Regression_Model/
    <ul>
      <li>main.py</li>
      <li>data/
        <ul>
          <li>02_Linear_Regression_Model_Data.csv</li>
        </ul>
      </li>
      <li>results/</li>
    </ul>
  </li>
</ul>

<ol>
  <li>일반적으로 데이터는 <code class="highlighter-rouge">data/</code> 디렉토리에 넣는다.</li>
  <li>코드는 git에 두고, <code class="highlighter-rouge">data/</code>는 <code class="highlighter-rouge">.gitignore</code> 파일에 추가하여 데이터는 git에 올리지 않는다. 파일은 다른 서버에 두고 필요할 때 다운로드한다. 일반적으로 dataset은 그 크기가 수 GB 혹은 그 이상도 될 수 있기 때문에 upload/download 시간이 굉장히 길어지기도 하고, <a href="https://github.com/">Git</a>이 100MB 이상의 큰 파일은 업로드를 지원하지 않기 때문이기도 하다.</li>
</ol>

<p>물론 이 예제 프로젝트는 너무 간단하여 그냥 <code class="highlighter-rouge">data/</code> 디렉토리 없이 해도 상관없다.<br />
그리고 <code class="highlighter-rouge">output/</code> 또는 <code class="highlighter-rouge">results/</code> 디렉토리를 만들도록 한다.</p>

<hr />

<h2 id="import">Import</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>다음 파일을 다운로드하여 <code class="highlighter-rouge">data/</code> 디렉토리에 넣는다.</p>

<p><a href="https://github.com/greeksharifa/Tutorial.code/blob/master/Python/PyTorch_Usage/02_Linear_Regression_Model/data/02_Linear_Regression_Model_Data.csv">02_Linear_Regression_Model_Data.csv</a></p>

<ol>
  <li><a href="https://pytorch.org/">torch</a>: 설명이 필요없다.</li>
  <li><a href="https://pytorch.org/docs/stable/nn.html">from torch import nn</a>: nn은 Neural Network의 약자이다. torch의 nn 라이브러리는 Neural Network의 모든 것을 포괄하며, Deep-Learning의 가장 기본이 되는 1-Layer Linear Model도 <code class="highlighter-rouge">nn.Linear</code> 클래스를 사용한다. 이 예제에서도 <strong>nn.Linear</strong>를 쓴다.
    <ul>
      <li><strong>nn.Module</strong>은 모든 Neural Network Model의 Base Class이다. 모든 Neural Network Model(흔히 Net이라고 쓴다)은 <strong>nn.Module</strong>의 subclass이다. nn.Module을 상속한 어떤 subclass가 Neural Network Model로 사용되려면 다음 두 메서드를 override해야 한다.
        <ul>
          <li><code class="highlighter-rouge">__init__(self)</code>: <strong><em>Initialize.</em></strong> 여러분이 사용하고 싶은, Model에 사용될 구성 요소들을 정의 및 초기화한다. 대개 다음과 같이 사용된다.
            <ul>
              <li>self.conv1 = nn.Conv2d(1, 20, 5)</li>
              <li>self.conv2 = nn.Conv2d(20, 20, 5)</li>
              <li>self.linear1 = nn.Linear(1, 20, bias=True)</li>
            </ul>
          </li>
          <li><code class="highlighter-rouge">forward(self, x)</code>: <strong><em>Specify the connections.</em></strong> <code class="highlighter-rouge">__init__</code>에서 정의된 요소들을 잘 연결하여 모델을 구성한다. Nested Tree Structure가 될 수도 있다. 주로 다음처럼 사용된다.
            <ul>
              <li>x = F.relu(self.conv1(x))</li>
              <li>return F.relu(self.conv2(x))</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>다른 말로는 위의 두 메서드를 override하기만 하면 손쉽게 Custom net을 구현할 수 있다는 뜻이기도 하다.</li>
    </ul>
  </li>
  <li>참고: <strong>torch.autograd.Variable</strong>은 이전에는 auto gradient 계산을 위해 tensor에 필수적으로 씌워 주어야 했으나, PyTorch 0.4.0 버전 이후로 <code class="highlighter-rouge">torch.Tensor</code>와 <code class="highlighter-rouge">torch.autograd.Variable</code> 클래스가 통합되었다. 따라서 PyTorch 구버전을 사용할 예정이 아니라면 Variable은 쓸 필요가 전혀 없다.
    <ul>
      <li>인터넷에 돌아다니는 수많은 코드의 Variable Class는 0.4.0 버전 이전에 PyTorch를 시작한 사람들이 쓴 것이다.</li>
      <li><a href="https://pytorch.org/docs/stable/autograd.html#variable-deprecated/">https://pytorch.org/docs/stable/autograd.html#variable-deprecated/</a></li>
      <li><a href="https://pytorch.org/blog/pytorch-0_4_0-migration-guide/">https://pytorch.org/blog/pytorch-0_4_0-migration-guide/</a></li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="load-data">Load Data</h2>

<h3 id="데이터-준비">데이터 준비</h3>

<p>지금의 경우는 전처리할 필요가 없으므로 그냥 데이터를 불러오기만 하면 된다. 데이터가 어떻게 생겼는지도 확인해 보자.<br />
데이터가 어떤지 살펴보는 것은 모델을 결정하는 데 있어 매우 중요하다.</p>

<p>다운로드는 <a href="https://drive.google.com/open?id=1gVxV5eD5NfyEO4aHSyAGmsDgUco8FQPb">여기</a>에서 할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data/02_Linear_Regression_Model_Data.csv'</span><span class="p">)</span>
<span class="c1"># Avoid copy data, just refer
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'x'</span><span class="p">].</span><span class="n">values</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'y'</span><span class="p">].</span><span class="n">values</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">);</span>    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'02_Linear_Regression_Model_Data'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/public/img/PyTorch/2018-11-02-pytorch-usage-02-Linear-Regression-Model/02_Linear_Regression_Model_Data.png" alt="02_Linear_Regression_Model_Data" /></p>

<p><strong>from_numpy</strong>로 불러오는 이유는 데이터를 복사하여 새로 텐서를 생성하는 대신 원 데이터와 메모리를 공유하는 텐서를 쓰기 위함이다. 지금은 상관없지만 대용량의 데이터를 다룰 때에는 어떤 함수가 데이터를 복사하는지 아닌지를 확실하게 알아둘 필요가 있다.<br />
물론, 정말 대용량의 데이터의 경우는 read_csv로 한번에 불러오지 못한다. 이는 데이터를 <em>batch</em>로 조금씩 가져오는 것으로 해결하는데, 이에 대해서는 나중에 살펴보자.</p>

<p>참고: 이 데이터는 다음 코드를 통해 생성되었다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">5</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">data</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'data/02_Linear_Regression_Model_Data.csv'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="p">[</span><span class="s">'x'</span><span class="p">,</span> <span class="s">'y'</span><span class="p">])</span>
</code></pre></div></div>

<hr />

<h2 id="define-and-load-model">Define and Load Model</h2>

<p>매우 간단한 모델이므로 코드도 짧다.<br />
여기서는 여러분의 편의를 위해 함수들의 parameter 이름을 명시하도록 한다.</p>

<p>PyTorch에서 Linear 모델은 <code class="highlighter-rouge">torch.nn.Linear</code> 클래스를 사용한다. 여기서는 단지 x를 y로 mapping하는 일차원 직선($ y = wx + b $)을 찾고 싶은 것이므로, <code class="highlighter-rouge">in_features</code>와 <code class="highlighter-rouge">out_features</code>는 모두 1이다.<br />
<strong>nn.Linear</strong>은 <strong>nn.Module</strong>의 subclass로 in_features개의 input을 선형변환을 거쳐 out_features개의 output으로 변환한다. parameter 개수는 $ (in _ features \times out _ features [ + out _ features]) $ 개이다. 마지막 항은 <strong>bias</strong>이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>

<span class="s">"""
Linear(in_features=1, out_features=1, bias=True)
Parameter containing:
tensor([[-0.9360]], requires_grad=True)
Parameter containing:
tensor([0.7960], requires_grad=True)
"""</span>
</code></pre></div></div>

<p>별다른 utility 함수가 필요 없으므로 따로 <code class="highlighter-rouge">utils.py</code>는 만들지 않는다.</p>

<hr />

<h2 id="set-loss-functioncreterion-and-optimizer">Set Loss function(creterion) and Optimizer</h2>

<p>적절한 모델을 선정할 때와 마찬가지로 loss function과 optimizer를 결정하는 것은 학습 속도와 성능을 결정짓는 중요한 부분이다.<br />
지금과 같이 간단한 Linear Regression Model에서는 어느 것을 사용해도 학습이 잘 된다. 하지만, 일반적으로 성능이 좋은 <code class="highlighter-rouge">AdamOptimizer</code>를 사용하도록 하겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="s">"""
tensor([[-0.1399],
        [-1.0759],
        [-2.0119],
        [-2.9478],
        [-3.8838],
        [-4.8197],
        [-5.7557],
        [-6.6917],
        [-7.6276],
        [-8.5636]], grad_fn=&lt;ThAddmmBackward&gt;)
"""</span>
</code></pre></div></div>

<p>참고: 보통 변수명은 criterion 혹은 loss_function 등을 이용한다.</p>

<hr />

<h2 id="train-model">Train Model</h2>

<p>Train은 다음과 같이 이루어진다.</p>

<ol>
  <li>모델에 데이터를 통과시켜 예측값(현재 모델의 weights로 prediction)을 얻은 뒤</li>
  <li>실제 정답과 loss를 비교하고</li>
  <li>gradient를 계산한다.</li>
  <li>이 값을 통해 weights를 업데이트한다(backpropagation).</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="s">"""
        Show your intermediate results
        """</span>
        <span class="k">pass</span>
</code></pre></div></div>

<p>코드의 각 라인을 설명하면 다음과 같다.</p>

<ol>
  <li><code class="highlighter-rouge">prediction</code>: 모델에 데이터(x)를 집어넣었을 때 예측값(y). 여기서는 $ y = wx + b $의 결과들이다.</li>
  <li><code class="highlighter-rouge">loss</code>: criterion이 MSELoss로 설정되어 있으므로, prediction과 y의 평균제곱오차를 계산한다.</li>
  <li><code class="highlighter-rouge">optimizer.zero_grad()</code>: optimizer의 grad를 0으로 설정한다. PyTorch는 parameter들의 gradient를 계산해줄 때 grad는 계속 누적되도록 되어 있다. 따라서 gradient를 다시 계산할 때에는 0으로 세팅해주어야 한다.</li>
  <li><code class="highlighter-rouge">loss.backward()</code>: gradient 계산을 역전파(backpropagation)한다.</li>
  <li><code class="highlighter-rouge">optimizer.step()</code>: 계산한 gradient를 토대로 parameter를 업데이트한다($ w \leftarrow w - \alpha \Delta w, b \leftarrow b - \alpha \Delta b $)</li>
  <li>학습 결과를 중도에 확인하고 싶으면 그래프를 중간에 계속 그려주는 것도 한 방법이다.</li>
</ol>

<hr />

<h2 id="visualize-and-save-results">Visualize and save results</h2>

<p>결과를 그래프로 보여주는 부분은 <code class="highlighter-rouge">matplotlib.pyplot</code>에 대한 내용이므로 여기서는 넘어가도록 하겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">display_results</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">);</span>    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">prediction</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s">'b--'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'loss={:.4}, w={:.4}, b={:.4}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">model</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">model</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">item</span><span class="p">()))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="c1"># plt.savefig('results/02_Linear_Regression_Model_trained.png')
</span>
<span class="n">display_results</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/public/img/PyTorch/2018-11-02-pytorch-usage-02-Linear-Regression-Model/02_Linear_Regression_Model_trained.png" alt="02_Linear_Regression_Model_Trained" /></p>

<p>모델을 저장하려면 <code class="highlighter-rouge">torch.save</code> 함수를 이용한다. 저장할 모델은 대개 <code class="highlighter-rouge">.pt</code> 확장자를 사용한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">obj</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="s">'02_Linear_Regression_Model.pt'</span><span class="p">)</span>
</code></pre></div></div>

<p>참고: <code class="highlighter-rouge">.pt</code> 파일로 저장한 PyTorch 모델을 load해서 사용하려면 다음과 같이 한다. 이는 나중에 <strong>Transfer Learning</strong>과 함께 자세히 다루도록 하겠다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="s">'02_Linear_Regression_Model.pt'</span><span class="p">)</span>

<span class="n">display_results</span><span class="p">(</span><span class="n">loaded_model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>정확히 같은 결과를 볼 수 있을 것이다.</p>

<hr />

<p>전체 코드는 <a href="https://github.com/greeksharifa/Tutorial.code/blob/master/Python/PyTorch_Usage/02_Linear_Regression_Model/main.py">여기</a>에서 살펴볼 수 있다.</p>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/pytorch/2018/11/02/pytorch-usage-01-introduction/">
        PyTorch 사용법 - 01. 소개 및 설치
      </a>
    </h1>

    <span class="post-date">02 Nov 2018</span>
     |
    
    <a href="/blog/tags/#pytorch" class="post-tag">PyTorch</a>
    
    <a href="/blog/tags/#tensorflow" class="post-tag">TensorFlow</a>
    
    <a href="/blog/tags/#usage" class="post-tag">usage</a>
    
    

    <article>
      <hr />

<p><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-00-references/">PyTorch 사용법 - 00. References</a><br />
<strong><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-01-introduction/">PyTorch 사용법 - 01. 소개 및 설치</a></strong><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/">PyTorch 사용법 - 02. Linear Regression Model</a><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/">PyTorch 사용법 - 03. How to Use PyTorch</a><br />
<a href="https://greeksharifa.github.io/pytorch/2019/06/12/pytorch-usage-04-RNN-Model/">PyTorch 사용법 - 04. Recurrent Neural Network Model</a></p>

<hr />

<h2 id="간단한-소개">간단한 소개</h2>

<p>PyTorch는 유연성과 속도를 모두 갖춘 딥러닝 연구 플랫폼이다. GPU 사용이 가능하기 때문에 속도가 상당히 빠르다.<br />
또 입문 난이도가 높지 않은 편이고 코드가 간결하다.<br />
현재는 TensorFlow의 사용자가 많지만, 그 특유의 비직관적인 구조와 난이도 때문에 요즘은 PyTorch의 사용자가 늘어나는 추세이다.</p>

<h3 id="tensorflow와의-비교">TensorFlow와의 비교</h3>

<p>TensorFlow는 구현 패러다임이 Define and Run인데 비해, PyTorch는 Define by Run이다.</p>

<p>무슨 말인가 하면, TensorFlow는 코드를 직접 돌리는 환경인 세션을 만들고, placeholder를 선언하고 이것으로 계산 그래프를 만들고(Define), 코드를 실행하는 시점에 데이터를 넣어 실행하는(Run) 방식이다.<br />
이는 계산 그래프를 명확히 보여주면서 실행시점에 데이터만 바꿔줘도 되는 유연함을 장점으로 가지지만, 그 자체로 비직관적이다. 그래서 딥러닝 프레임워크 중 난이도가 가장 높은 편이다.</p>

<p>하지만 PyTorch는 그런 어려움이 없다. 일반적인 Python 코딩이랑 크게 다를 바 없다. 선언과 동시에 데이터를 집어넣고, 세션 같은 것도 필요없이 그냥 돌리면 끝이다. 덕분에 코드가 간결하고, 난이도가 낮다.<br />
한 가지 단점은 아직 사용자가 적어 구글링했을 때 검색 결과가 많지 않다는 정도?</p>

<hr />

<h2 id="설치-방법">설치 방법</h2>

<script data-ad-client="ca-pub-9951774327887666" async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<p><a href="https://pytorch.org/">여기</a>를 참조한다. 자신에게 맞는 OS, package manager, Python 버전, CUDA 버전 등을 선택하면 그에 맞는 명령어 집합이 나온다. 이를 명령창에 실행하면 설치가 진행된다.<br />
torchvision을 설치할 경우에 무슨 라이브러리가 없다면서 에러 메시지가 뜨긴 하는데, 사용하는 데 별 문제는 없을 것이다. 만약 자신이 그 부분을 꼭 써야 한다면 에러를 해결하고 넘어가자.</p>

<p>설치를 완료했으면, 명령창에 다음과 같이 입력해보자. Anadonda를 플랫폼으로 사용한다면 conda 설정은 직접 해 주어야 한다.</p>

<p><code class="highlighter-rouge">python</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 이 부분은 Python Interpreter에서 입력함.
</span><span class="kn">import</span> <span class="nn">torch</span>  
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>  
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>결과가 대략 다음과 같이 나오면 설치가 완료되었다. 숫자가 다른 것은 랜덤이니 신경 쓰지 말자.</p>

<p><img src="/public/img/PyTorch/2018-11-02-pytorch-usage-01-Introduction/01_run_pytorch.PNG" alt="01_run_pytorch.PNG" /></p>

<hr />

<h2 id="gpu-사용을-위한-설치">GPU 사용을 위한 설치</h2>

<p>GPU 사용을 위한 필수 절차는 다음과 같다. <em>경우의 수가 너무 많아서 스크린샷은 생략</em></p>

<ol>
  <li><strong>호환성 체크</strong>
    <ol>
      <li>컴퓨터에 있는 GPU의 <strong>compute capability</strong> 확인
        <ul>
          <li><a href="https://developer.nvidia.com/cuda-gpus">여기</a>에서 확인</li>
        </ul>
      </li>
      <li>compute capability에 맞는 CUDA SDK 버전 확인
        <ul>
          <li><a href="https://en.wikipedia.org/wiki/CUDA#GPUs_supported">여기</a>에서 확인</li>
        </ul>
      </li>
      <li>Pytorch와 CUDA의 호환성 확인
        <ul>
          <li>설치하고자 하는 PyTorch(또는 Tensorflow)가 지원하는 최신 CUDA 버전이 있다. 이보다 상위 버전의 CUDA를 설치하면 PyTorch 코드가 제대로 돌아가지 않는다.</li>
          <li><a href="https://pytorch.org/">Pytorch 홈페이지</a>에서 정해주는 CUDA 버전을 설치하는 쪽이 편하다. 2020.02.13 기준 최신 버전은 10.1이다.</li>
        </ul>
      </li>
      <li>CUDA에 맞는 cuDNN 버전 확인
        <ul>
          <li><a href="https://developer.nvidia.com/rdp/cudnn-archive">여기</a>에서 확인할 수 있다.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li><strong>CUDA 설치</strong>
    <ol>
      <li><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA toolkit archive</a>에서 원하는 CUDA를 다운받는다. 운영체제와 버전 등을 체크하고, 가능하면 Installer Type은 network가 아닌 local로 받는다. 인터넷으로 설치하면서 받는 것이 아닌 한번에 설치파일을 받는 식이다.
        <ul>
          <li>같은 버전인데 update가 추가된 버전이 있다. 보통은 이것까지 추가로 설치해 주는 쪽이 좋다. base installer를 먼저 설치한 뒤에 추가로 설치해 주도록 하자.</li>
        </ul>
      </li>
      <li>설치 파일로 CUDA를 설치한다. 설치 시에는 다른 프로그램을 설치하거나 제거하는 중이면 실행이 되지 않으니 주의하자.</li>
      <li>cuda visual studio integration 관련해서 설치 실패가 뜨는 경우가 많은데, 이 부분이 필요한 코드를 실행할 일이 있다면 이 단계에서 설치해 주는 것이 좋다.</li>
    </ol>
  </li>
  <li><strong>cuDNN 설치</strong>
    <ol>
      <li>우선 <a href="https://developer.nvidia.com/rdp/cudnn-archive">cudnn-archive</a>에서 사용하고자 하는 CUDA에 맞는 버전을 찾아 다운받는다.</li>
      <li>윈도우의 경우 압축을 풀어 CUDA 설치 폴더(<code class="highlighter-rouge">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1</code>)에 붙여넣기 하면 된다. 폴더 경로는 설치한 CUDA 버전에 따라 달라진다.</li>
      <li>Ubuntu 등의 경우는 홈페이지에 명시된 절차를 따르도록 하자.</li>
    </ol>
  </li>
  <li><strong>환경변수 등록</strong>
    <ol>
      <li>윈도우의 경우 <code class="highlighter-rouge">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin</code>을 등록하자.
        <ul>
          <li>실행이 잘 안 되는 경우 상위 또는 하위 폴더 몇 개를 추가 등록하면 되는 경우도 있다.</li>
        </ul>
      </li>
      <li>Ubuntu 등의 경우는 다음과 비슷하다. 자신의 OS에 맞춰서 구글링하자.
        <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> export PATH=/usr/local/cuda-10.1/bin${PATH:+:${PATH}}
 export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
  <li><strong>설치 확인</strong>
    <ol>
      <li>다음 코드를 python을 실행하여 입력해보고 <code class="highlighter-rouge">True</code>가 뜨면 성공한 것이다.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">import</span> <span class="nn">torch</span>
 <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span>
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h2 id="pytorch-project-구조">PyTorch Project 구조</h2>

<p>프로젝트의 구조는 코딩하는 사람 마음대로이긴 하나, 기본적으로는 다음과 같은 구조를 따른다.</p>

<ol>
  <li>Set HyperParameter and Run</li>
  <li>Load Data</li>
  <li>Define and Load Models
3-1. Define util functions</li>
  <li>Set Loss function(creterion) and Optimizer</li>
  <li>Train Model</li>
  <li>Visualize and save results</li>
</ol>

<p>PyTorch는 각 단계에서 다음의 장점을 갖는다.</p>
<ol>
  <li>PyTorch가 아닌 Python의 특징인데, 여러분은 많은 Machine Learning 코드를 보면서 <code class="highlighter-rouge">python train.py --epochs 50 --batch-size 16</code> 등 많은 옵션을 설정할 수 있는 것을 보았을 것이다. Python의 <code class="highlighter-rouge">argparse</code> 패키지는 이것을 가능하게 해 준다.</li>
  <li>데이터 로드 시 <code class="highlighter-rouge">DataLoader</code>라는 클래스를 제공한다. <code class="highlighter-rouge">DataLoader</code>를 통해 데이터를 불러오면, 이 안에서 데이터 처리에 대한 거의 모든 것을 쉽게 수행할 수 있다.
    <ul>
      <li>이를테면 Data Augmentation 같은 것도 전부 제공된다.</li>
      <li>여러 종류의 Data Transformation이 지원된다.</li>
    </ul>
  </li>
  <li>일반적인 모델을 불러올 때는 다른 Deep Learning Framework도 대체로 간결하지만, PyTorch는 <code class="highlighter-rouge">torchvision</code>이라는 패키지에서 따로 pretrain까지 된 모델들을 제공하므로 다른 곳에서 모델을 다운로드할 필요 없이 이를 바로 쓸 수 있다.
2-1. 많은 프로그래머들이 <code class="highlighter-rouge">utils.py</code>에 유틸리티 함수(예를 들면 <a href="https://greeksharifa.github.io/paper_review/2018/10/26/YOLOv2/">YOLO</a>에서 <a href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">IoU</a>를 구하는 함수)를 따로 빼내어 여러 가지를 한번에 정의한다. 프로젝트에서 부가적인 부분은 따로 관리하는 것이 가독성이 좋다.</li>
  <li>이 부분은 다른 Deep Learning Framework와 비슷하다.</li>
  <li>Tensorflow와는 달리 Session을 설정할 필요가 없다.</li>
  <li>이 부분도 역시 비슷하다.</li>
</ol>

<hr />

<p><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/">다음 글</a>에서는 Linear Regression Model을 예로 들어서 간단한 프로젝트의 구조를 설명하도록 하겠다.</p>

<hr />

<h2 id="references">References</h2>

<ul>
  <li><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-00-references/">Reference</a></li>
</ul>

<p>PyTorch에서 자주 사용되는 함수들을 정리한 글이다.</p>

    </article>
    <div class="post-more">
      
      <a href="/pytorch/2018/11/02/pytorch-usage-01-introduction/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/pytorch/2018/11/02/pytorch-usage-01-introduction/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page10">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/page8">Newer</a>
    
  
</div>


  </div>
</div>

<label for="sidebar-checkbox" class="sidebar-toggle"></label>

<script>
  (function (document) {
    let toggle = document.querySelector('.sidebar-toggle');
    let sidebar = document.querySelector('#sidebar');
    let checkbox = document.querySelector('#sidebar-checkbox');

    document.addEventListener('click', function (e) {
      let target = e.target;

      if (target === toggle) {
        checkbox.checked = !checkbox.checked;
        e.preventDefault();
      } else if (checkbox.checked && !sidebar.contains(target)) {
        /* click outside the sidebar when sidebar is open */
        checkbox.checked = false;
      }
    }, false);
  })(document);
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    };
    i[r].l = 1 * new Date();
    a = s.createElement(o);
    m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-00000000-1', 'auto');
  ga('send', 'pageview');
</script>


<!-- Naver Analytics -->	
<script type="text/javascript" src="//wcs.naver.net/wcslog.js"></script>
<script type="text/javascript">
  if(!wcs_add) var wcs_add = {};
    wcs_add["wa"] = "18cbce78e94161";
  wcs_do();
</script>

</body>

<script id="dsq-count-scr" src="//greeksharifa-github-io.disqus.com/count.js" async></script>

</html>
