<!DOCTYPE html>
<html lang="en-us">
<head>
  <head>
  <!-- Description of Blog -->
  <meta name="description" content="Python, Machine & Deep Learning">
  <link rel="canonical" href="https://greeksharifa.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Python, Machine & Deep Learning">
  <meta property="og:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta property="og:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta property="og:url" content="https://greeksharifa.github.io/">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python, Machine & Deep Learning">
  <meta name="twitter:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta name="twitter:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta name="twitter:domain" content="https://greeksharifa.github.io/">

  <!-- link -->
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="icon-144x144" sizes="144x144" href="/public/img/icon-144x144.png">
  <link rel="shortcut icon" href="/public/img/icon_32x32.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  

  <!-- Ads -->
  <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
  </script>
</head>

  <!-- for Google AdSense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9951774327887666",
    enable_page_level_ads: true
  });
</script>

  <style>blockquote {
    font-size: 1em;
    line-height: 1.4
  }</style>
  <link href='http://fonts.googleapis.com/css?family=Gill+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Consolas' rel='stylesheet' type='text/css'>
</head>
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/3c2986ad7ac1f2230ea3596f44563328">
          <img src="/public/img/maple_tree.jpg" title="Cover Photo" alt="Maple tree" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>Developer and Analyst</strong>, YW & YY.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/greeksharifa">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:greeksharifa@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://greeksharifa.github.io/">
          Github Project
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 YW & YY. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://greeksharifa.github.io">YW & YY</a>
    </p>
  </div>
</div>


<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
     content to avoid any CSS collisions with our real content. -->
<div class="wrap">
  <div class="masthead">
    <div class="container">
      <h3 class="masthead-title" align="center">
        <a href="/" title="Home" title="YW & YY">
          <img class="masthead-logo" src="/public/img/logo.png"/>
        </a>
        <small>YW & YY's Python, Machine & Deep Learning</small>
        <!-- HTML elements for search -->
        <a href="/search/" id="search_icon">
          <img src="/public/img/search.png" width="25" height="25"
               align="right" style="margin-top:5px; margin-bottom:0;"
               onmouseover="this.style.opacity=0.7" onmouseout="this.style.opacity=0.5"
               alt="search">
        </a>
      </h3>
    </div>
  </div>

  <div class="container content">
    <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/pytorch-usage-00-references/">
        PyTorch 사용법 - 00. References
      </a>
    </h1>

    <span class="post-date">02 Nov 2018</span>
     |
    
    <a href="/blog/tags/#pytorch" class="post-tag">PyTorch</a>
    
    <a href="/blog/tags/#usage" class="post-tag">usage</a>
    
    

    <article>
      <hr />

<p><strong><a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-00-references/">PyTorch 사용법 - 00. References</a></strong><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-01-introduction/">PyTorch 사용법 - 01. 소개 및 설치</a><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/02/pytorch-usage-02-Linear-Regression-Model/">PyTorch 사용법 - 02. Linear Regression Model</a><br />
<a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/">PyTorch 사용법 - 03. How to Use PyTorch</a><br />
<a href="https://greeksharifa.github.io/pytorch/2019/06/12/pytorch-usage-04-RNN-Model/">PyTorch 사용법 - 04. Recurrent Neural Network Model</a></p>

<hr />

<p>본 글의 일부 예제는 <a href="https://pytorch.org/docs/stable/index.html">Pytorch Documentation</a>에서 가져왔음을 밝힙니다.</p>

<hr />

<h2 id="데이터-타입dtype"><a href="https://pytorch.org/docs/stable/tensor_attributes.html?highlight=dtype#torch.torch.dtype">데이터 타입(dtype)</a></h2>

<p>모든 텐서는 기본적으로 dtype을 갖고 있다. 데이터 타입(dtype)이란 데이터가 정수형인지, 실수형인지, 얼마나 큰 범위를 가질 수 있는지 등을 나타낸다.<br />
종류는 아래 표와 같다.</p>

<table>
  <thead>
    <tr>
      <th>Data type</th>
      <th>dtype</th>
      <th>CPU tensor</th>
      <th>GPU tensor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>32-bit floating point</td>
      <td>torch.float32 or torch.float</td>
      <td>torch.FloatTensor</td>
      <td>torch.cuda.FloatTensor</td>
    </tr>
    <tr>
      <td>64-bit floating point</td>
      <td>torch.float64 or torch.double</td>
      <td>torch.DoubleTensor</td>
      <td>torch.cuda.DoubleTensor</td>
    </tr>
    <tr>
      <td>16-bit floating point</td>
      <td>torch.float16 or torch.half</td>
      <td>torch.HalfTensor</td>
      <td>torch.cuda.HalfTensor</td>
    </tr>
    <tr>
      <td>8-bit integer (unsigned)</td>
      <td>torch.uint8</td>
      <td>torch.ByteTensor</td>
      <td>torch.cuda.ByteTensor</td>
    </tr>
    <tr>
      <td>8-bit integer (signed)</td>
      <td>torch.int8</td>
      <td>torch.CharTensor</td>
      <td>torch.cuda.CharTensor</td>
    </tr>
    <tr>
      <td>16-bit integer (signed)</td>
      <td>torch.int16 or torch.short</td>
      <td>torch.ShortTensor</td>
      <td>torch.cuda.ShortTensor</td>
    </tr>
    <tr>
      <td>32-bit integer (signed)</td>
      <td>torch.int32 or torch.int</td>
      <td>torch.IntTensor</td>
      <td>torch.cuda.IntTensor</td>
    </tr>
    <tr>
      <td>64-bit integer (signed)</td>
      <td>torch.int64 or torch.long</td>
      <td>torch.LongTensor</td>
      <td>torch.cuda.LongTensor</td>
    </tr>
  </tbody>
</table>

<p>사용법은 어렵지 않다. 텐서 생성시 <code class="highlighter-rouge">dtype=torch.float</code>과 같이 parameter를 지정해 주기만 하면 된다.</p>

<hr />

<h2 id="tensor-creation">Tensor Creation</h2>

<h3 id="torcharange"><a href="https://pytorch.org/docs/stable/torch.html?highlight=arange#torch.arange">torch.arange</a></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.arange(start=0, end, step=1, out=None, dtype=None, 
#              layout=torch.strided, device=None, requires_grad=False) → Tensor
</span></code></pre></div></div>

<p>start 이상 end 미만까지 step 간격으로 dtype 타입인 1차원 텐서를 <strong>생성</strong>한다.</p>

<p><code class="highlighter-rouge">out</code> parameter로 결과 텐서를 저장할 변수(텐서)를 지정할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="torchlinspace"><a href="https://pytorch.org/docs/stable/torch.html?highlight=linspace#torch.linspace">torch.linspace</a></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.linspace(start, end, steps=100, out=None, dtype=None, 
#                layout=torch.strided, device=None, requires_grad=False) → Tensor
</span></code></pre></div></div>

<p>start 이상 end 미만까지 총 steps 개수의 dtype 타입인 1차원 텐서를 <strong>생성</strong>한다.<br />
<strong>torch.arange</strong>에서 step은 간격을, <strong>torch.linspace</strong>에서 steps는 개수를 의미한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">10.</span><span class="p">,</span>  <span class="o">-</span><span class="mf">5.</span><span class="p">,</span>   <span class="mf">0.</span><span class="p">,</span>   <span class="mf">5.</span><span class="p">,</span>  <span class="mf">10.</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">1.1111</span><span class="p">,</span>  <span class="mf">2.2222</span><span class="p">,</span>  <span class="mf">3.3333</span><span class="p">,</span>  <span class="mf">4.4444</span><span class="p">,</span>  
         <span class="mf">5.5556</span><span class="p">,</span>  <span class="mf">6.6667</span><span class="p">,</span>  <span class="mf">7.7778</span><span class="p">,</span>  <span class="mf">8.8889</span><span class="p">,</span> <span class="mf">10.0000</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="torchfrom_numpy"><a href="https://pytorch.org/docs/stable/torch.html?highlight=from_numpy#torch.from_numpy">torch.from_numpy</a></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.from_numpy(ndarray) → Tensor
</span></code></pre></div></div>

<p>numpy array인 ndarray로부터 텐서를 만든다. 이 함수는 데이터를 <strong>복사가 아닌 참조</strong>를 한다.<br />
<code class="highlighter-rouge">from_numpy</code>로 만들어진 텐서는 해당 ndarray와 메모리를 공유하며, 어느 한쪽의 데이터를 변경 시 둘 다 변경된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="torchrandn"><a href="https://pytorch.org/docs/stable/torch.html?highlight=randn#torch.randn">torch.randn</a></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.randn(*sizes, out=None, dtype=None, 
#             layout=torch.strided, device=None, requires_grad=False) → Tensor
</span></code></pre></div></div>

<p>N(0, 1) 정규분포를 따르는 sizes 크기의 텐서를 <strong>생성</strong>한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">1.5954</span><span class="p">,</span>  <span class="mf">2.8929</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0923</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.1719</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4709</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1996</span><span class="p">]])</span>
</code></pre></div></div>

<hr />

<h2 id="tensor-reshape">Tensor Reshape</h2>

<h3 id="torchunsqueezetensorunsqueeze"><a href="https://pytorch.org/docs/stable/torch.html#torch.unsqueeze">torch.unsqueeze(Tensor.unsqueeze)</a></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.unsqueeze(input, dim, out=None) → Tensor
</span></code></pre></div></div>

<p><code class="highlighter-rouge">dim</code> parameter 위치에 길이 1짜리 차원을 추가한 텐서를 만든다. 이 함수는 데이터를 <strong>복사가 아닌 참조</strong>를 한다. 원본 텐서와 메모리를 공유하며, 어느 한쪽의 데이터를 변경 시 둘 다 변경된다.</p>

<p><code class="highlighter-rouge">dim</code>은 [ -input.dim() - 1, input.dim() + 1] 범위를 갖는다. 음수 dim은 dim + input.dim() + 1과 같다.<br />
원본 텐서의 size가 (2, 3, 4)라면, unsqueeze(1) 버전은 (2, 1, 3, 4), unsqueeze(2) 버전은 (2, 3, 1, 4)이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
<span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">]),</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="tensor-operation">Tensor Operation</h2>

<h3 id="torchcat"><a href="https://pytorch.org/docs/stable/torch.html?highlight=cat#torch.cat">torch.cat</a></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># torch.cat(seq, dim=0, out=None) → Tensor
</span></code></pre></div></div>

<p>두 텐서를 이어 붙인다(concatenate). 데이터를 <strong>복사</strong>한다.<br />
concatenate하는 차원을 제외하고는 size가 같거나 empty여야 한다. 즉 shape=(2, 3, 4)인 텐서는 shape=(2, 1, 4)와는 <code class="highlighter-rouge">dim=1</code>일 때만 concatenate가 가능하다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">104</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">101</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">102</span><span class="p">,</span> <span class="mi">103</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span>  <span class="mi">0</span><span class="p">,</span>   <span class="mi">1</span><span class="p">,</span>   <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">101</span><span class="p">],</span>
        <span class="p">[</span>  <span class="mi">3</span><span class="p">,</span>   <span class="mi">4</span><span class="p">,</span>   <span class="mi">5</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">103</span><span class="p">]])</span>
</code></pre></div></div>

<h3 id="torchtensorbackward"><a href="https://pytorch.org/docs/stable/autograd.html?highlight=backward#torch.Tensor.backward">torch.Tensor.backward</a></h3>

<hr />

<h2 id="torchnn">torch.nn</h2>

<h3 id="torchnnlinear"><a href="https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear">torch.nn.Linear</a></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># class torch.nn.Linear(in_features, out_features, bias=True)
</span></code></pre></div></div>

<p>Linear 모델 클래스를 생성한다.<br />
<code class="highlighter-rouge">in_features</code> 길이의 데이터를 Linear Transformation을 통해 <code class="highlighter-rouge">out_features</code> 길이의 데이터로 변환할 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
<span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.3469</span><span class="p">,</span>  <span class="mf">0.1542</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4830</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2903</span><span class="p">,</span>  <span class="mf">0.4949</span><span class="p">,</span>  <span class="mf">0.4592</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
<span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">0.0965</span><span class="p">,</span>  <span class="mf">0.5427</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="torchnnmseloss"><a href="https://pytorch.org/docs/stable/nn.html?highlight=mseloss#torch.nn.MSELoss">torch.nn.MSELoss</a></h3>

<hr />

<h2 id="torchoptim">torch.optim</h2>

<h3 id="torchoptimadam"><a href="https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam">torch.optim.Adam</a></h3>

<h3 id="torchoptimoptimizerzero_grad"><a href="https://pytorch.org/docs/stable/optim.html?highlight=zero_grad#torch.optim.Optimizer.zero_grad">torch.optim.Optimizer.zero_grad</a></h3>

<h3 id="torchoptimoptimizerstep"><a href="https://pytorch.org/docs/stable/optim.html?highlight=optimizer%20step#torch.optim.Optimizer.step">torch.optim.Optimizer.step</a></h3>

<hr />

<h2 id="save-and-load">Save and Load</h2>

<h3 id="torchsave"><a href="https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save">torch.save</a></h3>

<hr />

<center><img src="/public/img/Andre_Derain_Fishing_Boats_Collioure.jpg" width="50%" /></center>

<p><img src="/public/img/Andre_Derain_Fishing_Boats_Collioure.jpg" alt="01_new_repository" /></p>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/pytorch-usage-00-references/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/pytorch-usage-00-references/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/RC-task/">
        RC task using CNN/Daily dataset
      </a>
    </h1>

    <span class="post-date">01 Nov 2018</span>
     |
    
    <a href="/blog/tags/#nlp" class="post-tag">NLP</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <h3 id="a-thorough-examination-of-the-cnndaily-mail-reading-comprehension-task">A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task</h3>
<blockquote>
  <p>본 글은 Danqi Chen, Jason Bolton, Christopher D. Manning가 2016년에 Publish한 위 논문을 리뷰한 것이다.</p>
</blockquote>

<h3 id="introduction">Introduction</h3>
<p>본 논문은 CNN/Daily Mail 데이터셋의 심도 있는 분석을 목적으로 하며, 어떠한 수준의 자연어가 요구되는지를 파악하는 것을 목적으로 한다.</p>

<h3 id="rc-task-and-systems">RC Task and systems</h3>
<p>데이터셋은 아래와 같은 3가지의 구성 요소를 지닌다.</p>

<table>
  <thead>
    <tr>
      <th>Element</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>passage</td>
      <td>new article</td>
    </tr>
    <tr>
      <td>question</td>
      <td>close_style task</td>
    </tr>
    <tr>
      <td>answer</td>
      <td>question entity</td>
    </tr>
  </tbody>
</table>

<p>한 passage의 단어들을 d차원으로 임베딩하여 m개의 token을 형성하였다고 할 때,</p>

<script type="math/tex; mode=display">p = {p_1, p_2, ..., p_m}, q = {q_1, ..., q_l}</script>

<p>q의 경우 오직 하나의 “placeholder” token을 가진다.<br />
목적은 결국 p와 E(모든 abstract entity markers의 집합)의 교집합에 속하는 a를 찾는 것인데,<br />
이는 answer(답)가 named entity 리스트에서 선택된다는 것을 의미한다.</p>

<p>본 논문의 모델은 Hermann의 Attentive Model을 기반으로 하며 일부 수정을 기하였다.<br />
대략적인 구조는 아래 그림과 같다.</p>

<center><img src="/public/img/Paper_Review/2018-11-01-RC-task/01.jpg" width="100%" /></center>

<p>d차원으로 임베딩 된 $ p_i $, $ q_j $ 벡터들은 (i = 1~m, j = 1~l) 각각 Bidirectional RNN에 인풋으로 투입된다. hidden state들을 수직으로 concat한 것을 $ \tilde{p_i} $라고 한며 이 벡터의 차원은 h이다. q역시 마찬가지로 h차원이다.</p>

<p>기본 구조는 GRU를 사용하였다. (계산 심플)</p>

<p>Attention의 경우 아래와 같이 계산된다.</p>

<center><img src="/public/img/Paper_Review/2018-11-01-RC-task/02.jpg" width="50%" /></center>

<p>attention weight인 $ \alpha_i $ 는 스칼라이며, contextual embedding vector인 $ \tilde{p_i} $ 과 q의 관련 정도를 의미한다. 이 값이 클 수록 질문과 paragraph의 관련성이 강하다는 것을 의미한다.</p>

<p><strong>output vector</strong>인 <strong>o</strong>는 (n, 1) 벡터이며 모든 contextual embedding vectors의 가중합으로 생각하면 된다.</p>

<p>이를 통해 최종적으로 아래와 같이 답을 계산한다. 손실함수는 NLL을 사용한다.</p>

<center><img src="/public/img/Paper_Review/2018-11-01-RC-task/03.jpg" width="50%" /></center>

<p>Hermann의 모델과의 차이점은 아래와 같다.</p>
<ol>
  <li>Bilinear Term을 사용하여 q와 p사이의 상호작용을 명확히 했다.</li>
  <li>output vector를 직접적으로 예측에 사용하였다.</li>
  <li>모든 단어를 후보로 두지 않고 entities 중 passage에 나타나는 (p와 E의 교집합) 단어들만 후보로 두었다.</li>
</ol>

<h3 id="training">Training</h3>
<p>빈번하게 등장하는 5만개의 단어만을 사용하였고, 임베딩 차원이 d=100인 GloVe를 사용하였다.<br />
attention과 output 모수들은 균일분포로 초기화되었고, GRU weights은 정규분포로 초기화 되었다.</p>

<p>hidden size는 128과 256을 사용하였고, SGD 알고리즘을 32 배치사이즈로서 사용하였다.<br />
0.2의 dropout을 임베딩 layer에 사용하였고, gradient의 norm이 10을 넘을 경우 gradient clipping을 사용하였다.</p>


    </article>
    <div class="post-more">
      
      <a href="/RC-task/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/RC-task/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/Self-Attention/">
        Self Attention
      </a>
    </h1>

    <span class="post-date">29 Oct 2018</span>
     |
    
    <a href="/blog/tags/#nlp" class="post-tag">NLP</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <h3 id="a-structured-self-attentive-sentence-embedding">A Structured Self-Attentive Sentence Embedding</h3>
<blockquote>
  <p>본 글은 Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, Yoshua Bengio가 2017년에 Publish한 위 논문을 리뷰한 것이다.<br />
특별히 본 글은 Presentation에 사용된 PDF 발표자료로 대체하겠다.</p>
</blockquote>

<center><img src="/public/img/Paper_Review/2018-10-29-Self-Attention/01.jpg" width="100%" /></center>
<center><img src="/public/img/Paper_Review/2018-10-29-Self-Attention/02.jpg" width="100%" /></center>
<center><img src="/public/img/Paper_Review/2018-10-29-Self-Attention/03.jpg" width="100%" /></center>
<center><img src="/public/img/Paper_Review/2018-10-29-Self-Attention/04.jpg" width="100%" /></center>
<center><img src="/public/img/Paper_Review/2018-10-29-Self-Attention/05.jpg" width="100%" /></center>
<center><img src="/public/img/Paper_Review/2018-10-29-Self-Attention/06.jpg" width="100%" /></center>
<center><img src="/public/img/Paper_Review/2018-10-29-Self-Attention/07.jpg" width="100%" /></center>

    </article>
    <div class="post-more">
      
      <a href="/Self-Attention/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/Self-Attention/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/YOLOv2/">
        YOLOv2
      </a>
    </h1>

    <span class="post-date">26 Oct 2018</span>
     |
    
    <a href="/blog/tags/#cnn" class="post-tag">CNN</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <h3 id="you-only-look-once-unified-real-time-object-detection">You Only Look Once: Unified, Real-Time Object Detection</h3>
<blockquote>
  <p>본 글은 Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi가 2016년에 Publish한 위 논문을 리뷰한 것이며, 추가적으로 구현을 위한 Loss Function 설명과 코드를 첨부하였다.</p>
</blockquote>

<h3 id="unified-detection">Unified Detection</h3>
<p>YOLO version2는 기본적으로 end-to-end training 방법을 취하고 있으며 학습이 완료되었을 때 실시간으로
테스트가 가능할 만큼 빠른 속도를 보이는 강점을 갖고 있다.<br />
본 글은 416 X 416 이미지를 기준으로 설명을 진행하도록 하겠다.<br />
YOLO의 핵심은 이미지를 Grid Cell로 나누어 각각의 Cell이 Object Detection을 위한 정체성을 갖게끔 만든다는 것이다.<br />
예를 들어 416 X 416의 이미지는 아래와 같은 Darknet이라는 CNN 구조를 거치게 된다.<br />
(참고로 아래는 Pytorch 기준으로 Channels_first를 적용하였다.)</p>

<table>
  <thead>
    <tr>
      <th>layer</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>[None, 3, 416, 416]</td>
    </tr>
    <tr>
      <td>1</td>
      <td>[None, 32, 208, 208]</td>
    </tr>
    <tr>
      <td>2</td>
      <td>[None, 64, 104, 104]</td>
    </tr>
    <tr>
      <td>5</td>
      <td>[None, 128, 52, 52]</td>
    </tr>
    <tr>
      <td>8</td>
      <td>[None, 256, 26, 26]</td>
    </tr>
    <tr>
      <td>13</td>
      <td>[None, 512, 13, 13]</td>
    </tr>
    <tr>
      <td>18</td>
      <td>[None, 1024, 13, 13]</td>
    </tr>
    <tr>
      <td>19</td>
      <td>[None, 1024, 13, 13]</td>
    </tr>
    <tr>
      <td>20</td>
      <td>[None, 1024, 13, 13]</td>
    </tr>
    <tr>
      <td>skip:</td>
      <td>[None, 64, 26, 26] -&gt;</td>
    </tr>
    <tr>
      <td>skip:</td>
      <td>[None, 256, 13, 13]</td>
    </tr>
    <tr>
      <td>21</td>
      <td>[None, 1280, 13, 13]</td>
    </tr>
    <tr>
      <td>22</td>
      <td>[None, 1024, 13, 13]</td>
    </tr>
    <tr>
      <td>23</td>
      <td>[None, 35, 13, 13]</td>
    </tr>
  </tbody>
</table>

<p>Output으로 나온 [35, 13, 13]에서 13 X 13은 Grid의 size이다. 35는 추후에 설명하겠다.</p>

<center><img src="/public/img/Paper_Review/2018-10-26-YOLOv2/01.jpg" width="60%" /></center>

<blockquote>
  <blockquote>
    <p>출처: https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/</p>
  </blockquote>
</blockquote>

<p>위 그림과 같이 정리된 13 X 13 = 169개의 Cell은 이제 각각 Object을 detect하기 위한 정체성을 갖게 된다. 만일 실제(Ground-truth: GT) Object의 <strong>중심 좌표(center coordinate)</strong>가 Cell 내부에 위치한다면, 그 Cell은 해당 Object를 detect할 책임을 지게 되는 것이다.<br />
(If the center of an object falls into a grid cell, that grid cell is reponsible for detecting that object)</p>

<p>각각의 Grid Cell은 이제 5개의 bbox를 예측하게 되고, 각각의 box에 대해 confidence score를 계산하게 된다. 5개는 YOLOv2에서 정한 숫자이고, YOLOv3에선 총 9개가 등장하게 된다.</p>

<p>자세한 설명을 위해 35라는 숫자에 대해 부연 설명을 하도록 하겠다.</p>

<script type="math/tex; mode=display">35 = 5 * (1 + 4 + 2)</script>

<p>5 = bbox 개수<br />
1 = box_confidence = P(Object) * IOU_truth_pred<br />
4 = boxes = box coordinates (bounding box 좌표 4개: x, y, w, h)<br />
2 = box_class_probs (예측하고자 하는 class의 개수와 길이가 같다.)</p>

<p>box_confidence는 그 Cell에 Object가 있을 확률에 IOU_truth_pred를 곱하게 되는데, P(Object)는 당연히 0 또는 1이다. 이 값에 GT_bbox(truth)와 pred_bbox(pred)의 IOU를 계산하여 곱해주면 box_confidence가 되는 것이다. P(Object)가 0일 경우 이 값은 물론 0이 된다.</p>

<p>boxes의 경우 bbox 좌표를 뜻하는데, 후에 IOU를 계산할 때에는 이와 같이 중심 좌표(x, y)와 box 길이(w, h)를 기준으로 계산하는 것이 불편하기 때문에 왼쪽 상단 좌표(x1, y1)과 오른쪽 하단 좌표(x2, y2)로 고쳐주도록 한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">box_to_corner</span><span class="p">(</span><span class="n">box1</span><span class="p">,</span> <span class="n">box2</span><span class="p">):</span>
    <span class="s">"""
    abs_coord 형식인 bbox의 [x, y, w, h]를 [x1, y1, x2, y2]로 변환한다.
    :param box1: [..., 4]
    :param box2: [..., 4]
    :return: [..., 1] X 8
    """</span>
    <span class="n">b1_x</span><span class="p">,</span> <span class="n">b1_y</span><span class="p">,</span> <span class="n">b1_w</span><span class="p">,</span> <span class="n">b1_h</span> <span class="o">=</span> <span class="n">box1</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">box1</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">box1</span><span class="p">[...,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">box1</span><span class="p">[...,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="n">b2_x</span><span class="p">,</span> <span class="n">b2_y</span><span class="p">,</span> <span class="n">b2_w</span><span class="p">,</span> <span class="n">b2_h</span> <span class="o">=</span> <span class="n">box2</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">box2</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">box2</span><span class="p">[...,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">box2</span><span class="p">[...,</span> <span class="mi">3</span><span class="p">]</span>

    <span class="n">b1_x1</span><span class="p">,</span> <span class="n">b1_x2</span> <span class="o">=</span> <span class="n">b1_x</span> <span class="o">-</span> <span class="n">b1_w</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">b1_x</span> <span class="o">+</span> <span class="n">b1_w</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">b1_y1</span><span class="p">,</span> <span class="n">b1_y2</span> <span class="o">=</span> <span class="n">b1_y</span> <span class="o">-</span> <span class="n">b1_h</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">b1_y</span> <span class="o">+</span> <span class="n">b1_h</span><span class="o">/</span><span class="mi">2</span>

    <span class="n">b2_x1</span><span class="p">,</span> <span class="n">b2_x2</span> <span class="o">=</span> <span class="n">b2_x</span> <span class="o">-</span> <span class="n">b2_w</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">b2_x</span> <span class="o">+</span> <span class="n">b2_w</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">b2_y1</span><span class="p">,</span> <span class="n">b2_y2</span> <span class="o">=</span> <span class="n">b2_y</span> <span class="o">-</span> <span class="n">b2_h</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">b2_y</span> <span class="o">+</span> <span class="n">b2_h</span><span class="o">/</span><span class="mi">2</span>

    <span class="k">return</span> <span class="n">b1_x1</span><span class="p">,</span> <span class="n">b1_x2</span><span class="p">,</span> <span class="n">b1_y1</span><span class="p">,</span> <span class="n">b1_y2</span><span class="p">,</span> <span class="n">b2_x1</span><span class="p">,</span> <span class="n">b2_x2</span><span class="p">,</span> <span class="n">b2_y1</span><span class="p">,</span> <span class="n">b2_y2</span>
</code></pre></div></div>

<p>참고로 현재 https://github.com/KU-DIA/BasicNet에서 관련 코드를 확인할 수 있다.</p>

<blockquote>
  <blockquote>
    <p>이제 box_class_probs를 보면 이 값은 P(Class_i | Object)를 뜻하는데, Object가 있을 경우 i번째 Class일 조건부 확률을 뜻한다.
사실 이 값만으로는 추후 과정 진행이 어렵기 때문에 위에서 구한 box_confidence와 이 box_class_probs를 브로드캐스팅 곱을 통해 계산해주면,</p>
  </blockquote>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">class_scores</span> <span class="o">=</span> <span class="n">box_confidence</span> <span class="o">*</span> <span class="n">box_class_probs</span>
             <span class="o">=</span> <span class="n">P</span><span class="p">(</span><span class="n">Object</span><span class="p">)</span> <span class="o">*</span> <span class="n">IOU</span> <span class="o">*</span> <span class="n">P</span><span class="p">(</span><span class="n">Class_i</span> <span class="o">|</span> <span class="n">Object</span><span class="p">)</span>
             <span class="o">=</span> <span class="n">P</span><span class="p">(</span><span class="n">Class_i</span><span class="p">)</span> <span class="o">*</span> <span class="n">IOU</span>
</code></pre></div></div>

<p>위와 같이 class_scores를 구할 수 있다.<br />
이 class_scores 텐서는 본인이 설정한 Class 수만큼의 길이를 가지는데(정확히 “길이”는 아니지만), 본 예에서는 2로 설정하였기 때문에 앞으로도 2라는 숫자를 계속 사용하도록 하겠다.</p>

<p>이 class_scores는 각각의 box가 가지는 class-specific confidence score를 의미하게 된다. 만약 설정한 Class를 사람, 고양이라고 한다면, 각 class_scores 텐서는 그 Cell이 “사람”을 detect할 확률, “고양이”를 detect할 확률을 담고 있는 것이다.</p>

<p>이후에 여러 과정이 추가되기는 하지만, 본질적으로 이렇게 표현된 class_scores에서 가장 높은 값을 갖는 class_index를 찾아 output으로 반환하게 된다.</p>

<p>다시 위로 돌아가서 <strong>[None, 35, 13, 13]</strong> 구조에서, 35는 5 X 7이라는 것을 확인하였다.<br />
바로 위에서 7은 1(box_confidence), 4(bbox 좌표), 2(box_class_probs)로 분해되는 것을 보았는데,<br />
1과 2가 브로드캐스팅 곱을 통해 길이 2의 class_scores로 정리되었다.<br />
위 class_scores는 cell 기준으로 존재하는데, cell 하나당 5개의 bbox를 갖고, 이러한 cell은 총 13 X 13개 있으므로, 이제 우리는 13 X 13 X 5개의 class_scores 텐서를 갖게 되었다.</p>

<p>그런데 그냥 이런식으로 진행하게 되면, 845개의 텐서가 등장하는데, 너무 많다.<br />
이제부터는 텐서의 수를 효과적으로 줄여 학습을 준비하는 과정에 대해 설명하겠다.<br />
사실 아예 삭제하는 것은 아니고, 필요없는 텐서의 값들을 죄다 0으로 바꿔주는 작업이다.</p>

<ol>
  <li>filter_by_confidence
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">filter_by_confidence</span><span class="p">(</span><span class="n">confidence</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">class_probs</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.6</span><span class="p">):</span>
 <span class="s">"""
 confidence가 threshold보다 낮은 경우 제거해준다.
 남은 confidence와 class_probs를 곱하여 class_scores를 생성한다.
 :param confidence: (None, 1)
 :param boxes: (None, 4)
 :param class_probs: (None, C)
 :param threshold: 필터링 threshold
 """</span>
 <span class="n">confidence</span><span class="p">[</span><span class="n">confidence</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
    
 <span class="p">(</span><span class="n">하략</span><span class="p">:</span> <span class="n">class_scores를</span> <span class="n">계산하여</span> <span class="n">반환함</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>용어를 정리하자면, confidence = box_confidence, class_probs = box_class_probs이다.<br />
위 함수는 일정 threshold보다 작은 box_confidence를 갖는 box_confidence를 아예 0으로 바꿔준다.<br />
왜냐하면 P(Object)가 0에 근접할 경우, background(Object가 없음)라는 의미인데, 이들의 bbox를 찾는 것은 의미가 없기 때문이다.</p>

<ol>
  <li>Non_max_suppression
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">nms</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">class_scores</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.6</span><span class="p">):</span>
 <span class="s">"""
 :param boxes: bbox 좌표, (None, 4)
 :param class_scores: confidence * class_prob_scores, 클래스 별 score, (None, C)
 :param threshold: NMS Threshold
 """</span>

 <span class="k">for</span> <span class="n">class_number</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
     <span class="n">target</span> <span class="o">=</span> <span class="n">class_scores</span><span class="p">[...,</span> <span class="n">class_number</span><span class="p">]</span>

     <span class="c1"># 현재 class 내에서 class_score를 기준으로 내림차순으로 정렬한다.
</span>     <span class="n">sorted_class_score</span><span class="p">,</span> <span class="n">sorted_class_index</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

     <span class="c1"># idx: 아래 bbox_max의 Index
</span>     <span class="c1"># bbox_max_idx: 정렬된 class_scores를 기준으로 가장 큰 score를 가지는 bbox Index
</span>     <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">bbox_max_idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">sorted_class_index</span><span class="p">.</span><span class="n">numpy</span><span class="p">())):</span>
         <span class="c1"># 기준 class_score가 0이라면 비교할 필요가 없다.
</span>         <span class="c1"># 아래 threshold 필터링에서 0으로 바뀐 값이기 때문이다.
</span>         <span class="k">if</span> <span class="n">class_scores</span><span class="p">[</span><span class="n">bbox_max_idx</span><span class="p">,</span> <span class="n">class_number</span><span class="p">]</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">:</span>
             <span class="c1"># 0이 아니라면 순서대로 criterion_box(bbox_max)로 지정된다.
</span>             <span class="n">bbox_max</span> <span class="o">=</span> <span class="n">boxes</span><span class="p">[</span><span class="n">bbox_max_idx</span><span class="p">,</span> <span class="p">:]</span>

             <span class="c1"># criterion_box(bbox_max)가 아닌 다른 box들을 리스트로 미리 지정한다.
</span>             <span class="c1">#others = [index for index in list(sorted_class_index.numpy()) if index != i]
</span>             <span class="n">others</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sorted_class_index</span><span class="p">.</span><span class="n">numpy</span><span class="p">())[</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>

             <span class="c1"># 비교 대상 box들에 대해서
</span>             <span class="c1"># bbox_cur_idx: 비교 대상 bbox Index
</span>             <span class="k">for</span> <span class="n">bbox_cur_idx</span> <span class="ow">in</span> <span class="n">others</span><span class="p">:</span>
                 <span class="n">bbox_cur</span> <span class="o">=</span> <span class="n">boxes</span><span class="p">[</span><span class="n">bbox_cur_idx</span><span class="p">,</span> <span class="p">:]</span>
                 <span class="n">iou</span> <span class="o">=</span> <span class="n">get_iou</span><span class="p">(</span><span class="n">bbox_max</span><span class="p">,</span> <span class="n">bbox_cur</span><span class="p">)</span>
                 <span class="c1"># print(bbox_max_idx, bbox_cur_idx, iou)
</span>
                 <span class="c1"># iou가 threshold를 넘으면 (기준 box와 비교 대상 box가 너무 많이 겹치면)
</span>                 <span class="c1"># 그 해당 box의 현재 class의 class_score를 0으로 만들어 준다.
</span>                 <span class="k">if</span> <span class="n">iou</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
                     <span class="n">class_scores</span><span class="p">[</span><span class="n">bbox_cur_idx</span><span class="p">,</span> <span class="n">class_number</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

 <span class="k">return</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">class_scores</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>사실 Cell 별로 각각 bbox를 5개씩 갖게 되면 인접한 Cell들끼리는 bbox가 마구 겹칠 것이다. 또, bbox의 크기가 충분히 클 경우 이미지 바깥으로 벗어나기도 할 것인데, 한 이미지에 Object 수가 수백개 있는 것이 아닌 이상, 이렇게 많은 bbox는 필요하지 않은 것이 자명하다.</p>

<p>NMS 작업은 이 문제를 효과적으로 해결해준다. 쉽게 말해서 “왕”을 뽑는 느낌인데, 특정 class_scores가 높은 bbox와 과하게 겹치는 (IOU가 높은) 다른 녀석들을 제거하는 것이다.</p>

<p>“사람”을 detect하는 class_scores를 기준으로 class_scores를 내림차순으로 정렬한다. 제일 큰 첫 번째 값과 나머지 값들을 쌍으로 IOU를 계산하여 과하게 겹치는 (기준 threshold)를 넘는 값은 0으로 바꿔준다.</p>

<p>이 과정이 끝나면 [None, 35, 13, 13]이라는 크기 자체는 바뀌진 않지만, 중간중간에 많은 숫자가 0으로 바뀌어 있을 것이다. (1, 2번 기준을 충족하지 못한 값들)<br />
이제 이를 바탕으로 training을 시키면 된다.</p>

<h3 id="training">Training</h3>
<center><img src="/public/img/Paper_Review/2018-10-26-YOLOv2/02.jpg" width="90%" /></center>

<p>위 그림은 YOLOv2의 Loss Function이다. YOLO의 최대 장점은 이렇게 Loss Function을 하나로 통합하여 효과적인 학습을 가능하게 했다는 점이다.</p>

<p>https://curt-park.github.io/2017-03-26/yolo/에서 Loss Function과 관련한 기본적인 설명을 얻을 수 있는데, 추가적으로 설명을 하도록 하겠다.</p>

<p>먼저 1, 2줄은 bbox 좌표에 관한 Loss Function이다. 앞에 있는 $ \lambda_{coord} $는 5로 설정되었다.<br />
$ S^2 $은 Grid Cell의 개수를 의미하며 본 예에서는 13 X 13을 의미한다. $ B $는 정해둔 bbox (anchors) 개수이며 본 예에서는 5를 의미한다. 이렇게 모든 Cell에서 5개 씩의 bbox를 계산하여 GT_bbox와 차이를 좁혀나가는 것이다.</p>

<p>여기서 앵커에 대해 잠깐 설명하자면, 이 앵커는 빠른 학습을 위해 설정된 bbox의 초기값이라고 생각하면 된다. 그냥 무작정 Cell에다가 bbox를 그린다면 그 크기의 편차가 매우 심할 것이다. 미리 object들의 크기를 대략적으로 계산하여 가장 많이 등장할 법한, 가장 유사한 크기의 bbox 크기를 미리 계산해두어 저장한 것이 바로 앵커인데, 이는 보통 Clustering 기법을 통해 미리 계산된다.</p>

<p>이렇게 미리 계산된 앵커를 초기값으로 투입하고, GT_bbox 좌표와의 차이를 빠르게 줄여 나가는 것이 1, 2번째 줄의 목표라고 하겠다.</p>

<p>그리고 $ 1_{i, j}^{obj} $ 라는 Indicator Function의 기능이 매우 중요한데, 이 지시함수는 i번째 Grid Cell에서 j번째 bbox에 Object가 존재할 경우 1이고, 아니면 0이다.</p>

<p>아래의 $ 1<em>{i, j}^{noobj} $ 는 반대의 의미를 가지며, $ 1</em>{i}^{obj} $ 는 오직 Cell 소속 여부와 관련이 있다.</p>

<p>3, 4번째 줄은 Object가 있는지 없는지에 대한 Loss를 계산하게 되고,<br />
5번째 줄은 P(Class_i | Object) = Conditional Class Probability의 Loss를 계산하게 된다.</p>

<h3 id="conclusion">Conclusion</h3>
<p>빠른 속도와 괜찮은 정확도를 가졌지만 YOLOv2의 단점은 작은 물체나 겹치는 물체들을 효과적으로 Localization하지 못한다는 것이다. 이는 version3에서 상당부분 업그레이드 된다.</p>


    </article>
    <div class="post-more">
      
      <a href="/YOLOv2/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/YOLOv2/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/Attention/">
        Attention
      </a>
    </h1>

    <span class="post-date">17 Sep 2018</span>
     |
    
    <a href="/blog/tags/#nlp" class="post-tag">NLP</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <h3 id="neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural Machine Translation by Jointly Learning to Align and Translate</h3>
<blockquote>
  <p>본 글은 Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio가 2014년에 Publish한 위 논문을 리뷰한 것이다.</p>
</blockquote>

<h3 id="introduction">Introduction</h3>
<p>Basic Encoder-Decoder 모델은 source sentence의 모든 정보를 fixed-length vector로 압축하는 방식을 골자로 한다.<br />
그러나 이 모델은 긴 문장을 대상으로 할 때 어려움을 겪는 것이 일반적이다.<br />
이를 해결하기 위해 본 논문에서 제안된 새로운 모델은 <strong>target word</strong>를 예측하는 것과 관련된 source sentence의 부분을 자동적으로 soft-search하여 이와 같은 문제를 해결해낸다.<br />
(learns to align and translate jointly)</p>

<blockquote>
  <p>encodes the input sentence into sequence of vectors and<br />
chooses a subset of these vectors adaptively while decoding the translation.</p>
</blockquote>

<h3 id="background-nmt">Background: NMT</h3>
<p>translation의 핵심은 source sentence <strong>x</strong>가 주어졌을 때의 <strong>y</strong>의 조건부확률을 최대화하는 target sentence <strong>y</strong>를 찾는 것이다.</p>

<script type="math/tex; mode=display">arg\max_{y} p(\vec{y}|x)</script>

<p>번역 모델에 의해 이러한 조건부 분포가 학습되면, source sentence가 주어졌을 때 상응하는 번역된 문장은 위의 조건부 확률을 최대화하는 문장을 찾음으로써 generate된다.</p>

<p><em>RNN Encoder-Decoder</em>는 이전 리뷰에서 다룬 적이 있으므로 생략하도록 한다.</p>

<h3 id="learning-to-align-and-translate">Learning to align and translate</h3>
<p>모델의 구성 요소를 하나하나 살펴보기 이전에 notation에 관한 정리를 진행하겠다.</p>

<p>$y_i$: i time에서의 target word<br />
$s_i$: i time에서의 디코더 Hidden State<br />
$c_i$: i time에서의 context vector = annotations의 가중합<br />
$\alpha_{ij}$: attention weight = normalized score = 연결 확률<br />
$h_j$: j time에서의 인코더 Hidden State = annotations<br />
$e_{ij}$: attention score = unnormalized score<br />
$f, g$ = 비선형 함수</p>

<p>명확히 하자면, subscript <strong>i</strong>는 디코더를 명시하며, subscript <strong>j</strong>는 인코더를 명시한다.</p>

<p>이제 모델의 구성 요소를 살펴볼 것이다.<br />
먼거 타겟 word $y_i$를 예측하기 위한 조건부 확률은 아래와 같이 정의된다.</p>

<script type="math/tex; mode=display">p(y_i|y_1, ..., y_{i-1}, \vec{x}) = g(y_{i-1}, s_i, c_i)</script>

<p>이 중 디코더의 i time Hidden State인 $s_i$를 먼저 살펴보면,</p>

<script type="math/tex; mode=display">s_i = f(s_{i-1}, y_{i-1}, c_i)</script>

<p>Basic Encoder-Decoder 모델과 달리 target word를 예측하기 위한 조건부 확률은 분리된 context vector $c_i$에 의존한다.</p>

<p><strong>Context Vector</strong> $c_i$는 annotations $h_j$의 가중합이다.</p>

<script type="math/tex; mode=display">c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j</script>

<p>여기서 $h_j$는 j time annotation으로, input sequence의 i번째 단어 주위 부분에 강하게 집중하여 input sequence에 대한 정보를 담게 된다.</p>

<p><em>Bidirectional RNN</em><br />
이 $h_j$는 forward RNN의 Hidden States와 backward RNN의 Hidden States를 세로로 합친 열벡터이다.</p>

<script type="math/tex; mode=display">h_j = [\overrightarrow{h_j}^T | \overleftarrow{h_j}^T]^T</script>

<p>이러한 방식으로 $h_j$는 두 방향 모두로 words들을 요약한 정보를 담게 된다.</p>

<p>이제 <strong>attention weight</strong> $a_{ij}$가 어떻게 계산되는지 살펴보겠다.</p>

<script type="math/tex; mode=display">a_{ij} = \frac{ exp(e_{ij}) } {\sum_{k=1}^{T_x} exp(e_{ik}) }</script>

<p>이 $a_{ij}$는 <strong>Normalized Score</strong>라고 할 수 도 있다. 왜냐하면 softmax함수의 확률로서 계산되기 때문이다.</p>

<p><strong>Unnormalized Score</strong>인 $e_{ij}$는 아래와 같이 계산된다.</p>

<script type="math/tex; mode=display">e_{ij} = a(s_{i-1}, h_j)</script>

<p>여기서 a함수는 <strong>alignment model</strong>이다. 이 a를 다른 component와 함께 학습되는 순전파 신경망으로서 모수화한다.<br />
이 alignment model은 j time 인풋이 i time 아웃풋과 얼마나 유사한지를 평가하게 된다.<br />
또한 이 모델은 잠재변수로 설정되지 않고, soft alignment를 직접적으로 계산하여 cost function의 gradient가 역전파될 수 있도록 하게 만든다.<br />
계산 방법은 마지막 부분에서 설명하도록 하겠다.</p>

<p>위 설명을 보면, 결국 i번째 <strong>Context Vector</strong>인 $c_i$는 expected annotation over all the annotations with probabilities $\alpha_{ij}$라고 할 수 있다.<br />
이 $\alpha_{ij}$는 다음 Hidden State인 $s_i$를 결정하고 target word $y_i$를 generate하는 데에 있어 $h_j$의 중요성을 결정하는 역할을 하게 된다.</p>

<p>즉 이는 일종의 <strong>attention</strong>이라는 개념으로 설명될 수 있다.<br />
디코더는 source sentence의 어떤 부분에 <strong>집중</strong>해야 하는지 결정하게 되는 것이다.</p>

<h3 id="conclusion">Conclusion</h3>
<p>제안된 모델은 다음 target word를 generate하는 데에 관련이 있는 정보에만 집중하며 이 때문에 source sentence의 길이에 상당히 robust하다.<br />
다만 unknown or rare words를 다루는 데 있어서는 좀 더 보완이 필요하다.</p>

<h3 id="details-about-model-architecture">Details about Model Architecture</h3>
<p>이 부분에서는 Appendix에 나와 있는 수식들을 종합하여, 본 논문에서 제안한 RNNSearch라는 모델의 구조에 대해 정리하도록 하겠다.<br />
논문이 굉장히 친절하여 Matrix의 차원이 정확하고 자세하게 나와있으므로 반드시 참고할 필요가 있다.</p>

<ol>
  <li>
    <p>상수에 대한 설명은 아래와 같다.<br />
$m$: word embedding 차원, 본 모델에선 620 <br />
$n$: 인코더/디코더 Hidden Units의 수, 본 모델에선 1000<br />
$n’$: Alignment Model 내에서의 Hidden Units의 수, 본 모델에선 1000<br />
$l$: , 본 모델에선 500<br />
$T_x$: source sentence의 길이<br />
$K_x$: source language의 vocab_size</p>
  </li>
  <li>
    <p>벡터들의 크기는 아래와 같다.<br />
$y_i$: (k, 1)<br />
$s_i$: (n, 1)<br />
$h_i$: (n, 1)<br />
$v_a$: (n’, 1)<br />
$z_i$: (n, 1)<br />
$r_i$: (n, 1)</p>
  </li>
  <li>
    <p>행렬들의 크기는 아래와 같다. W, U, C는 모두 Parameter Matrix이다.<br />
$X$: ($T_x$, $K_x$)<br />
$Y$: ($T_y$, $K_y$)<br />
$E$: (m, K), x와 결합할 때는 $K_x$, y와 결합할 때는 $K_y$<br />
$W$: (n, m), $W, W_z, W_r$에 한정<br />
$W_a$: (n’, n), Alignment 모델에서 사용<br />
$U$: (n, n), $U, U_z, U_r$에 한정<br />
$U_a$: (n’, 2n), Alignment 모델에서 사용<br />
$C$: (n, 2n), $C, C_z, C_r$에 한정</p>
  </li>
</ol>

<p><strong>Encoder</strong><br />
source sentence Matrix <strong>X</strong>는 번역 대상인 하나의 문장을 뜻한다.<br />
각각의 열벡터는 $\vec{x_i}$로 표기되며 이 벡터의 크기는 $K_x$로,<br />
source language의 vocab_size를 의미한다.</p>

<p>인코더의 Bidirectional RNN은 아래와 같이 계산된다.<br />
(Bias항은 잠시 삭제한다.)</p>

<script type="math/tex; mode=display">h_j = (1 - z_i) \odot h_{j-1} + z_i \odot \tilde{h_j}</script>

<p>위에서 $z_i$가 Update Gate이며, 각 Hidden State가 이전 activation을 유지하느냐 마느냐를 결정한다.</p>

<script type="math/tex; mode=display">\tilde{h_j} = tanh(W*Ex_j + U[r_j \odot h_{j-1}])</script>

<p>위에서 $r_j$가 Reset Gate이며, 이전 State의 정보를 얼마나 Reset할지 결정한다.</p>

<script type="math/tex; mode=display">z_j = \sigma(W_z * Ex_j + U_z * h_{j-1})</script>

<script type="math/tex; mode=display">r_j = \sigma(W_r * Er_j + U_x * h_{j-1})</script>

<p>위에서 계산한 식은 $\overrightarrow{h_j}$, $\overleftarrow{h_j}$ 모두에게 통용되며,<br />
이를 stack하여 annotation $h_j$를 만들게 되는 것이다.</p>

<p><strong>Decoder</strong><br />
디코더의 Hidden State인 $s_i$ 역시 계산 방식은 유사하다.</p>

<script type="math/tex; mode=display">s_i = (1 - z_i) \odot s_{i-1} + z_i \odot \tilde{s_i}</script>

<script type="math/tex; mode=display">\tilde{s_i} = tanh(W*Ex_i + U[r_i \odot s_{i-1}] + C*c_i)</script>

<script type="math/tex; mode=display">z_i = \sigma(W_z * Ex_i + U_z * s_{i-1} + C_zc_i)</script>

<script type="math/tex; mode=display">r_i = \sigma(W_r * Er_j + U_x * s_{i-1} + C_rc_i)</script>

<script type="math/tex; mode=display">c_i = \sum_{j=1}^{T_x}a_{ij}h_j</script>

<script type="math/tex; mode=display">a_{ij} = \frac{ exp(e_{ij}) } {\sum_{k=1}^{T_x} exp(e_{ik}) }</script>

<script type="math/tex; mode=display">e_{ij} = v_a^T tanh(W_a * s_{i-1} + U_a * h_j)</script>

<p>최종적으로 Decoder State $s_{i-1}$, Context Vector $c_i$, 마지막 generated word $y_{i-1}$을 기반으로, target word $y_i$의 확률을 아래와 같이 정의한다.</p>

<script type="math/tex; mode=display">p(y_i|s_i, y_{i-1}, c_i) \propto exp(y_i^T W_o t_i)</script>

<p>즉 오른쪽 편에 있는 스칼라값에 정비례한다는 뜻이다.<br />
잠시 행렬의 차원을 정의하고 진행하겠다.</p>

<p>$W_o$: ($K_y$, $l$)<br />
$U_o$: ($2l$, n)<br />
$V_o$: ($2l$, m)<br />
$C_o$: ($2l$, 2n)<br />
이들은 모두 Parameter이다.</p>

<p>이제 $t_i$를 정의할 것인데, 그 전에 두 배 크기인 candidate $\tilde{t_i}$를 먼저 정의하겠다.</p>

<script type="math/tex; mode=display">\tilde{t_i} = U_o * s_{i-1} + V_o * Ey_{i-1} + C_oc_i</script>

<p>차원을 맞춰보면 위 벡터는 크기가 ($2l$, 1)인 것을 알 수 있을 것이다.<br />
이제 이 벡터에서 아래와 같은 maxout과정을 거치면,</p>

<center><img src="/public/img/Paper_Review/2018-09-27-Attention/a1.png" width="50%" /></center>

<p>$t_i$는 아래와 같이 정의된다.</p>

<script type="math/tex; mode=display">t_i = [ max(\tilde{t_{i, 2j-1}}, \tilde{t_{i, 2j}}) ]_{j=1, ..., l}^T</script>

<p>아주 멋지다.<br />
<strong>The End</strong></p>

    </article>
    <div class="post-more">
      
      <a href="/Attention/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/Attention/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page11">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/page9">Newer</a>
    
  
</div>


  </div>
</div>

<label for="sidebar-checkbox" class="sidebar-toggle"></label>

<script>
  (function (document) {
    let toggle = document.querySelector('.sidebar-toggle');
    let sidebar = document.querySelector('#sidebar');
    let checkbox = document.querySelector('#sidebar-checkbox');

    document.addEventListener('click', function (e) {
      let target = e.target;

      if (target === toggle) {
        checkbox.checked = !checkbox.checked;
        e.preventDefault();
      } else if (checkbox.checked && !sidebar.contains(target)) {
        /* click outside the sidebar when sidebar is open */
        checkbox.checked = false;
      }
    }, false);
  })(document);
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    };
    i[r].l = 1 * new Date();
    a = s.createElement(o);
    m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-00000000-1', 'auto');
  ga('send', 'pageview');
</script>


<!-- Naver Analytics -->	
<script type="text/javascript" src="//wcs.naver.net/wcslog.js"></script>
<script type="text/javascript">
  if(!wcs_add) var wcs_add = {};
    wcs_add["wa"] = "18cbce78e94161";
  wcs_do();
</script>

</body>

<script id="dsq-count-scr" src="//greeksharifa-github-io.disqus.com/count.js" async></script>

</html>
