<!DOCTYPE html>
<html lang="en-us">
<head>
  <head>
  <!-- Description of Blog -->
  <meta name="description" content="Python, Machine & Deep Learning">
  <link rel="canonical" href="https://greeksharifa.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Python, Machine & Deep Learning">
  <meta property="og:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta property="og:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta property="og:url" content="https://greeksharifa.github.io/">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python, Machine & Deep Learning">
  <meta name="twitter:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta name="twitter:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta name="twitter:domain" content="https://greeksharifa.github.io/">

  <!-- link -->
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Blog
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="icon-144x144" sizes="144x144" href="/public/img/icon-144x144.png">
  <link rel="shortcut icon" href="/public/img/icon_32x32.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  

  <!-- Ads -->
  <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
  </script>
</head>

  <!-- for Google AdSense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9951774327887666",
    enable_page_level_ads: true
  });
</script>

  <style>blockquote {
    font-size: 1em;
    line-height: 1.4
  }</style>
  <link href='http://fonts.googleapis.com/css?family=Gill+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Consolas' rel='stylesheet' type='text/css'>
</head>
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/3c2986ad7ac1f2230ea3596f44563328">
          <img src="/public/img/maple_tree.jpg" title="Cover Photo" alt="Maple tree" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>Developer and Analyst</strong>, YW & YY.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/greeksharifa">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:greeksharifa@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://greeksharifa.github.io/">
          Github Project
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 YW & YY. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://greeksharifa.github.io">YW & YY</a>
    </p>
  </div>
</div>


<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
     content to avoid any CSS collisions with our real content. -->
<div class="wrap">
  <div class="masthead">
    <div class="container">
      <h3 class="masthead-title" align="center">
        <a href="/" title="Home" title="YW & YY">
          <img class="masthead-logo" src="/public/img/logo.png"/>
        </a>
        <small>YW & YY's Python, Machine & Deep Learning</small>
        <!-- HTML elements for search -->
        <a href="/search/" id="search_icon">
          <img src="/public/img/search.png" width="25" height="25"
               align="right" style="margin-top:5px; margin-bottom:0;"
               onmouseover="this.style.opacity=0.7" onmouseout="this.style.opacity=0.5"
               alt="search">
        </a>
      </h3>
    </div>
  </div>

  <div class="container content">
    <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/Dual-Attention-Networks/">
        DANs(Dual Attention Networks for Multimodal Reasoning and Matching)
      </a>
    </h1>

    <span class="post-date">17 Apr 2019</span>
     |
    
    <a href="/blog/tags/#attention-mechanism" class="post-tag">Attention Mechanism</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    <a href="/blog/tags/#vqa" class="post-tag">VQA</a>
    
    

    <article>
      <hr />

<p>이 글에서는 네이버랩스(Naver Corp.)에서 2017년 발표한 논문인 Dual Attention Networks for Multimodal Reasoning and Matching에 대해 알아보고자 한다.<br />
네이버랩스는 인공지능 국제대회 ‘CVPR 2016: VQA Challenge’에서 2위를 차지하였고, 해당 챌린지에서 DAN(Dual Attention Networks)라는 알고리즘을 개발하였다. 이어 이 알고리즘을 조금 더 일반화하여 2017년 발표한 논문이 이 논문이다.</p>

<p>VQA가 무엇인지는 <a href="https://greeksharifa.github.io/computer%20vision/2019/04/17/Visual-Question-Answering/">여기</a>를 참조하면 된다.</p>

<p>간단히, DANs은 따로 존재하던 Visual 모델과 Textual 모델을 잘 합쳐 하나의 framework로 만든 모델이라고 할 수 있겠다.</p>

<p>중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.</p>

<hr />

<h1 id="dansdual-attention-networks-for-multimodal-reasoning-and-matching">DANs(Dual Attention Networks for Multimodal Reasoning and Matching)</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1611.00471">DANs(Dual Attention Networks for Multimodal Reasoning and Matching)</a></strong></p>

<h2 id="초록abstract">초록(Abstract)</h2>

<p>vision과 language 사이의 세밀한 상호작용을 포착하기 위해 우리는 visual 및 textual attention을 잘 조정한 Dual Attention Networks(DANs)를 제안하고자 한다. DANs는 이미지와 텍스트 모두로부터 각각의 중요한 부분에 여러 단계에 걸쳐 집중(attend / attention)하고 중요한 정보를 모아 이미지/텍스트의 특정 부분에만 집중하고자 한다. 이 framework에 기반해서, 우리는 multimodal reasoning(추론)과 matching(매칭)을 위한 두 종류의 DANs를 소개한다. 각각의 모델은 VQA(Visual Question Answering), 이미지-텍스트 매칭에 특화된 것이고 state-of-the-art 성능을 얻을 수 있었다.</p>

<hr />

<h2 id="서론introduction">서론(Introduction)</h2>

<p>Vision과 language는 실제 세계를 이해하기 위한 인간 지능의 중요한 두 부분이다. 이는 AI에도 마찬가지이며, 최근 딥러닝의 발전으로 인해 이 두 분야의 경계조차 허물어지고 있다. VQA, Image Captioning, image-text matching, visual grounding 등등.</p>

<p>최근 기술 발전 중 하나는 attention mechanism인데, 이는 이미지 등 전체 데이터 중에서 중요한 부분에만 ‘집중’한다는 것을 구현한 것으로 많은 신경망의 성능을 향상시키는 데 기여했다. <br />
시각 데이터와 텍스트 데이터 각각에서는 attention이 많은 발전을 가져다 주었지만, 이 두 모델을 결합시키는 것은 연구가 별로 진행되지 못했다.</p>

<p>VQA같은 경우 “(이미지 속) 저 우산의 색깔은 무엇인가?” 와 같은 질문에 대한 답은 ‘우산’과 ‘색깔’에 집중함으로써 얻을 수 있고, 이미지와 텍스트를 매칭하는 task에서는 이미지 속 ‘girl’과 ‘pool’에 집중함으로써 해답을 얻을 수 있다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/01.png" width="80%" /></center>

<p>이 논문에서 우리는 vision과 language의 fine-grained 상호작용을 위한 visual 모델과 textual 모델 두 가지를 잘 결합한 Dual Attention Networks(DANs)를 소개한다. DANs의 두 가지 변형 버전이 있는데, reasoning-DAN(r-DAN, 추론용 모델)과 matching-DAN(m-DAN, 매칭용 모델)이다.</p>

<p>r-DAN은 이전 attention 결과와 다음 attention을 모은 결합 메모리를 사용하여 시각적 그리고 언어적 attention을 협동 수행한다. 이는 VQA같은 multimodal 추론에 적합하다.<br />
m-DAN은 시각 집중 모델과 언어 집중 모델을 분리하여 각각 다른 메모리에 넣지만 이미지와 문장 사이의 의미를 찾기 위해 학습은 동시에 진행하는 모델이다. 이 접근법은 최종적으로 효율적인 cross-modal 매칭을 용이하게 해 준다.<br />
두 알고리즘 모두 시각적 그리고 언어적(문자적, textual) 집중 mechanism을 하나의 framework 안에 긴밀히 연결한 것이다.</p>

<p>이제 우리가 기여한 바는 다음과 같다:</p>

<ul>
  <li>시각적 그리고 언어적 attention을 위한 통합된 framework를 제안하였다. 이미지 내 중요한 부분과 단어들은 여러 단계에서 합쳐진 곳에 위치한다.</li>
  <li>이 framework의 변형 버전 두 가지는 실제로 추론 및 매칭을 위한 모델로 구현되어 VQA와 image-text 매칭에 적용되었다.</li>
  <li>attention 결과의 상세한 시각화는 우리의 모델이 task에 핵심적인 이미지 및 문장 부분에 잘 집중하고 있음을 보여주는 것을 가능하게 한다.</li>
  <li>이 framework는 VQA와 Flickr30K 데이터셋에서 SOTA(state-of-the-art) 결과를 보여주었다.</li>
</ul>

<hr />

<h2 id="관련-연구related-works">관련 연구(Related Works)</h2>

<ul>
  <li><strong>Attention Mechanisms:</strong> 간단히 말해 시각적 또는 언어적 입력에서 task를 해결하는 데 중요한 일부분에만 집중하도록 해 문제를 잘 풀 수 있게 하는 방법이다.</li>
  <li><strong>Visual Question Answering(VQA):</strong> 이미지와 그 이미지와 연관된 질문이 주어지면 적절한 답을 찾는 task이다. 자세한 내용은 <a href="https://greeksharifa.github.io/computer%20vision/2019/04/17/Visual-Question-Answering/">여기</a>를 참조하라.</li>
  <li><strong>Image-Text Matching:</strong> 시각자료(이미지)와 글자자료(=문장, 언어적 부분) 사이의 의미적 유사도를 찾는 것이 가장 중요하다. 많은 경우 이미지 특징벡터(feature vector)와 문장 특징벡터를 직접 비교할 수 있도록 변형해 비교하는 방법이 자주 쓰인다. 이 비교방법은 양방향 손실함수 또는 CNN으로 결합하는 방법 등이 쓰인다. 그러나 multimodal attention 모델을 개발하려는 시도는 없었다.</li>
</ul>

<hr />

<h2 id="dual-attention-networksdans">Dual Attention Networks(DANs)</h2>

<h3 id="input-representation">Input Representation</h3>

<h4 id="image-representation">Image representation</h4>

<ul>
  <li>이미지 특징은 19-layer VGGNet 또는 152-layer ResNet으로 추출했다.</li>
  <li>448 $\times$ 448 으로 바꿔 CNN에 집어넣는다.</li>
  <li>다른 ‘지역’(region)으로부터 특징벡터를 얻기 위해 VGGNet 및 ResNet의 마지막 pooling layer를 취했다.</li>
  <li>이제 이미지는 ${v_1, …, v_N}$으로 표현된다. $N$은 이미지 지역의 개수, $v_n$은 512(VGGNet) 또는 2048(ResNet)이다.</li>
</ul>

<h4 id="text-representation">Text representation</h4>

<p>one-hot 인코딩으로 주어진 $T$개의 입력 단어들 ${w_1, …, w_T}$을 임베딩시킨 후 양방향 LSTM에 집어넣는다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/02.png" width="80%" /></center>

<p>임베딩 행렬(embedding matrix)와 LSTM은 end-to-end로 학습된다.</p>

<h3 id="attention-mechanisms">Attention Mechanisms</h3>

<p>bias $b$는 생략되어 있다.</p>

<h4 id="visual-attention">Visual Attention</h4>

<p>이미지의 특정 부분에 집중하게 하는 context vector를 생성하는 것을 주목적으로 한다.</p>

<p>step $k$에서, 시각문맥벡터(visual context vector) $v^{(k)}$는</p>

<script type="math/tex; mode=display">v^{(k)} = \text{V\_Att} (\{v_n\}^N_{n=1}, \ m_v^{(k-1)}</script>

<p>$m_v^{(k-1)}$는 step $k-1$까지 집중했었던 정보를 인코딩하는 메모리 벡터이다.<br />
여기에다가 soft attention mechanism을 적용하게 된다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/03.png" width="80%" /></center>

<p>attention weights $\alpha$는 2-layer FNN과 softmax로 구해진다. $W$들은 네트워크 parameter이다.</p>

<h4 id="textual-attention">Textual Attention</h4>

<p>마찬가지로 문장의 특정 부분에 집중할 수 있도록 문맥벡터 $u^{(k)}$를 매 step마다 생성하는 것이다.</p>

<script type="math/tex; mode=display">u^{(k)} = \text{T\_Att} (\{u_t\}^T_{t=1}, \ m_u^{(k-1)}</script>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/04.png" width="80%" /></center>

<h3 id="r-dan-for-visual-question-answering">r-DAN for Visual Question Answering</h3>

<p>VQA는 multimodal 데이터를 결합 추론하는 것을 필요로 하는 문제이다. 이를 위해 r-DAN은 step $k$에서 시각 및 언어적 정보를 축적하는 메모리 벡터 $m^{(k)}$를 유지한다. 이는 재귀적으로 다음 식을 통해 업데이트된다.</p>

<script type="math/tex; mode=display">m^{(k)} = m^{(k-1)} + v^{(k)} \  (\cdot) \ u^{(k)}</script>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/05.png" width="60%" /></center>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/06.png" width="100%" /></center>

<p>최종 답은 다음과 같이 계산된다. $ \text{p}_{\text{ans}}$는 정답 후보들의 확률을 나타낸다.</p>

<script type="math/tex; mode=display">\bold{\text{p}}_{\text{ans}} = \text{softmax} \bigr( W_{\text{ans}} \ m^{(K)} \bigl)</script>

<h3 id="m-dan-for-image-text-matching">m-DAN for Image-Text Matching</h3>

<p>수식의 형태는 꽤 비슷하다.</p>

<script type="math/tex; mode=display">m_v^{(k)} = m_v^{(k-1)} + v^{(k)}</script>

<script type="math/tex; mode=display">m_u^{(k)} = m_u^{(k-1)} + u^{(k)}</script>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/07.png" width="100%" /></center>

<p><script type="math/tex">s^{(k)} = v^{(k)} \cdot u^{(k)}, \ S = \sum_{k=0}^K s^{(k)}</script>
Loss function은 다음과 같이 정의된다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/08.png" width="60%" /></center>

<p>추론할 시점에는 어떤 이미지나 문장이든 결합공간 안에 임베딩된다.</p>

<script type="math/tex; mode=display">z_v = [v^{(0)}; ... ; v^{(K)}],</script>

<script type="math/tex; mode=display">z_u = [u^{(0)}; ... ; u^{(K)}],</script>

<hr />

<h2 id="실험experiments">실험(Experiments)</h2>

<h3 id="experimental-setup">Experimental Setup</h3>

<p>r-DAN과 m-DAN 모두에 대해 모든 hyper-parameters들은 전부 고정되었다.</p>

<p>$K$=2, LSTM을 포함한 모든 네트워크의 hidden layer의 dimension=512,<br />
lr=0.1, momentum=0.9, weight decay=0.0005, dropout rate=0.5, gradient clipping=0.1,<br />
epochs=60, 30epoch 이후 lr=0.01,<br />
minibatch=128 $\times$ 128 quadruplets(긍정 이미지, 긍정 문장, 부정 이미지, 부정 문장),<br />
가능한 답변의 수 C=2000, margin $m$=100이다.</p>

<h3 id="evaluation-on-visual-question-answering">Evaluation on Visual Question Answering</h3>

<h4 id="dataset-and-evaluation-metric">Dataset and Evaluation Metric</h4>

<p>VQA 데이터셋을 사용하였고, train(이미지 8만 장), val(이미지 4만 장), test-dev(이미지 2만 장), test-std(이미지 2만 장)이다. 측정방법은</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/09.png" width="60%" /></center>

<p>$\hat{a}$는 예측된 답이다.</p>

<h4 id="results-and-analysis">Results and Analysis</h4>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/10.png" width="100%" /></center>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/11.png" width="100%" /></center>

<p>결과를 보면 대부분의 상황에서 SOTA 결과를 얻었으며, 이미지와 문장에서 집중해야 할 부분을 잘 찾았음을 확인할 수 있다.</p>

<h3 id="evaluation-on-image-text-matching">Evaluation on Image-Text Matching</h3>

<p>분석결과는 비슷하므로 생략한다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/12.png" width="100%" /></center>

<hr />

<h2 id="결론conclusion">결론(Conclusion)</h2>

<p>우리는 시각 및 언어적 attention mechanism을 연결하기 위한 Dual Attention Networks (DANs)를 제안하였다. 추론과 매칭을 위한 모델을 하나씩 만들었고, 각각의 모델은 이미지와 문장으로부터 공통 의미를 찾아낸다.<br />
이 모델들은 VQA와 image-text 매칭 task에서 SOTA 결과를 얻어냄으로써 DANs의 효과를 입증하였다. 제안된 이 framework는 image captioning, visual grounding, video question answering 등등 많은 시각 및 언어 task들로 확장될 수 있다.</p>

<hr />

<h2 id="참고문헌references">참고문헌(References)</h2>

<p>논문 참조! 부록은 없다. <del>읽기 편하다</del></p>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/Dual-Attention-Networks/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/Dual-Attention-Networks/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/Pix2Pix/">
        Pix2Pix(Image-to-Image Translation with Conditional Adversarial Networks)
      </a>
    </h1>

    <span class="post-date">07 Apr 2019</span>
     |
    
    <a href="/blog/tags/#gan" class="post-tag">GAN</a>
    
    <a href="/blog/tags/#machine-learning" class="post-tag">Machine Learning</a>
    
    <a href="/blog/tags/#cnn" class="post-tag">CNN</a>
    
    <a href="/blog/tags/#generative-model" class="post-tag">Generative Model</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <hr />

<p>이 글에서는 2016년 11월 <em>Phillip Isola</em> 등이 발표한 Image-to-Image Translation with Conditional Adversarial Networks(Pix2Pix)를 살펴보도록 한다.</p>

<p>Pix2Pix는 Berkeley AI Research(BAIR) Lab 소속 Phillip Isola 등이 2016 최초 발표(2018년까지 업데이트됨)한 논문이다.</p>

<center><img src="/public/img/2019-04-07-Pix2Pix/01.png" width="100%" /></center>

<p>Pix2Pix는 Image to Image Translation을 다루는 논문이다. 이러한 변환은 Colorization(black &amp; white $\rightarrow$ color image) 등을 포함하는데, Pix2Pix에서는 이미지 변환 문제를 colorization처럼 한 분야에만 국한되지 않고 좀 더 일반화한 문제를 풀고자 했다. 그리고 그 수단으로써 Conditional adversarial nets를 사용했다.</p>

<p>중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.</p>

<hr />

<h1 id="pix2piximage-to-image-translation-with-conditional-adversarial-networks">Pix2Pix(Image-to-Image Translation with Conditional Adversarial Networks)</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1611.07004">Pix2Pix(Image-to-Image Translation with Conditional Adversarial Networks)</a></strong></p>

<h2 id="초록abstract">초록(Abstract)</h2>

<p>우리는 conditional adversarial networks를 일반화된 이미지 변환 문제에 테스트하였다. 이 네트워크는 단지 input-output mapping만 배우는 것이 아니라 이를 학습하기 위한 loss function까지 배운다. 따라서 전통적으로 매우 다른 loss function을 쓰던 문제에들도 이 접근법을 적용할 수 있다.<br />
우리는 이 접근이 label과 동기화, 경계선만 있는 이미지를 복원, 흑백이미지에 색깔 입히기 등등의 문제에 효과적임을 보였다.</p>

<hr />

<h2 id="서론introduction">서론(Introduction)</h2>

<p>이미지를 이미지로 변환할 뿐인 수많은 문제들은 그 세팅이 똑같음에도 각각 따로 연구되어 왔다(위에서 말한 이미지 변환 문제들). 우리는 이러한 변환 문제를 위한 일반적인 framework를 개발하는 것이 목표이다.</p>

<p>이쪽 방향으로는 이미 CNN이라는 좋은 기계가 있다. CNN은 결과의 품질을 알려주는 loss function을 최소화한다. 그러나 학습 과정 자체는 자동화되어 있지만 결과를 잘 나오게 하기 위해서는 여전히 수동으로 조절해야 할 것이 많다. 즉, 우리는 <em>무엇을 최소화해야하는지</em> CNN에게 말해주어야 한다.<br />
만약 우리가 단순히 결과와 정답 사이의 유클리드 거리를 최소화하라고만 하면 뿌연(blurry) 이미지를 생성하게 된다. 이는 유클리드 거리는 그럴듯한 결과를 평균했을 때 최소화되기 때문이고, 결과적으로 이미지가 흐려진다. 실제 같은(realistic) 이미지를 얻기 위해서는 더 전문 지식이 필요하다.</p>

<p>만약 우리가 원하는 것을 고수준으로(high-level goal) 말할 수만 있다면, 네트워크는 스스로 그러한 목표에 맞게 loss를 줄여나갈 것이다. 운 좋게도, 최근에 정확히 이것을 해주는 GAN이 발표되었다. GAN은 실제와 가짜를 구분하지 못하도록 학습을 진행하며, 이는 흐린 이미지를 생성하지 않게 할 수 있다(뿌연 이미지는 실제 사진처럼 보일 리 없으므로).</p>

<p>이 논문에서, 우리는 CGAN이라는 조건부 생성모델을 사용한다. 우리는 input image라는 조건을 줄 것이고 그에 맞는 output image를 생성할 것이기 때문에 이는 이미지 변환 문제에 잘 맞는다.</p>

<p>이 논문이 기여하는 바는</p>

<ul>
  <li>conditional GAN이 넓은 범위의 문제에서 충분히 합리적인 결과를 가져다준다는 것을 밝혔고</li>
  <li>좋은 결과를 얻기에 충분한 간단한 framework를 제안하고 여러 중요한 architecture의 효과를 분석하였다.</li>
</ul>

<hr />

<h2 id="관련-연구related-works">관련 연구(Related Works)</h2>

<ul>
  <li><strong>Structures losses for image modeling:</strong> 이미지 변환 문제는 per-pixel 분류 또는 회귀 문제로 다뤄졌다. 이러한 공식화는 output space는 “unstructured”이며 각 결과 픽셀은 다른 픽셀에 독립적인 것처럼 다룬다. CGAN는 “structured loss”를 학습하며 많은 논문들이 이러한 loss를 다룬다. conditional random fields, SSIM metric, nonparametric losses 등등.</li>
  <li><strong>Conditional GANs:</strong> 사실 이 논문에서 GAN을 처음 사용한 것은 아니다. 그러나 조건부 GAN을 이미지 변환 문제에 사용한 적은 없었다. CGAN에 대한 설명은 <a href="https://greeksharifa.github.io/generative%20model/2019/03/19/CGAN/">여기</a>를 참조하자.</li>
</ul>

<hr />

<h2 id="방법method">방법(Method)</h2>

<p>GAN은 random noise vector $z$로부터 output image $y$를 생성하는 $G: z \rightarrow y$를 학습하는 생성모델이다. 이에 비해 CGAN은 $z$와 observed image $x$로부터 $y$로의 mapping인 $G: {x, z} \rightarrow y$를 학습한다.</p>

<center><img src="/public/img/2019-04-07-Pix2Pix/02.png" width="70%" /></center>

<script type="math/tex; mode=display">\\</script>

<h3 id="목적함수objective">목적함수(Objective)</h3>

<p>CGAN의 목적함수는 다음과 같다.</p>

<script type="math/tex; mode=display">\mathcal{L}_{\text{cGAN}}(G, D) = \mathbb{E}_{x , \ y}[log \ D(x,y)] + \mathbb{E}_{x , \ z}[log \ (1-D(G(x, z)))]</script>

<p>D를 조건부로 학습시키는 것을 중요하게 여겨, D가 $x$를 관측하지 못하도록 unconditional variant를 비교하도록 했다:</p>

<script type="math/tex; mode=display">\mathcal{L}_{\text{GAN}}(G, D) = \mathbb{E}_{ y}[log \ D(y)] + \mathbb{E}_{x , \ z}[log \ (1-D(G(x, z)))]</script>

<p>D의 할일은 그대로이지만, G는 단지 D를 속이는 것뿐만 아니라 L2 distance에서의 ground truth에도 가깝도록 만들어야 한다.<br />
사실 L2보다는 L1을 사용하는 것이 덜 흐린 이미지를 생성하는 데 도움이 되었다:</p>

<script type="math/tex; mode=display">\mathcal{L}_{L1}(G) = \mathbb{E}_{x, \ y, \ z }[ \Vert y - G(x, z) \Vert_1 ]</script>

<p>그래서 최종 목적함수는</p>

<script type="math/tex; mode=display">G^\ast = arg \ min_G \ max_D \ \mathcal{L}_{\text{cGAN}}(G, D) + \lambda \mathcal{L}_{L1}(G)</script>

<p>이다.</p>

<p>$z$가 없이도 네트워크는 $x \rightarrow y$ mapping을 학습할 수 있지만, 결정론적인 결과를 생성할 수 있고, 따라서 delta function 이외의 어떤 분포와도 맞지 않을 수 있다. 과거의 conditional GAN은 이를 인정하여 $x$에 더해 Gaussian noise $z$를 입력으로 주었다.<br />
초기 실험에서 우리는 noise를 단순히 무시하도록 했지만, 최종 모델에서는 dropout 시에만 noise를 제공하여 학습과 테스트 시 모두에 G의 여러 레이어에 적용되도록 만들었다. dropout noise에도 불구하고 우리는 매우 조금의 stochasiticity만을 관측하였다. 아주 stochastic한 결과를 생성하는 conditional GAN을 설계하는 것은 아주 중요한 문제이다.</p>

<h3 id="네트워크-구조network-architectures">네트워크 구조(Network architectures)</h3>

<p>우리는 DCGAN을 G와 D의 기본 모델로 하였고 둘 다 convolution-BatchNorm-ReLU 구조를 따른다.</p>

<h4 id="generator-with-skips">Generator with skips</h4>

<p>이미지 변환(image-to-image translation) 문제에서 어려운 점은 고해상도 input grid를 고해상도 output grid로 mapping하는 것이다. 심지어 표면의 외관은 다른데 각각 같은 근본적인 구조를 가진다는 것이다.<br />
많은 이전 연구들은 encoder-decoder 네트워크를 사용한다. 이러한 네트워크에서는 bottleneck 레이어를 통과하기 때문에 정보의 손실이 필연적으로 발생할 수밖에 없다. 그래서, skip-connection을 추가한 <strong>U-Net</strong>이라는 구조를 사용했다.<br />
정확히는, 전체 레이어 개수를 $n$이라 할 때 모든 $i$번째 레이어와 $n-i$번째 레이어를 연결했다. 각 연결은 단순히 concatenate한 것이다.</p>

<center><img src="/public/img/2019-04-07-Pix2Pix/03.png" width="80%" /></center>

<h4 id="markovian-discriminatorpatchgan">Markovian discriminator(PatchGAN)</h4>

<p>high-frequency 모델링을 위해, 집중할 부분(attention)을 local image patch 단위로만 제한하는 것으로 충분하다. 그래서, 우리는 D를 PatchGAN(일반 GAN인데 단지 Patch 단위로만 보는 것) 구조로 만들었다.<br />
그래서 우리의 D는 $N \times N$개의 각 Patch별로 이 부분이 진짜인지 가짜인지를 판별한다.</p>

<p>실험 단계에서 우리는 $N$이 작아도 전체 이미지를 한번에 보는 것보다는 더 좋은 결과를 얻을 수 있음을 보였다. 이는 더 작은 PatchGAN은 더 적은 parameter를 가지고, 더 빠르며, 더 큰 이미지에 적용하는 데에서도 이점이 있음을 보여준다.</p>

<p>D가 이미지를 Markov random field처럼 보는 것이 효과적인 모델링 방법이므로, patch의 지름보다 더 먼 pixel들은 독립적이라고 보았다. 이러한 접근은 이미 연구된 바 있고, texture/style 모델에서 꽤 흔하며 적절한 가정이다. 따라서 PatchGAN은 texture/style loss면에서 충분히 이해가능한 모델이다.</p>

<h3 id="최적화-및-추론optimization-and-inference">최적화 및 추론(Optimization and inference)</h3>

<p>일반적인 GAN 접근법을 따랐다. original GAN에서는 $log \ (1-D(x, G(x,z)))$를 최소화하는 대신 $log \ D(x, G(x,z))$를 최대화하는 것이 낫다고 했다.<br />
그러나 우리는 D를 최적화하는 목적함수를 2로 나누어 D가 G보다 상대적으로 더 빠르게 학습되지 않도록 하였다.<br />
또한 minibatch SGD와 Adam을 사용하였다($lr=0.0002, \beta_1 = 0.5, \beta_2 = 0.999$).  또한 batch size는 실험에 따라 1~10으로 조정하였다.</p>

<hr />

<h2 id="실험experiments">실험(Experiments)</h2>

<p>conditional GAN의 보편성을 테스트하기 위해, 다양하게 진행하였다.</p>

<table>
  <thead>
    <tr>
      <th>Problem</th>
      <th>Dataset</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Semantic labels $\leftrightarrow$ photo</td>
      <td>Cityspaces dataset</td>
    </tr>
    <tr>
      <td>Architectural labels $\leftrightarrow$ photo</td>
      <td>CMP Facades</td>
    </tr>
    <tr>
      <td>Map $\leftrightarrow$ aerial photo</td>
      <td>Google Maps</td>
    </tr>
    <tr>
      <td>BW $\rightarrow$ color photos</td>
      <td>ImageNet</td>
    </tr>
    <tr>
      <td>Edges $\rightarrow$ photo</td>
      <td>Natural Image manifold</td>
    </tr>
    <tr>
      <td>Sketch $\rightarrow$ photo</td>
      <td>human sketches</td>
    </tr>
    <tr>
      <td>Day $\rightarrow$ night</td>
      <td>ACM Transactions on Graphics</td>
    </tr>
    <tr>
      <td>Thermal $\rightarrow$ color photos</td>
      <td>Benchmark dataset and baseline</td>
    </tr>
    <tr>
      <td>Photo with missing pixels $\rightarrow$ inpainted photo</td>
      <td>Paris StreetView</td>
    </tr>
  </tbody>
</table>

<p>다른 네트워크보다 더 좋은 결과:</p>

<center><img src="/public/img/2019-04-07-Pix2Pix/04.png" width="100%" /></center>

<p>encoder-decoder보다 더 효과적인 U-Net:</p>

<center><img src="/public/img/2019-04-07-Pix2Pix/05.png" width="70%" /></center>

<p>Patch의 개수를 늘렸을 때의 선명도 상승:</p>

<center><img src="/public/img/2019-04-07-Pix2Pix/06.png" width="100%" /></center>

<p>구글맵 사진과 도식화한 그림 간 변환 결과:</p>

<center><img src="/public/img/2019-04-07-Pix2Pix/07.png" width="100%" /></center>

<p>Colorization과 이미지 도식화:</p>

<center><img src="/public/img/2019-04-07-Pix2Pix/08.png" width="100%" /></center>

<p>등등 많은 결과가 논문에 나타나 있다.</p>

<p>사실 colorization 문제와 같은 것에서는 colorization에 특화된 네트워크가 더 좋은 결과를 내기는 한다.<br />
그러나 이 Pix2Pix는 훨씬 더 넓은 범위의 문제를 커버할 수 있다는 점에서 의의가 있다.</p>

<p>더 많은 결과에 대해서는 <a href="https://phillipi.github.io/pix2pix/">여기</a>를 참조하라.</p>

<hr />

<h2 id="결론conclusion">결론(Conclusion)</h2>

<p>이 논문에서는 image-to-image translation 문제에 대해, 특히 고도로 구조화된 그래픽 결과에 대해 conditional adversarial networks가 괜찮은 접근법이라는 것을 보여주었다. 이 네트워크는 문제와 데이터에 대한 loss를 학습함으로써 넓은 범위의 문제에 대해 적합함을 보여주었다.</p>

<h3 id="acknowledgments">Acknowledgments</h3>

<p><del>매우 많다 ㅎㅎ</del></p>

<hr />

<h2 id="참고문헌references">참고문헌(References)</h2>

<p>논문 참조!</p>

<hr />

<p>결론 이후에도 많은 실험 결과가 있으니 참조하시라. 매우 흥미로운 것들이 많다.</p>

<center><img src="/public/img/2019-04-07-Pix2Pix/09.png" width="100%" /></center>

<center><img src="/public/img/2019-04-07-Pix2Pix/10.png" width="100%" /></center>

<center><img src="/public/img/2019-04-07-Pix2Pix/11.png" width="100%" /></center>

<center><img src="/public/img/2019-04-07-Pix2Pix/12.png" width="100%" /></center>

<hr />

<h2 id="부록">부록</h2>

<h3 id="generator-architectures">Generator architectures</h3>

<p>코드는 <a href="https://github.com/phillipi/pix2pix">여기</a>에 있다.</p>

<p>encoder는 C64-C128-C256-C512-C512-C512-C512-C512 구조이다(convolution layer).<br />
decoder는 CD512-CD512-CD512-C512-C256-C128-C64  구조이다.</p>

<p>decoder의 마지막 레이어 이후 output 채널에 맞게 mapping되고(3, colorization에서는 2), Tanh 함수가 그 뒤를 따른다.<br />
또한 encoder의 C64에서는 BatchNorm이 없다.<br />
encoder의 모든 ReLU는 기울기 0.2의 Leaky ReLU이며, decoder는 그냥 ReLU이다.</p>

<p>U-Net decoder는 다음과 같이 생겼다. 앞서 언급했든 $i$와 $n-i$번째 레이어 사이에 skip-connection이 존재한다.  이는 decoder의 채널의 수를 변화시킨다.</p>

<p>CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128</p>

<h3 id="discriminator-architectures">Discriminator architectures</h3>

<p>$ 70 \times 70 $ discriminator의 구조는:</p>

<p>C64-C128-C256-C512</p>

<p>단 C64에는 BatchNorm이 적용되지 않는다.<br />
마지막 레이어 이후 convolution을 통해 1차원으로 mapping하며 마지막에 sigmoid 함수가 적용된다.<br />
0.2짜리 Leaky ReLU가 적용되었다.</p>

<p>다른 크기의(patch) D들은 조금씩 깊이가 다르다.</p>

<p>$ 1 \times 1 $ discriminator: C64-C128(convolution들은 $ 1 \times 1 $ spatial 필터를 사용)</p>

<p>$ 16 \times 16 $ discriminator: C64-C128</p>

<p>$ 286 \times 286 $ discriminator: C64-C128-C256-C512-C512-C512</p>

<h3 id="학습-상세">학습 상세</h3>

<ul>
  <li>$ 256 \times 256 $ 이미지는 $ 286 \times 286 $ 크기로 resize되었다가 random cropping을 통해 다시 $ 256 \times 256 $가 되었다.</li>
  <li>모든 네트워크는 scratch로부터 학습되었다.</li>
  <li>weights는 (0, 0.02) 가우시안 분포를 따르는 랜덤 초기값을 가진다.</li>
  <li>데이터셋마다 조금씩 다른 기타 설정은 논문을 참조하자.</li>
</ul>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/Pix2Pix/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/Pix2Pix/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/advanced-GANs/">
        GAN의 개선 모델들(catGAN, Semi-supervised GAN, LSGAN, WGAN, WGAN_GP, DRAGAN, EBGAN, BEGAN, ACGAN, infoGAN)
      </a>
    </h1>

    <span class="post-date">20 Mar 2019</span>
     |
    
    <a href="/blog/tags/#gan" class="post-tag">GAN</a>
    
    <a href="/blog/tags/#machine-learning" class="post-tag">Machine Learning</a>
    
    <a href="/blog/tags/#cnn" class="post-tag">CNN</a>
    
    <a href="/blog/tags/#generative-model" class="post-tag">Generative Model</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <hr />

<p>이 글에서는 catGAN, Semi-supervised GAN, LSGAN, WGAN, WGAN_GP, DRAGAN, EBGAN, BEGAN, ACGAN, infoGAN 등에 대해 알아보도록 하겠다.</p>

<p>아래쪽의 ACGAN, infoGAN은 발표 시기가 아주 최신은 아니지만 conditional GAN(CGAN)의 연장선상에 있다고 할 수 있기 때문에 따로 빼 놓았다.</p>

<p>각각에 대해 간단히 설명하면,</p>

<ul>
  <li><strong>catGAN(Categorical GAN):</strong> D가 real/fake만 판별하는 대신 class label/fake class을 출력하도록 바꿔서 unsupervised 또는 semi-supervised learning이 가능하도록 하였고 또한 더 높은 품질의 sample을 생성할 수 있게 되었다.</li>
  <li><strong>Semi-supervised GAN:</strong> catGAN과 거의 비슷하다. original GAN과는 달리 DCGAN을 기반으로 만들어졌다.</li>
  <li><strong>LSGAN:</strong> 진짜 분포 $ p_{data} $와 가짜 데이터 분포 $p_g$를 비슷하게 만들기 위해, decision boundary에서 멀리 떨어진 sample에게 penalty를 주어 진짜 데이터에 근접하게 만드는 아이디어를 사용했다. 이름답게 loss function에는 Least Square가 사용되었고, 이를 통해 더 선명한 출력 이미지와 학습 과정의 높은 안정성을 얻었다. 또한, 이 최적화 과정이 $\chi^2$ divergence 최소화와 같음을 보였다.</li>
  <li><strong>WGAN:</strong> 실제 데이터의 분포와 가짜 데이터의 분포의 거리를 측정하는 방법으로 <em>Wasserstein Distance</em>를 정의하여 가짜 데이터를 실제 데이터에 근접하도록 하는 방법을 제시하였는데, 기존의 GAN들이 최적 값으로 잘 수렴하지 않던 문제를 해결, 거의 대부분의 데이터셋에서 학습이 잘 되는 GAN을 만들어냈다.</li>
  <li><strong>WGAN_GP:</strong> Improved WGAN이다. WGAN이 <em>k</em>-Lipschitz constraints를 만족시키기 위해 단순히 clipping을 수행하는데, 이것이 학습을 방해하는 요인으로 작용할 수 있다. WGAN_GP에서는 gradient penalty라는 것을 목적함수에 추가하여 이를 해결하였고, 학습 안정성을 데이터셋뿐만 아니라 모델 architecture에 대해서도 얻어냈다.</li>
  <li><strong>DRAGAN:</strong> Deep Regret Analytic GAN이다. WGAN에 더불어 gradient penalty를 정규화하고 더 다듬어 gradient penalty schemes(또는 heuristics)를 만들었고, 이를 저자들은 DRAGAN algorithm이라 하였다. 결과적으로 여전히 남아 있던 mode collapse 문제를 더 완화하였다.</li>
  <li><strong>EBGAN:</strong> Energy-Based GAN. 지금까지 대부분의 GAN이 D가 real일 확률을 0/1로 나타냈었다면, 이 모델은 그 구조를 깨고 에너지 기반 모델로 바꿨다는 데 의의가 있다. 그래서 D는 단지 real/fake를 구분하는 것이 아닌 G에 대한 일종의 loss function처럼 동작하며, 실제 구현은 Auto-Encoder으로 이루어졌다.</li>
  <li><strong>BEGAN:</strong> Boundary Equilibrium GAN으로, EBGAN을 베이스로 하고 Watterstein distance를 사용하였으며, 모델 구조를 단순화하고 이미지 다양성과 품질 간 trade-off를 조절할 수 있는 방법 또한 알아냈다고 한다. 이 논문에서는 스스로 <strong><em>milestone</em></strong>한 품질을 얻었다고 한다.</li>
  <li><strong>ACGAN:</strong> D를 2개의 분류기로 구성하고 목적함수도 두 개로 나눠서 real/fake, 데이터의 class를 구하는 과정을 분리하여 disentangled한 $z$를 만들었다.</li>
  <li><strong>infoGAN:</strong> 많은 GAN들이 그 내부의 parameter가 심하게 꼬여(entangled) 있고 이는 parameter의 어떤 부분이 어느 역할을 하는지 전혀 알 수 없게 만든다. infoGAN에서는 이를 잘 분리하여, semantic feature를 잘 조작하면 어떤 인자를 조작했느냐에 따라 생성되는 이미지의 각도, 밝기, 너비 등을 임의로 조작할 수 있게 하였다.</li>
</ul>

<p>이 글에 소개된 대부분의 GAN은 다음 repository에 구현되어 있다.</p>

<p><a href="https://github.com/znxlwm/pytorch-generative-model-collections">Pytorch version</a><br />
<a href="https://github.com/hwalsuklee/tensorflow-generative-model-collections?fbclid=IwAR1VSa7c9QOdVcrzuPX995FBwqI1WhOAl43jM2HSzp84sfMw2hMZwsB_KPQ">Tensorflow version</a></p>

<hr />

<h1 id="catgan">catGAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1511.06390">catGAN</a></strong></p>

<p>2015년 11월 처음 제안되었다.</p>

<p>데이터의 전체 또는 일부가 unlabeled인 경우 clustering은 $p_x$를 직접 예측하는 generative model과 분포를 예측하는 대신 데이터를 직접 잘 구분된 카테고리로 묶는 discriminative model로 나누어지는데, 이 모델에서는 이 두 아이디어를 합치고자 했다.<br />
논문에서 이 <strong>catGAN</strong>은 original GAN이 $real, fake$만 구분하던 것을 real인 경우에는 그 class가 무엇인지까지 구분하게($C_1, C_2, …, C_N, C_{fake}$)했다는 점에서 original GAN의 일반화 버전이라고 하였으며, 또한 <a href="https://papers.nips.cc/paper/4154-discriminative-clustering-by-regularized-information-maximization">RIM(Regularized Information Maximization)</a>에서 regularization이 추가가 되었듯 catGAN에선 G가 D에 대한 regularization을 하기 때문에 RIM의 확장판이라고도 하였다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/catGAN1.png" width="50%" /></center>

<p>RIM에서 최적의 unsupervised classifier의 목적함수로 엔트로피를 사용하였듯 catGAN도 목적함수로 엔트로피 개념을 사용한다. 아래는 논문에 나온 그림이다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/catGAN2.png" width="100%" /></center>

<p>왼쪽에서 초록색은 G(generate라고 되어 있다), 보라색은 D를 의미한다. 여기서 H는 엔트로피이다.</p>

<p>오른쪽 그림을 보면, D의 입장에서는:</p>

<ul>
  <li>i) real data는 실제 class label을 딱 하나 갖고 있기 때문에 해당하는 label일 확률만 1에 가깝고 나머지는 0이어야 한다. 따라서 엔트로피( $ H[p(y \vert x, D)] $ )를 최소화한다.</li>
  <li>ii) fake data의 경우 특정 class에 속하지 않기 때문에 class label별로 확률은 비슷해야 한다. 따라서 엔트로피$H[p(y \vert x, G(z))]$를 최대화한다.</li>
  <li>iii) 학습 sample이 특정 class에 속할 확률이 비슷해야 한다는 가정을 했기 때문에, input data $x$에 대한 marginal distribution(주변확률분포)의 엔트로피($H[p(y \vert D)]$)가 최대가 되어야 한다.</li>
</ul>

<p>G의 입장에서는:</p>

<ul>
  <li>D를 속여야 하기 때문에 G가 만든 가짜 데이터는 가짜임에도 특정 class에 속한 것처럼 해야 한다. 즉, D의 i) 경우처럼 엔트로피($H[p(y \vert x, G(z))]$)를 최소화한다.</li>
  <li>생성된 sample은 특정 class에 속할 확률이 비슷해야 하기 때문에 marginal distribution의 엔트로피($H[p(y \vert D)]$)가 최대화되어야 한다.</li>
</ul>

<p>따라서 D와 G의 목적함수를 정리하면,</p>

<script type="math/tex; mode=display">L_D = max_D ~~~ H_{\chi}[p(y| D)] - \mathbb{E}_{x\sim \chi} [H[p(y|x, D)]] + \mathbb{E}_{z\sim P(z)}[H[p(y|G(z), D)]]</script>

<script type="math/tex; mode=display">L_G = min_G ~~~ H_G[p(y| D)] + \mathbb{E}_{z\sim P(z)}[H[p(y|G(z), D)]]</script>

<p>다만 $L_D$의 마지막 항을 직접 구하는 것은 어렵기 때문에, $z \sim P(z) $를 $M$개 뽑아 평균을 계산하는 몬테카를로 방법을 쓴다.</p>

<p>위 목적함수를 사용하여 실험한 결과는 다음과 같다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/catGAN3.png" width="100%" /></center>

<p>Unsupervised catGAN은 9.7%의 error를 보이는 데 반해 $n=100$만의 labeled data가 있는 버전의 경우 error가 1.91%까지 떨어진다. $n=1000$, $n=전체$인 경우 error는 점점 떨어지는 것을 볼 수 있다. 즉, 아주 적은 labeled data를 가진 semi-supervised learning이라도 굉장히 쓸모있다는 뜻이다.</p>

<p>또한 k-means나 RIM과 비교했을 때 두 원을 잘 분리해내는 것을 볼 수 있다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/catGAN4.png" width="100%" /></center>

<p>MNIST나 CIFAR-10 데이터도 잘 생성해내는 것을 확인하였다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/catGAN5.png" width="100%" /></center>

<hr />

<h1 id="semi-supervised-gan">Semi-supervised GAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1606.01583">Semi-supervised GAN</a></strong></p>

<p>2016년 6월 처음 제안되었다.</p>

<p>위의 catGAN과 거의 비슷한 역할을 한다. 전체적인 구조도 비슷하다.</p>

<p>논문 자체가 짧고 목적함수에 대한 내용이 없어서 자세한 설명은 생략한다. 특징을 몇 개만 적자면,</p>

<ul>
  <li>original GAN과는 달리 sigmoid 대신 softmax를 사용하였다. $N+1$개로 분류해야 하니 당연하다.</li>
  <li>DCGAN을 기반으로 작성하였다.</li>
  <li>D가 classifier의 역할을 한다. 그래서 논문에서는 D/C network라고 부른다(D이자 C).</li>
  <li>classifier의 정확도는 sample의 수가 적을 때 CNN보다 더 높다는 것을 보여주었다. sample이 많을 때는 거의 같았다.</li>
  <li>original GAN보다 생성하는 이미지의 품질이 좋다.</li>
</ul>

<center><img src="/public/img/2019-03-20-advanced-GANs/semiGAN.png" width="60%" /></center>

<hr />

<h1 id="lsgan">LSGAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1611.04076">LSGAN</a></strong></p>

<p>2016년 11월 처음 제안되었다.</p>

<p>original GAN의 sigmoid cross entropy loss function은 vanishing gradients 문제가 있고, 따라서 출력 이미지는 실제 이미지에 비해선 분명히 품질이 떨어진다.</p>

<p>아래 그림의 (b)에서, 오른쪽 아래의 가짜 데이터는 D를 잘 속이고 있지만 vanishing gradient(sigmoid 그래프의 양쪽 끝을 생각하라) 문제로 인해 거의 업데이트되지 않고, 따라서 가짜 이미지는 실제 이미지와는 동떨어진 결과를 갖는다.<br />
그러나 (c)처럼 이렇게 경계로부터 멀리 떨어진 sample들을 거리에 penalty를 줘서 경계 근처로 끌어올 수 있다면 가짜 이미지는 실제에 거의 근접하게 될 것이다. LSGAN은 이 아이디어에서 출발한다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/LSGAN1.png" width="100%" /></center>

<p>그래서, D를 위한 loss function을 least squares로 대체하면, 경계(decision boundary)로부터 먼 sample들은 penalty를 받아 경계 근처로 끌려온다.</p>

<p>original GAN의 목적함수는 다음과 같았다.</p>

<script type="math/tex; mode=display">min_G max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[log D(x)] + \mathbb{E}_{x \sim p_{z}(z)}[log (1-D(G(z)))]</script>

<p>LSGAN의 목적함수는 다음과 같다. $a$: fake data label , $b$: real data label.<br />
$c$: G가 원하는 것은 이 $c$라는 값을 D가 fake data라고 믿는 것이다.</p>

<script type="math/tex; mode=display">min_D V_{\text{LSGAN}}(D) = \frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[(D(x)-b)^2] + \frac{1}{2} \mathbb{E}_{x \sim p_{z}(z)}[(D(G(z)) - a)^2]</script>

<script type="math/tex; mode=display">min_G V_{\text{LSGAN}}(G) = \frac{1}{2} \mathbb{E}_{x \sim p_{z}(z)}[(D(G(z)) - c)^2]</script>

<p>이렇게 목적함수를 바꿈으로써 얻는 이득은 두 가지다.</p>

<ol>
  <li>original GAN과는 달리 decision boundary에서 멀리 떨어진 sample을 오랫동안 가만히 두지 않고, 설령 맞는 영역에 위치한다고 해도 이에 penalty를 준다. 이는 결과적으로 G가 이미지를 생성할 때 decision boundary에 최대한 가까운, 즉 실제 이미지에 가깝게 생성하도록 한다.</li>
  <li>멀리 떨어진 sample일수록 square 함수에 의해 penalty를 크게 받는다. 따라서 vanishing gradients 문제가 많이 해소되며, 따라서 학습이 안정적이게 된다. original GAN의 sigmoid는 $\vert x \vert$가 클 때 gradient가 매우 작다.</li>
</ol>

<p>또 한 가지 더: LSGAN의 목적함수를 최적화하는 과정은 $\chi^2$ divergence를 최소화하는 것과 같다.<br />
간략히 설명하면,</p>

<p>original GAN에서는 최적화 과정이 Jensen-Shannon divergence를 최소화하는 것을 보였다.</p>

<script type="math/tex; mode=display">C(G) = KL \biggl( p_{data} \Vert \frac{p_{data}+p_g}{2} \biggr) + KL \biggl( p_{g} \Vert \frac{p_{data}+p_g}{2} \biggr) - log(4)</script>

<p>이제 LSGAN의 목적함수를 확장해 보면,</p>

<script type="math/tex; mode=display">min_D V_{\text{LSGAN}}(D) = \frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[(D(x)-b)^2] + \frac{1}{2} \mathbb{E}_{x \sim p_{z}(z)}[(D(G(z)) - a)^2]</script>

<script type="math/tex; mode=display">min_G V_{\text{LSGAN}}(G) = \frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[(D(x)-c)^2] +  \frac{1}{2} \mathbb{E}_{x \sim p_{z}(z)}[(D(G(z)) - c)^2]</script>

<p>$ V_{\text{LSGAN}}(G) $의 추가된 항은 G의 parameter를 포함하지 않기 때문에 최적값에 영향을 주지 않는다.</p>

<p>우선 G를 고정했을 때 D의 최적값은:</p>

<script type="math/tex; mode=display">D^\ast(x) = {bp_{data}(x) + ap_g(x) \over p_{data}(x) + p_g(x)}</script>

<p>중간 과정을 조금 생략하고 적으면,  $b-c=1, b-a=2$라 했을 때</p>

<script type="math/tex; mode=display">2C(G) = \mathbb{E}_{x \sim p_{data}} [(D^\ast(x)-c)^2] + \mathbb{E}_{x \sim p_{g}} [(D^\ast(x)-c)^2]</script>

<script type="math/tex; mode=display">= \int_\chi {((b-c)(p_d(x) + p_g(x)) - (b-a)p_g(x))^2 \over p_d(x) + p_g(x)} dx</script>

<script type="math/tex; mode=display">= \int_\chi {(2p_g(x) - (p_d(x) + p_g(x)))^2 \over p_d(x) + p_g(x)} dx</script>

<script type="math/tex; mode=display">= \chi^2_{Pearson} (p_d + p_g \Vert 2p_g)</script>

<p>그러므로 LSGAN의 최적화 과정은 $b-c=1, b-a=2$일 때 $p_d + p_g$와 $2p_g$ 사이의 Pearson $\chi^2$ divergence를 최소화하는 과정과 같다.</p>

<p>학습시킬 때 $a, b, c$ 값을 $a=-1, b=1, c=0$ 또는 $a=0, b=c=1$ 등을 쓸 수 있다. 둘 사이의 차이는 실험 결과 별로 없으므로,  논문에서는 후자를 택했다.</p>

<p>LSGAN의 구조는 두 가지가 제안되어 있다. 하나는 112$ \times $112 size의 이미지를 출력하는 모델, 다른 하나는 class 개수가 3470개인 task를 위한 것(한자를 분류한다)인데, 충분히 읽기 쉬운 글자를 만들어내는 것을 볼 수 있다.</p>

<p>아래에 모델 구조를 나타내었다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/LSGAN2.png" width="100%" /></center>

<center><img src="/public/img/2019-03-20-advanced-GANs/LSGAN3.png" width="100%" /></center>

<p>많은 class 수를 가진 경우 생성된 이미지 품질이 좋지 못한데, 이유는 입력 class 종류는 매우 많지만 출력은 하나뿐이기 때문이다. 이를 해결하는 방법은 conditional GAN을 쓰는 것이다.<br />
그러나 one-hot encoding은 너무 비용이 크기 때문에 그 대신 각각의 class에 대응하는 작은 벡터를 linear mapping을 통해 하나 만들어서 모델의 레이어에 붙이는 방식을 썼다. 그 결과가 위 그림과 같으며, 목적함수는 다음과 같이 정의된다:</p>

<script type="math/tex; mode=display">min_D V_{\text{LSGAN}}(D) = \frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[(D(x \vert \Phi(y))-1)^2] + \frac{1}{2} \mathbb{E}_{x \sim p_{z}(z)}[(D(G(z) \vert \Phi(y)))^2]</script>

<script type="math/tex; mode=display">min_G V_{\text{LSGAN}}(G) = \frac{1}{2} \mathbb{E}_{x \sim p_{z}(z)}[(D(G(z \vert \Phi(y))) - 1)^2]</script>

<p>$y$는 label vector, $ \Phi(\cdot) $은 linear mapping 함수이다.</p>

<p>LSUN-bedroom 등 여러 데이터셋에 대한 실험 결과이다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/LSGAN4.png" width="100%" /></center>

<center><img src="/public/img/2019-03-20-advanced-GANs/LSGAN5.png" width="100%" /></center>

<center><img src="/public/img/2019-03-20-advanced-GANs/LSGAN6.png" width="100%" /></center>

<center><img src="/public/img/2019-03-20-advanced-GANs/LSGAN7.png" width="100%" /></center>

<p>마지막 그림의 경우 한자 글자를 꽤 잘 생성해내는 것을 볼 수 있다.</p>

<p>LSGAN도 GAN의 역사에서 꽤 중요한 논문 중 하나이다.</p>

<hr />

<h1 id="wgan">WGAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1701.07875">WGAN</a></strong></p>

<p>2017년 1월 처음 제안되었다.</p>

<p>소스코드: <a href="https://github.com/martinarjovsky/WassersteinGAN">pytorch</a></p>

<p>참고할 만한 사이트: <a href="https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">링크</a></p>

<p><em>이 논문도 f-GAN처럼 수학으로 넘쳐흐른다. 다만 요약하지 않을 뿐</em></p>

<p>이 논문의 수학을 이해하는 데 있어 매우 좋은 참고자료가 있다: <a href="https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i">링크</a></p>

<p>이 논문은 실제 데이터 분포와 가짜 데이터 분포 사이의 거리를 측정하는 방법을 바꿈으로써 GAN이 <em>매우</em> 안정적인 학습을 할 수 있도록 만들었다는 것에 의의가 있다.<br />
기억할 것은 하나다: <strong>거의 대부분의 데이터셋에서 학습이 안정적으로 잘 진행된다</strong>(다만 경우에 따라 약간 느리다고 한다).</p>

<p>original GAN부터 시작해서 GAN의 기본 아이디어는 두 분포 사이의 거리를 최소화하도록 G(와 D)를 잘 학습시키는 것이다. original GAN의 경우 이 최적화 과정이 <em>Jenson-Shannon divergence</em>(JSD)를 최소화하는 것과 같다는 것은 이미 증명되어있다.</p>

<p>그러나 이 JSD는 모든 분포의 거리를 효과적으로 측정해주지 못한다. 예를 들어</p>

<script type="math/tex; mode=display">\mathbb{P}_0(x=0, y>0), \quad \mathbb{P}_\theta(x=\theta, y>0)</script>

<center><img src="/public/img/2019-03-20-advanced-GANs/WGAN1.png" width="50%" /></center>

<p>두 (반직선 형태인) 분포 간의 거리를 JSD로 측정하면,</p>

<script type="math/tex; mode=display">JS ( \mathbb{P}_{0}, \mathbb{P}_\theta ) = 0 \ \ if \ \theta=0, \quad log \ 2 \quad otherwise</script>

<p>즉, $ \theta $가 1이든 0.0001이든 상관없이 두 분포가 얼마나 가까운지에 대한 정보를 JSD는 전혀 제공해주지 못한다. 이는 KL divergence도 마찬가지이다.</p>

<script type="math/tex; mode=display">KL ( \mathbb{P}_{0}, \mathbb{P}_{\theta}) = 0 \ \ if \ \theta=0, \quad \infty  \quad otherwise</script>

<p>참고로 논문에 나온 다른 측정방식으로 <em>Total Variation</em>(TV)이 있는데 별반 다를 것은 없다.</p>

<script type="math/tex; mode=display">\lambda( \mathbb{P}_{0}, \mathbb{P}_{\theta}) = 0 \ \ if \ \theta=0, \quad 1 \quad otherwise</script>

<p>참고로 TV는 이렇게 정의된다.</p>

<script type="math/tex; mode=display">\delta(\mathbb{P}_r, \mathbb{P}_g) = sup_{A \in \Sigma} \vert \mathbb{P}_r(A) - \mathbb{P}_g(A) \vert</script>

<p>그래서 WGAN의 저자들은 이와 비슷한 분포를 가진 경우 등은 GAN이 수렴을 잘 하지 못할 것이라고 하며 분포 간 거리를 측정하는 새로운 <em>Earth-Mover</em>(EM) distance 또는 <em>Wasserstein-1</em> distance라고 부르는 것을 제안했다.</p>

<script type="math/tex; mode=display">W(\mathbb{P}_r, \mathbb{P}_g) = \text{inf}_{\gamma \in \Pi(\mathbb{P}_r, \mathbb{P}_g)} \int d(x, y) \gamma (dxdy)  \\ \qquad = \text{inf}_{\gamma \in \Pi(\mathbb{P}_r, \mathbb{P}_g)} \ \mathbb{E}_{(x, y) \sim \gamma} [ \Vert x - y \Vert ]</script>

<p>$\Pi(\mathbb{P}, \mathbb{Q})$는 두 확률분포 $\mathbb{P}, \mathbb{Q}$의 결합확률분포들의 집합이고, $\gamma$는 그 중 하나이다.<br />
즉 위 식은 모든 결합확률분포 $\Pi(\mathbb{P}, \mathbb{Q})$ 중 $d(x,y)$의 기댓값을 가장 작게 추정한 값이다.</p>

<p>이제 이 식을 위 그림의 두 분포에 적용하면 거리는</p>

<script type="math/tex; mode=display">W(\mathbb{P}_0, \mathbb{P}_\theta) = \vert \theta \vert</script>

<p>로 아주 적절하게 나온다.</p>

<p>그래서 이렇게 나온 Wasserstein distance는 <span>$\mathbb{P}_r$</span>과 <span>$\mathbb{P}_\theta$</span> 사이의 거리를 <span>$\mathbb{P}_r$</span>를 <span>$\mathbb{P}_\theta$</span>로 옮길 때 필요한 양과 거리의 곱으로 측정한다.<br />
이를 어떤 산(분포) 전체를 옮기는 것과 같다고 해서 <em>Earth Mover</em> 또는 EM distance라고 불린다.</p>

<script type="math/tex; mode=display">Cost = mass \times distance</script>

<p>original GAN과 목적함수의 차이를 비교하면,</p>

<table>
  <thead>
    <tr>
      <th>name</th>
      <th>Discriminator</th>
      <th>Generator</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GAN</td>
      <td>$\nabla_{\theta_d} \frac{1}{m} \sum^m_{i=1} \ [log D(x^{(i)}) + log (1-D(G(z^{(i)})))] $</td>
      <td>$\nabla_{\theta_g} \ \frac{1}{m} \sum^m_{i=1} log (D(G(z^{(i)}))) $</td>
    </tr>
    <tr>
      <td>WGAN</td>
      <td>$\nabla_w \frac{1}{m} \ \sum^m_{i=1} \ [f(x^{(i)}) + f(G(z^{(i)}))] $</td>
      <td>$\nabla_{\theta} \frac{1}{m} \ \sum^m_{i=1} \ f(G(z^{(i)})) $</td>
    </tr>
  </tbody>
</table>

<p>차이점이 더 있는데,</p>
<ul>
  <li>$f$는 <em>k</em>-Lipschitz function이어야 한다. 이를 위해 WGAN에서는 단순히 $[c, -c]$로 clipping한다.</li>
  <li>log_sigmoid를 사용하지 않는다.</li>
</ul>

<p>이제 WGAN 논문에 제시된 알고리즘을 보자.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/WGAN2.png" width="100%" /></center>

<p>알고리즘에 굉장히 특별하진 않다. optimizer로 <em>RMSProp</em>을 사용한 것이 약간의 차이점이다.</p>

<p>학습 과정에서의 장점을 보여주는 그림이 논문에 제시되어 있다. 두 Gaussian 분포를 볼 때 GAN의 수렴이 훨씬 잘 된다는 말이다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/WGAN3.png" width="100%" /></center>

<p>WGAN 실험 결과를 보면 다음과 같다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/WGAN4.png" width="100%" /></center>

<p><script type="math/tex">\\</script>
사실 이 논문은 부록을 포함해 32page짜리 논문으로 수학이 넘쳐흐르지만, 필자의 논문 리뷰는 이 논문이 무슨 내용인지 정도만 전달하려는, 내용을 적당히 요약하여 보여주는 것이 목적이므로 자세한 수식 및 증명 과정은 따로 적지 않는다.</p>

<p><del>궁금하면 직접 읽으면 된다</del></p>

<hr />

<h1 id="improved-wgan">Improved WGAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1704.00028">WGAN_GP</a></strong></p>

<p>2017년 3월 처음 제안되었다.</p>

<p>소스코드: <a href="https://github.com/caogang/wgan-gp">pytorch</a></p>

<p>참고할 만한 사이트: <a href="https://medium.com/@jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490">링크</a></p>

<p>WGAN은 clipping을 통해 Lipschitz 함수 제약을 해결하긴 했지만, 이는 예상치 못한 결과를 초래할 수 있다:</p>

<blockquote>
  <p>(WGAN 논문에서 인용)<br />
만약 clipping parameter($c$)가 너무 크다면, 어떤 weights든 그 한계에 다다르기까지 오랜 시간이 걸릴 것이며, 따라서 D가 최적화되기까지 오랜 시간이 걸린다.<br />
반대로 $c$가 너무 작다면, 레이어가 크거나 BatchNorm을 쓰지 않는다면 쉽게 vanishing gradients 문제가 생길 수 있다.</p>
</blockquote>

<p>clipping은 단순하지만 문제를 발생시킬 수 있다. 특히 $c$가 잘 정해지지 않았다면 품질이 낮은 이미지를 생성하고 수렴하지 않을 수 있다. 모델의 성능은 이 $c$에 매우 민감하다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/WGAN_GP1.png" width="100%" /></center>

<p>가중치 clipping은 가중치를 정규화하는 효과를 갖는다. 이는 모델 $f$의 어떤 한계치를 설정하는 것과 같다.</p>

<p>그래서 이 논문에서는 <em>gradient penalty</em>라는 것을 D의 목적함수에 추가해 이 한계를 극복하고자 한다(G의 목적함수는 건드리지 않은 듯 하다).</p>

<script type="math/tex; mode=display">L = \mathbb{E}_{\hat{x} \sim \mathbb{P}_g} \ [D(\hat{x})] - \mathbb{E}_{x \sim \mathbb{P}_r} \ [D(x)] + \lambda \ \mathbb{E}_{\hat{x} \sim \mathbb{P}_{\hat{x}}} \ [(\Vert \nabla_{\hat{x}}D(\hat{x}) \Vert_2 - 1)^2 ]</script>

<p>즉 clipping을 적용하는 대신 WGAN_GP는 gradient norm이 목표인 $1$에서 멀어지면 penalty를 주는 방식을 택했다.</p>

<ul>
  <li><strong>Sampling Distribution:</strong> $\mathbb{P}_{\hat{x}}$는 실제 데이터 분포 $\mathbb{P}_r$과 G가 생성한 데이터 분포 $\mathbb{P}_g$로부터 추출한 point 쌍들 사이에 직선을 하나 그어서 얻은 것이다.</li>
  <li><strong>Penalty coefficient:</strong> $\lambda$가 붙은 마지막 항(이 논문에서는 $\lambda=10$으로 고정됨)이 gradient penalty이다.</li>
  <li><strong>No critic batch normalization:</strong> BN은 D의 문제의 형식을 1-1 매칭 문제에서 전체 batch input-batch output으로 바꿔버린다. 이 논문에서 새로 만든 gradient penalty 목적함수는 이 조건에 맞지 않기 때문에 BN을 쓰지 않았다.</li>
  <li><strong>Two-sided penalty:</strong> gradient가 단지 $1$ 아래로 내려가는 것을 막는(one-sided) 대신 $1$ 근처에 머무르도록 했다(two-sided).</li>
</ul>

<p>그래서 발전시킨 알고리즘은 다음과 같다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/WGAN_GP2.png" width="100%" /></center>

<p>좀 특이하게도 이 논문에는 모델 구조(architecture)를 바꿔가면서 한 실험 결과가 있다. 확실히 WGAN_GP 버전이 뛰어남을 볼 수 있다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/WGAN_GP3.png" width="100%" /></center>

<p>WGAN_GP만이 (이 논문에서 실험한) 모든 architecture에 대해서 제대로 된 학습에 성공하였다고 한다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/WGAN_GP4.png" width="100%" /></center>

<p>여러 실험 결과들이 더 있지만 하나만 더 소개하면,<br />
논문에서는 아래 이미지(LSUN-bedroom)가 지금까지의 연구에 의해 나온 것 중 제일 잘 나온 것이라고 믿는다고 한다. 각각의 이미지가 $128 \times 128 $ 크기라 그다지 고해상도는 아니긴 하지만 어쨌든 실제로 꽤 깨끗한 이미지로 보인다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/WGAN_GP5.png" width="100%" /></center>

<script type="math/tex; mode=display">\\</script>

<p>종합하면 이 개선된 버전은 데이터셋뿐만 아니라(WGAN) 모델 구조에 대해서도(architecture) 학습 안정성을 얻었다고 할 수 있다.</p>

<hr />

<h1 id="dragan">DRAGAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1705.07215">DRAGAN</a></strong></p>

<p>2017년 5월 처음 제안되었다.</p>

<p>소스코드: <a href="https://github.com/kodalinaveen3/DRAGAN?fbclid=IwAR3mPLo134C3xx4QerWUCCTWqCVfH7seDkPK5Rlkr_trAjxwYfCHWvcs1dk">tensorflow</a>, <a href="https://github.com/jfsantos/dragan-pytorch">pytorch</a></p>

<p>참고할 만한 사이트: <a href="https://lernapparat.de/more-improved-wgan/">링크</a></p>

<p>WGAN_GP 논문과 차이점은 D(critic network)에 의해 계산되는 식별함수 $f$가 gradient에 있어 어떤 제한을 받는가이다.</p>
<ul>
  <li>WGAN_GP에서는 gradient가 실제 데이터와 가짜 데이터 사이의 직선 위 랜덤한 곳으로 설정되기 때문에 모든 곳에서 $ \vert \nabla f \vert = 1 $를 향한다.</li>
  <li>DRAGAN에서는 gradient가 실제에 “가깝게” sampling된다. 이는  실제 데이터 근처에 있을 때만 $ \vert \nabla f \vert = 1 $를 향한다.</li>
</ul>

<p>아래 그림은 위 차이를 보여준다. <a href="(https://lernapparat.de/more-improved-wgan/)">참고 사이트</a>에서 가져왔다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/DRAGAN1.png" width="80%" /></center>

<p>간단히 DRAGAN은 실제 데이터 분포(manifold)에 가까울 때만 gradient penalizing을 시켜 <a href="https://greeksharifa.github.io/generative%20model/2019/03/03/GAN/#mode-collapsing">mode collapsing</a>을 막을 수 있다.</p>

<p>$ \lambda $가 penalty hyperparameter로 사용되는데, 작은 $\lambda$는 toy tasks에 있어 특히 잘 학습됨을 볼 수 있다.</p>

<p>이 논문이 기여한 바는 다음과 같다:</p>

<ul>
  <li>AGD를 regret minimization으로 봄으로써 GAN 학습에 대한 추론을 제안하였다.</li>
  <li>nonparametric 한계 안에서 GAN 학습의 점근적 수렴과 매 단계마다 D가 최적이어야 할 필요가 없다는 것을 증명하였다.</li>
  <li>AGD가 비 볼록(non-convex) 게임에서 잠재적으로 어떻게 나쁜 국소평형 지점(local minima)으로 수렴하는지와 이것이 GAN의 학습에 있어 mode collapsing에 얼마나 큰 책임이 있는지를 논했다.</li>
  <li>실제 데이터에 근접한 경우에 D의 $f$의 gradient가 큰 값을 가질 때 어떻게 mode collapse 상황이 생기는지를 특징지었다.</li>
  <li>이러한 관찰에 의해 DRAGAN(a novel gradient penalty scheme)을 소개하였고 이것이 mode collapsing 문제를 완화해준다는 것을 보였다.</li>
</ul>

<p>원래의 GAN들은, sample이 real data에 가까움에도 sharp gradient를 갖기 떄문에 mode collapse의 정의에 의해 이것이 나타난다. 이러한 sharp gradient는 G가 많은 $z$ 벡터들을 하나의 출력값 $x$로 가게끔 하고 따라서 형평성(equilibrium, mode collapse의 정의를 생각하라)을 약화시키도록 한다.<br />
그래서 이러한 실패를 막으려면 D에게 다음과 같은 penalty를 줘서 gradient를 정규화시키는 것이다:</p>

<script type="math/tex; mode=display">\lambda \ \cdot \ \mathbb{E}_{x \sim P_{real}, \ \delta \sim N_d (0, \ cI)} [\Vert \nabla_X D_\theta(x+\delta) \Vert^2 ]</script>

<p>이 전략은 GAN 학습의 안정성을 증가시킨다. 이 논문에는 그 결과와 그렇게 되는 이유가 설명되어 있으니 자세한 부분은 이를 참고하자.</p>

<p>그러나, 이 논문에서는 위의 penalty 식이 여전히 불안정하며 지나치게 penalty를 주는(over-penalized) 경우가 있을 수 있고, 따라서 D는 real point $x$와 noise인 $x+\lambda$에게 동일한 “실제 데이터일” 확률을 부여할 수 있다는 것을 발견하였다. 따라서 더 나은 gradient penalty 식은</p>

<script type="math/tex; mode=display">\lambda \ \cdot \ \mathbb{E}_{x \sim P_{real}, \ \delta \sim N_d (0, \ cI)} [ \ max(0, \ \Vert \nabla_X D_\theta(x+\delta) \Vert^2 - k )\ ]</script>

<p>그리고, 실험적인 최적화를 적용한 최종 penalty 식은</p>

<script type="math/tex; mode=display">\lambda \ \cdot \ \mathbb{E}_{x \sim P_{real}, \ \delta \sim N_d (0, \ cI)} [ \ \Vert \nabla_X D_\theta(x+\delta) \Vert - k \ ]^2</script>

<p>결과적으로 real data의 작은 동요(변화, perturbations)에도 잘 작동하였다.</p>

<p>이 논문에서 사용한 gradient penalty schemes 또는 heuristics는 DRAGAN algorithm으로 부르기로 하였다.</p>

<hr />

<h1 id="ebgan">EBGAN</h1>

<p>2016년 9월 처음 제안되었다.</p>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1609.03126">EBGAN</a></strong></p>

<p>이 논문에서는 D를 data manifold에 가까운 지점에서는 낮은 에너지를, 그렇지 않은 지점에서는 높은 에너지를 갖도록 하는 일종의 energy function으로 보는 Energy-Based GAN을 소개한다. 일반 GAN과 비슷하게 G는 최대한 낮은 에너지를 갖는(즉, 실제 데이터와 비슷한) sample을 생성하고, D는 G가 생성한 이미지들에는 높은 에너지를 부여하도록 한다.<br />
D를 energy function으로 봄으로써 다양한 architecture과 loss function에 사용할 수 있게 되었다.  이 논문에서는 D를 auto-encoder로 구현하였다.<br />
결과적으로 EBGAN은 학습이 더 안정적이며 또한 고해상도 이미지를 생성하는 데에도 능하다는 것을 보여주었다.</p>

<p>우선 Energy Based Model은,</p>
<ul>
  <li>LeCun이 2006년 제안하였으며</li>
  <li>input space를 하나의 scalar(energy로 지칭된다)로 mapping하는 모델이다.</li>
  <li>학습이 제대로 된 경우 낮은 에너지를, 아니면 높은 에너지를 생성하며</li>
  <li>CNN등의 학습에서 cross entropy loss를 사용하여 loss를 낮춰가는 것과 비슷하다. 여기선 loss랑 energy랑 비슷하게 사용된다.</li>
</ul>

<p>간단히 이 Energy Based Model을 GAN에 적용시킨 것이 EBGAN이다.</p>

<p>이 논문의 contribution은,</p>
<ul>
  <li>GAN 학습에 energy-based를 적용시켰고</li>
  <li>simple hinge loss에 대해, 시스템이 수렴했을 때 G는 데이터 분포를 따르는 point를 생성하게 된다는 증명과</li>
  <li>energy를 reconstruction error로 본 auto-encoder architecture로 EBGAN framework를 만들었고</li>
  <li>EBGAN과 확률적 GAN 모두에게 좋은 결과를 얻을 수 있는 시스템적 실험셋(hyperparameter 등)</li>
  <li>ImageNet 데이터셋에 대해 256$\times$256 고해상도 이미질르 생성할 수 있음을 보여주었다.</li>
</ul>

<p>목적함수는 다음과 같이 정의된다. $[\cdot]^+ = max(0,\  \cdot)$이다.</p>

<script type="math/tex; mode=display">\mathcal{L}_D(x, z) = D(x) + [m - D(G(z))]^{+}</script>

<script type="math/tex; mode=display">\mathcal{L}_G(z) = D(G(z))</script>

<p>EBGAN은 per-pixel Euclidean distance를 사용한다.</p>

<p>찾아낸 해가 optimum인지에 대한 증명은 Theorem 1과 2로 나누어져 증명이 논문에 수록되어 있다. 간략히 소개하기엔 꽤 복잡하므로 넘어간다.</p>

<p>D의 구조를 나타내면 다음과 같다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/EBGAN1.png" width="100%" /></center>

<p>왜 auto-encoder를 썼냐 하면:</p>

<ul>
  <li>D가 오직 0과 1 두 값만 낸다면 한 minibatch 안에서 많은 다른 sample들이 orthogonal에서 멀어질 것임을 뜻한다. 이는 비효율적인 학습을 초래하며, minibatch size를 줄이는 것은 현재 하드웨어 상으로 별로 좋은 옵션이 아니다. 그래서 이 대신 reconstruction-based output을 씀으로써 D에게 좀 더 다양한 target을 제공한다.</li>
  <li>Auto-encoder는 전통적으로 energy-based model을 표현하는 좋은 모델이다. auto-encoder는 supervision이나 negative sample 같은 것 없이도 energy manifold를 잘 학습할 수 있다. 이는 EBGAN auto-encoding model이 <em>실제</em> 데이터를 복원하도록 학습했을 때, D는 그 data manifold를 스스로 찾아낼 수 있다는 뜻이다. 반대로 G로부터의 negative sample이 없다면 binary logistic loss로 학습된 D는 무의미하다는 뜻이기도 하다.</li>
</ul>

<p>이 논문에서는 <strong>repelling regularizer</strong>라는 것을 제안하는데, 이는 모델이 겨우 몇 개의 $p_{data}$로 뭉쳐 있는 sample들을 생성하는 것을 고의로 막기 위한 것으로 EBGAN auto-encoder model에 최적화된 것이다. <br />
Pulling-away Term, PT는 다음과 같이 정의된다:</p>

<script type="math/tex; mode=display">f_{PT}(S) = \frac{1}{N(N-1)} \sum_i \sum_{j \ne i} \Bigl( \frac{S_i S_j}{\Vert S_i \Vert \Vert S_j \Vert } \Bigr)^2</script>

<p>PT는 minibatch 상에서 동작하고 쌍으로 sample representation을 orthogonalize하려고 한다. 논문에서는 PT로 학습된 EBGAN auto-encoder model을 <strong>EBGAN-PT</strong>라고 부르기로 하였다.</p>

<p>이 논문의 실험결과는 다른 GAN과는 약간 다르다. <em>Inception score</em>를 생성 품질을 측정하는 척도로 사용하여 GAN과 EBGAN의 생성 품질을 비교한 것이다. 점수가 높을수록 품질이 좋은 것이도, 각 막대그래프는 해당 점수를 가진 sample의 비율이 얼마나 되는지를 나타낸 것이다. 따라서 각 막대가 오른쪽에 많이 분포할수록 생성 품질이 좋다고 할 수 있다.<br />
아래 그림은 일부만 가져온 것이다. 논문에서도 그림이 너무 작으니 pdf에서 확대해서 보라는 것을 추천하고 있다. 그림이 15개 정도 있는데, 실험 조건만 다를 뿐 대부분 비슷한 분포를 보이고 있다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/EBGAN2.png" width="80%" /></center>

<p>일반 GAN과 비교하면 MNIST 생성 품질도 확실히 좋은 것을 볼 수 있다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/EBGAN3.png" width="100%" /></center>

<p>또 LSUN, CELEBA, ImageNet 데이터셋에 대해서도 실험한 결과들이 논문에 실려 있다. 대부분의 이미지는 품질이 훨씬 좋고 선명한 이미지 품질을 볼 수 있다.</p>

<hr />

<h1 id="began">BEGAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1703.10717">BEGAN</a></strong></p>

<p>2017년 3월 처음 제안되었다.</p>

<p>구글이 내놓은 GAN 논문이다. 이 논문에서 중요한 특징 및 개선점은,</p>

<ul>
  <li>모델 구조는 더 단순해졌고, 여전히 빠르고 안정적인 학습이 가능하다.</li>
  <li>EBGAN을 바탕으로 해 energy와 auto-encoder를 사용한다. 다만 loss는 WGAN의 Wasserstein distance를 사용한다.</li>
  <li>대부분의 GAN이 ‘실제 데이터 분포’와 ‘가짜 데이터 분포’ 사이의 거리를 좁히기 위해 노력해왔다면, BEGAN은 ‘진짜 데이터에 대한 auto-encoder 데이터 분포’와 ‘가짜 데이터에 대한 auto-encoder 데이터 분포’ 사이의 거리를 계산한다.</li>
  <li>D가 G를 압도하는 상황이 발생하는 것을 막기 위해 D와 G의 equilibrium을 조절하는 hyperparameter $\gamma$를 도입하였다. <em>diversity ratio</em>라고 부른다는데, 이것으로
    <ul>
      <li>auto-encoder가 데이터를 복원하는 것과 진짜/가짜를 구별하는 것 사이의 균형을 맞추고</li>
      <li>$\gamma$가 낮으면 auto-encoder가 새 이미지를 생성하는 것에 집중한다는 것이므로 이미지 다양성이 떨어진다. 반대는 당연히 반대의 효과를 가진다.</li>
      <li>이 equilibrium 개념을 가져와서 수렴(즉, 학습)이 잘 되었는지를 판별하는 데 쓸 수도 있다.</li>
    </ul>
  </li>
</ul>

<p>이 논문은 결과에 비해 수식이 꽤 단순한 편이다.</p>

<p><strong>auto-encoder의 Wasserstein distance 하한</strong></p>

<p>우선 pixel-wise auto-encoder를 학습할 때 $ \mathcal{L}: \mathbb{R}^{N_x} \mapsto \mathbb{R}^+$ 를 정의하면,</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/BEGAN1.png" width="100%" /></center>

<p>$\mu_{1, 2}$를 auto-encoder loss의 두 분포라 하고, $\Gamma(\mu_1, \mu_2)$를 모든 $\mu_1$과 $\mu_2$의 결합들의 집합이라 하고, $m_{1, 2} \in \mathbb{R}$을 각 평균이라 하면, Wasserstein distance는</p>

<script type="math/tex; mode=display">W_1(\mu_1, \mu_2) = inf_{\gamma \in \Gamma(\mu_1, \mu_2)} \ \mathbb{E}_{(x_1, x_2) \sim \gamma} [\vert x_1 - x_2 \vert ]</script>

<p>Jensen’s inequality를 써서</p>

<script type="math/tex; mode=display">inf \mathbb{E}[ \vert x_1 - x_2 \vert ] \geqslant inf \vert \mathbb{E}[x_1 - x_2] \vert = \vert m_1 - m_2 \vert</script>

<p>데이터 분포 간 사이의 거리를 구하려는 것이 아니라 auto-encoder loss distribution의 Wasserstein distance를 구하려고 하는 것이라는 것을 알아둘 필요가 있다.</p>

<p>GAN의 목적함수에서, $\vert m_1 - m_2 \vert $를 최대화하는 것은 딱 두 가지인데, $m_1$이 0으로 가는 것이 auto-encoder가 실제 이미지를 생성하는 것으로 자연스럽기 때문에 선택한 것은 다음 중 (b)이다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/BEGAN2.png" width="100%" /></center>

<p>GAN의 목적함수를 정리하면,</p>

<script type="math/tex; mode=display">\mathcal{L}_D = \mathcal{L}(x;\theta_D) - \mathcal{L}(G(z_D;\theta_G);\theta_D) \qquad \text{for} \ \ \theta_D</script>

<script type="math/tex; mode=display">\mathcal{L}_G = -\mathcal{L}_D \qquad \qquad \qquad \qquad \qquad \qquad \text{for} \ \ \theta_G</script>

<p>참고: $ G(\cdot) = G(\cdot, \ \theta_G), \mathcal{L}(\cdot) = \mathcal{L}(\cdot ; \ \theta_D)$이다.</p>

<p><strong>D와 G의 평형(equilibrium)</strong></p>

<p>만약 평헝이 이루어졌다면 다음은 당연하다:</p>

<script type="math/tex; mode=display">\mathbb{E} [ \mathcal{L}(x)] = \mathbb{E}[\mathcal{L}(G(z))]</script>

<p>한쪽이 지나치게 강해지는 것을 막기 위해, diversity ratio $\gamma$를 정의하였다:</p>

<script type="math/tex; mode=display">\gamma = \frac{\mathbb{E}[\mathcal{L}(G(z))]}{ \mathbb{E}[\mathcal{L}(x)] } \in [0, 1]</script>

<p>이것으로 조금 위에서 말한 이미지의 다양성과 품질 간 trade-off, D와 G의 평형 등을 모두 얻을 수 있다.</p>

<p><strong>BEGAN의 목적함수</strong></p>

<center><img src="/public/img/2019-03-20-advanced-GANs/BEGAN3.png" width="100%" /></center>

<ul>
  <li>$ \mathbb{E}[\mathcal{L}(G(z))]  = \gamma \mathbb{E}[ \mathcal{L}(x)] $를 유지하기 위해 Proportional Control Theory를 사용하였다.
    <ul>
      <li>$k_t \in [0, 1]$를 사용하여 얼마나 경사하강법 중 $\mathcal{L}(G(z))$를 강조할 것인지를 조절한다.</li>
      <li>$k_0 = 0$</li>
      <li>t가 지날수록 값이 커진다.</li>
    </ul>
  </li>
  <li>$\lambda_k$는 learning rate와 비슷하다.</li>
</ul>

<p><strong>수렴 판별 방법</strong></p>

<p>조금 전의 equilibrium 컨셉을 생각해서, 수렴과정을 가장 가까운 복원 $\mathcal{L}(x)$를 찾는 것으로 생각할 수 있다.</p>

<p>수렴 측정방법은 다음과 같이 표현 가능하다:</p>

<script type="math/tex; mode=display">\mathcal{M}_{global} = \mathcal{L}(x) + \vert \gamma \mathcal{L}(x) - \mathcal{L}(G(z_G)) \vert</script>

<p>이는 모델이 잘 학습되어 최종 상태에 도달했는지, 아니면 mode collapsing했는지를 판별할 때 쓸 수 있다.</p>

<p><strong>Model architecture</strong></p>

<center><img src="/public/img/2019-03-20-advanced-GANs/BEGAN4.png" width="100%" /></center>

<p>DCGAN과는 달리</p>

<ul>
  <li>batch norm</li>
  <li>dropout</li>
  <li>transpose convolution</li>
  <li>exponential growth for convolution filters</li>
</ul>

<p>등이 다 없다. 모델 구조가 상당히 단순함을 알 수 있다.</p>

<p><strong>실험 결과</strong></p>

<p>간단히 말하면..좋다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/BEGAN5.png" width="100%" /></center>

<p>예전에 <a href="https://greeksharifa.github.io/generative%20model/2019/03/17/DCGAN/">DCGAN</a>에서 봤던 interpolating도 잘 됨을 확인할 수 있다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/BEGAN6.png" width="100%" /></center>

<hr />

<h1 id="acgan">ACGAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1610.09585">ACGAN</a></strong></p>

<p>2016년 10월 처음 제안되었다.</p>

<p><a href="https://greeksharifa.github.io/generative%20model/2019/03/17/DCGAN/">DCGAN</a>에서는 $z$가 속한 벡터공간의 각 차원별 특징은 사람이 해석할 수 없는 수준이다. 즉 $z$의 요소를 변화시킬 때 사진이 변화하는 형상은 알 수 있지만, 각각의 차원이 정확히 무슨 역할을 하고 어떤 특징을 갖는지는 알 수가 없다.<br />
그러나 해석하기 쉬운 특징량(disentangled latend code)에 의존하는 모델들이 여럿 제안되었는데, 그것은 앞에서 설명했던 <a href="https://greeksharifa.github.io/generative%20model/2019/03/19/CGAN/">CGAN</a>, ACGAN, <a href="https://greeksharifa.github.io/generative%20model/2019/03/20/advanced-GANs/#infogan">infoGAN</a> 등이 있다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/ACGAN1.png" width="50%" /></center>

<p>ACGAN이 original GAN 및 CGAN과 다른 점은,</p>

<ul>
  <li>D는 2개의 분류기로 구성되는데
    <ul>
      <li>하나는 original GAN과 같은 real/fake 판별</li>
      <li>다른 하나는 데이터의 class 판별</li>
    </ul>
  </li>
  <li>목적함수: 맞는 <strong>S</strong>ource의 log-likelihood $L_S$, 맞는 <strong>C</strong>lass의 log-likelihood $L_C$ 두 개로 나누어
    <ul>
      <li>$L_S$는 기존 GAN의 목적함수와 같다. 즉 real/fake를 판별하는 것과 관련이 있다.</li>
      <li>$L_C$는 그 데이터의 class를 판별하는 것과 관련이 있다. CGAN에서 본 것과 약간 비슷하다.</li>
      <li>D는 $L_S+L_C$를 최대화하고</li>
      <li>G는 $L_C-L_S$를 최대화하도록 학습된다.</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">L_S = E[log \ p(S=real \vert X_{real})] +  E[log \ p(S=fake \vert X_{fake})]</script>

<script type="math/tex; mode=display">L_C = E[log \ p(C=c \quad \ \vert X_{real})] + E[log \ p(C=c \ \ \quad  \vert X_{fake})]</script>

<p>실험은 ImageNet과 CIFAR-10에 대해 진행하였다고 한다. 결과는 (위의 BEGAN에 비해) 아주 놀랍지는 않아서(물론 예전 논문이다) 생략한다.</p>

<p>대신 실험 시 사용한 모델 구조를 가져왔다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/ACGAN2.png" width="100%" /></center>

<center><img src="/public/img/2019-03-20-advanced-GANs/ACGAN3.png" width="100%" /></center>

<hr />

<h1 id="infogan">infoGAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1606.03657">infoGAN</a></strong></p>

<p>2016년 6월 처음 제안되었다.</p>

<p>original GAN은 input vector $z$에 어떠한 제한도 없이 단순히 무작위 값을 집어넣었기 때문에, 이러한 $z$의 각 차원은 역할이 분리되지 않고 심하게 꼬여(entangled) 있다.<br />
그러나 이 domain들은 서로 다른 역할을 하는 여러 부분으로 분리될 수 있다.</p>

<p>그래서 이 논문에서는 noise 부분 $z$와, 데이터 분포의 가장 중요한 의미를 가지는 특징량(latent code) $c$ 두 부분으로 나누었다(CGAN과 비슷). 특징량은 설명 가능한 부분(semantic features), $z$는 원래의 것처럼 데이터를 생성하기 위한 incompressible noise이다.</p>

<p>G에 들어가는 input은 따라서 $G(z, c)$로 표시된다. 그러나 기존 GAN은 단지 $P_G(x \vert c) = P_G(x)$로 처리함으로써 특징량 $c$를 무시해버릴 수 있다. 따라서 정보이론적 정규화를 시행하도록 한다: $c$와 $G(z, c)$ 사이에는 아주 높은 상호정보량이 있기 때문에, $I(c;\ G(z,c))$ 역시 높을 것이다.</p>

<p>참고: 상호정보량은 다음과 같이 KLD로 측정한다. 서로 독립인 경우 0이 되는 것은 상호정보량의 이름에서 봤을 때 직관적이다.</p>

<script type="math/tex; mode=display">I(X;Y) = D_{KL}(p(x,y) \Vert p(x)p(y))</script>

<center><img src="/public/img/2019-03-20-advanced-GANs/infoGAN1.png" width="50%" /></center>

<p>그래서 목적함수는 다음과 같다.</p>

<script type="math/tex; mode=display">min_G max_D V_I(D, G) = V(D, G) - \lambda I(c; G(z, c))</script>

<p>$V(D, G)$는 기존 GAN의 목적함수이다.</p>

<p>상호정보량은 쉽게 구하긴 어렵기 때문에, 논문에서는 이를 직접적으로 구하는 대신 하한을 구해 이를 최대화하는 방식을 썼다. 수식을 중간과정을 일부 생략하고 적으면</p>

<script type="math/tex; mode=display">I(c; G(z, c)) = H(c) - H(c \vert G(z, c)) = \mathbb{E}_{x \sim G(z,c)} [ \mathbb{E}_{c' \sim P(c \vert x)}[log \ P(c' \vert x)]] + H(c)</script>

<script type="math/tex; mode=display">\qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \ \  \ge \mathbb{E}_{x \sim G(z,c)} [ \mathbb{E}_{c' \sim P(c \vert x)}[log \ Q(c' \vert x)]] + H(c)</script>

<p>상호정보량 I(c; G(z, c))의  variational lower bound $L_I(G, Q)$를 정의할 수 있는데,</p>

<script type="math/tex; mode=display">L_I(G, Q) = E_{c \sim P(c), x \sim G(z, c)}[log \ Q(c \vert x)] + H(c)</script>

<script type="math/tex; mode=display">\qquad \qquad \qquad \ = E_{x \sim G(z,c)} [ \mathbb{E}_{c' \sim P(c \vert x)}[log \ Q(c' \vert x)]] + H(c)</script>

<script type="math/tex; mode=display">\le  I(c; G(z, c)) \ \qquad \qquad \quad</script>

<p>그래서 infoGAN은 아래 minimax game을 하는 것이 된다:</p>

<script type="math/tex; mode=display">min_{G, Q} max_D V_{\text{infoGAN}}(D, G, Q) = V(D, G) - \lambda L_I(G,Q)</script>

<p><strong>실험 결과</strong></p>

<p>semantic features $c$를 적절히 조작하면 생성될 이미지에 어떤 변화를 줄 수 있는지를 중점적으로 보여주었다.<br />
MNIST의 경우 숫자의 종류(digit), 회전, 너비 등을 조작할 수 있고, 사람 얼굴의 경우 얼굴의 각도, 밝기, 너비 등을 바꿀 수 있음을 보여주었다.</p>

<center><img src="/public/img/2019-03-20-advanced-GANs/infoGAN2.png" width="100%" /></center>

<center><img src="/public/img/2019-03-20-advanced-GANs/infoGAN3.png" width="100%" /></center>

<p>더 많은 결과는 논문을 참조하자.</p>

<hr />

<h1 id="이후-연구들">이후 연구들</h1>

<p>GAN 이후로 수많은 발전된 GAN이 연구되어 발표되었다.<br />
GAN 학습에 관한 내용을 정리한 것으로는 다음 논문이 있다. <a href="https://arxiv.org/abs/1606.03498">Improved Techniques for Training GANs</a></p>

<p>또 다른 것으로는 PROGDAN, SLOGAN 등이 있다.</p>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/advanced-GANs/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/advanced-GANs/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/f-GAN/">
        f-GAN
      </a>
    </h1>

    <span class="post-date">19 Mar 2019</span>
     |
    
    <a href="/blog/tags/#gan" class="post-tag">GAN</a>
    
    <a href="/blog/tags/#machine-learning" class="post-tag">Machine Learning</a>
    
    <a href="/blog/tags/#cnn" class="post-tag">CNN</a>
    
    <a href="/blog/tags/#generative-model" class="post-tag">Generative Model</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <hr />

<p>이 글에서는 2016년 6월 <em>Sebastian Nowozin</em> 등이 발표한 <em>f</em>-GAN - Training Generative Neural Samplers using Variational Divergence Minimization를 살펴보도록 한다.</p>

<p><em>f</em>-GAN은 특정한 구조를 제안했다기보다는 약간 divergence에 대한 내용을 일반적으로 증명한 수학 논문에 가깝다.</p>

<p>중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.</p>

<p><em>이 논문은 수학이 넘쳐흐르는 논문이다.</em></p>

<hr />

<h1 id="f-gan"><em>f</em>-GAN</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1606.00709"><em>f</em>-GAN</a></strong></p>

<h2 id="초록abstract">초록(Abstract)</h2>

<p>2016년에 나온 논문임을 생각하라.</p>

<p>Generative neural sampler는 random input vector를 입력으로 받아 network weights에 정의된 확률분포로부터 sample을 만들어내는 확률적 모델이다. 이 모델들은 sample과 도함수 계산이 효율적이지만 우도(likelihood)나 주변화(marginalization)을 계산하진 못한다. 적대생성적 학습방법은 이런 모델이 추가 신경망을 통해 이를 학습할 수 있게 해준다.<br />
우리는 이 적대생성적 접근법이 더 일반적인 변분 발산(variational divergence) 추정 접근의 특별한 경우임을 보일 것이다. 우리는 임의의 <em>f-divergence</em>가 Generative neural sampler에 쓰일 수 있음을 보일 것이다. 우리는 이렇게 다양한 divergence 함수를 쓸 수 있는 것이 학습 복잡도와 생성모델의 품질 면에서 이득임을 논할 것이다.</p>

<hr />

<h2 id="서론introduction">서론(Introduction)</h2>

<p>확률적 생성모델은 주어진 domain $\chi$ 상의 확률분포를 서술한다. 예를 들면 자연언어 문장, 자연 이미지, 녹음된 파형 등의 분포가 있다.</p>

<p>가능한 모델 집합 $Q$에서 생성모델 Q가 주어졌을 때 우리는 일반적으로 다음에 관심이 있다:</p>
<ul>
  <li>Sampling. Q로부터 sample을 생성한다. sample을 살펴보거나 어떤 함숫값을 계산해봄으로써 우리는 분포에 대한 통찰을 얻거나 결정문제를 풀 수 있다.</li>
  <li>Estimation. 알려지지 않은 진짜 분포 P로부터 iid sample ${x_1, x_2, …, x_n}$이 주어졌을 때, 이 진짜 분포를 가장 잘 설명하는 Q $\in Q$를 찾는다.</li>
  <li>Point-wise 우도 측정. sample $x$가 주어지면, 우도 Q($x$)를 계산한다.</li>
</ul>

<p>GAN은 정확한 sampling과 근사추정이 가능한 인상적인 모델이다. 여기서 사용된 모델은 균등분포 같은 random input vector를 받는 feedforward 신경망이다. 최종적으로 모델을 통과하여 나오는 것은 예를 들면 이미지이다. GAN에서 sampling하는 것은 딱 1개의 input이 신경망을 통과하면 정확히 하나의 sample이 나온다는 점에서 효율적이다.</p>

<p>이런 확률적 feedforward 신경망을 <strong>generative neural samplers</strong>라고 부를 것이다. GAN도 여기에 포함되며, 또한 variational autoencoder의 decoder 모델이기도 하다.</p>

<p>original GAN에서, neural sample를 <a href="https://greeksharifa.github.io/generative%20model/2019/03/03/GAN/#%EB%AA%A9%EC%A0%81%ED%95%A8%EC%88%98-%EC%B5%9C%EC%A0%81%ED%99%94%EC%9D%98-%EC%9D%98%EB%AF%B8">JSD</a>의 근사적 최소화로 추정하는 것이 가능함이 증명되어 있다.</p>

<script type="math/tex; mode=display">D_{JS}(P \| Q) = {1 \over 2} D_{KL}(P \| {1 \over 2}(P+Q)) + {1 \over 2} D_{KL}(Q \| {1 \over 2}(P+Q))</script>

<p>$D_{KL}$은 <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence</a>이다.</p>

<p>GAN 학습의 중요한 테크닉은 동시에 최적화된 <strong>Discriminator</strong> 신경망을 만든 것에 있다. 왜냐하면 $D_{JS}$는 진짜 분포 $P$는 충분한 학습을 통해 Q가 $P$에 충분히 가까워졌을 때 분포 간 divergence를 측정하는 적정한 방법이기 때문이다.</p>

<p>우리는 이 논문에서 GAN 학습목적(training objectives)과 이를 임의의 <em>f-divergence</em>로 일반화하고자, GAN을 variational divergence 추정 framework로 확장할 것이다.</p>

<p>구체적으로, 이 논문에서 보여주는 state-of-the-art한 것은:</p>

<ul>
  <li>GAN 학습목적을 모든 <em>f</em>-divergence에 대해 유도하고 여러 divergence 함수를 소개할 것이다: Kullback-Leibler와 Pearson Divergence를 포함한다.</li>
  <li>우리는 GAN의 saddle-point 최적화를 단순화할 것이고 또 이론적으로 증명할 것이다.</li>
  <li>자연 이미지에 대한 generative neural sampler을 측정하는 데 어느 divergence 함수가 적당한지 실험적 결과를 제시하겠다.</li>
</ul>

<hr />

<h2 id="방법method">방법(Method)</h2>

<p>먼저 divergence 추정 framework를 리뷰부터 하겠다. 이후 divergence 추정에서 model 추정으로 확장하겠다.</p>

<h3 id="the-f-divergence-family">The <em>f</em>-divergence Family</h3>

<p>Kullback-Leibler divergence같이 잘 알려진 것은 두 확률분포 간 차이를 측정한다.</p>

<p>두 분포 $P$와 $Q$가 있고, domain $\chi$에서 연속밀도함수 $p$와 $q$에 대해 <em>f-divergence</em>는<br />
$ f : \mathbb{R}_+ \rightarrow \mathbb{R} $이 $f(1)=0$인 볼록이고 하반연속인(convex, lower-semicontinuous) 함수 $f$에 대해</p>

<script type="math/tex; mode=display">D_f(P \Vert Q) = \int_{\chi} q(x) f \Bigl( {p(x) \over q(x)} \Bigr) dx</script>

<p>로 정의된다.</p>

<h3 id="variational-estimation-of-f-divergences">Variational Estimation of <em>f</em>-divergences</h3>

<p><em>Nyugen</em> 등 연구자는 $P$와 $Q$로부터의 sample만 주어진 경우에서 <em>f</em>-divergence를 측정하는 일반적인 변분법을 유도했다. 우리는 이를 고정된 모델에서 그 parameter를 측정하는 것으로까지 확장할 것이고, 이를 <em>variational divergence minimization</em>(VDM)이라 부를 것이다. 또한 적대적 생성 학습법은 이 VDM의 특수한 경우임을 보인다.</p>

<p>모든 볼록이고 <a href="https://ko.wikipedia.org/wiki/%EB%B0%98%EC%97%B0%EC%86%8D%EC%84%B1">하반연속</a>인 볼록 켤레함수 $f^\ast$ (<em>Fenchel conjugate</em>)를 갖는다. 이는</p>

<script type="math/tex; mode=display">f^\ast(t) = \quad sup \quad  \{ ut-f(u) \} \\ u \in dom_f \qquad</script>

<p>로 정의된다.</p>

<p>또한 $f^\ast$ 역시 볼록이며 하반연속이고 $f^{\ast\ast} = f$이므로 $ f(u) = sup_{t \in dom_{f^\ast}} { tu-f^\ast(t) } $로 쓸 수 있다.</p>

<p><em>Nguyen</em> 등 연구자는 lower bound를 구했다: $\tau$가 $T: \chi \rightarrow \mathbb{R} $인 함수들의 임의의 집합일 때,</p>

<script type="math/tex; mode=display">D_f(P \Vert Q) \ge sup_{T \in \tau} (\mathbb{E}_{x \sim P} [T(x)] - \mathbb{E}_{x \sim Q} [f^\ast(T(x))])</script>

<p>변분법을 취해서,</p>

<script type="math/tex; mode=display">T^\ast(x) = f^{'} \Bigl( {p(x) \over q(x)} \Bigr)</script>

<p>아래는 여러 <em>f</em>-divergence를 생성함수와 함께 나타낸 것이다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/01.png" width="100%" /></center>

<h3 id="variational-divergence-minimizationvdm">Variational Divergence Minimization(VDM)</h3>

<p>이제 실제 분포 $P$가 주어졌을 때 생성모델 $Q$를 측정하기 위해 <em>f</em>-divergence $D_f(P\Vert Q)$에 하한을 적용할 수 있다.</p>

<p>벡터 $\theta$를 받는 모델 $Q$를 $Q_{\theta}$, $\omega$를 쓰는 $T$를 $T_{\omega}$로 썼을 때, 우리는 다음 <em>f</em>-GAN 목적함수의 saddle-point를 찾는 것으로 $Q_{\theta}$를 학습시킬 수 있다.</p>

<script type="math/tex; mode=display">F(\theta, \omega) = \mathbb{E}_{x \sim P} [T_{\omega}(x)] - \mathbb{E}_{x \sim Q_{\theta}} [f^\ast({T_\omega}(x))]</script>

<p>주어진 유한한 학습 데이터셋에 대해 위 식을 최적화하려면, minibatch를 통해 기댓값을 근사해야 한다. $\mathbb{E}<em>{x \sim P}[\cdot]$를 근사하기 위해 학습 셋으로부터 비복원추출하여 $B$개를 뽑고, $\mathbb{E}</em>{x \sim Q_{\theta}}[\cdot]$를 근사하기 위해 현재 생성모델 $Q_{\theta}$로부터 $B$개를 뽑는다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/02.png" width="100%" /></center>

<h3 id="representation-for-the-variational-function">Representation for the Variational Function</h3>

<p>위의 식을 다른 <em>f</em>-divergence에도 사용하려면 켤레함수 $f^\ast$의 도메인  $dom_{f^\ast}$를 생각해야 한다. $T_\omega (x) = g_f(V_\omega(x)) $로 바꿔 쓸 수 있다.</p>

<p>이제 GAN 목적함수를 보면, divergence가 sigmoid이므로</p>

<script type="math/tex; mode=display">F(\theta, \omega) = \mathbb{E}_{x \sim P} [log D_{\omega}(x)] - \mathbb{E}_{x \sim Q_{\theta}} [log(1-D_\omega(x))]</script>

<p>출력 활성함수는 Table 6을 보라(부록).</p>

<h3 id="example-univariate-mixture-of-gaussians">Example: Univariate Mixture of Gaussians</h3>

<p>가우시안 sample에 대해 근사한 결과를 적어 놓았다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/03.png" width="100%" /></center>

<hr />

<h2 id="vdm-알고리즘algorithms-for-variational-divergence-minimizationvdm">VDM 알고리즘(Algorithms for Variational Divergence Minimization(VDM))</h2>

<p>이제 우리는 목적함수의 saddle point를 찾기 위한 수치적 방법을 논할 것이다.</p>

<ol>
  <li>Goodfellow가 제안한 교대(alternative) 학습 방법</li>
  <li>더 직접적인 single-step 최적화 과정</li>
</ol>

<p>두 가지를 쓴다.</p>

<h3 id="single-step-gradient-method">Single-Step Gradient Method</h3>

<p>원래 것과는 달리 inner loop가 없고, 단 하나의 back-propagation으로 $\omega$와 $\theta$의 gradient가 계산된다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/04.png" width="100%" /></center>

<p>saddle point 근방에서 $\theta$에 대해 볼록하고 $\omega$ 에 대해  오목한 $F$에 대해 위 알고리즘 1은 saddle point $(\theta^\ast, \omega^\ast)$에서 수렴함을 보일 수 있다.</p>

<p>이를 위해 다음 정리를 논문 부록에서 보이고 있다.</p>

<p><strong>Theorem 1.</strong> $\pi^t := (\theta^t, \omega^t) $ 라 하고, 조금 위의 근방 조건을 만족하는 saddle point $ \pi^\ast = (\theta^\ast, \omega^\ast) $ 가 존재한다고 가정하자. 더욱이 위 근방에 포함되는 $ J(\pi) = {1\over 2} \Vert \nabla F(\pi) \Vert_2^2 $ 를 정의할 수 있고, $F$는 $ \pi^\ast $ 근방 모든 $ \pi, \pi^{‘} $ 에 대해 $ \Vert \nabla J(\pi^{‘}) - \nabla J(\pi) \Vert_2 \le L \Vert \pi^{‘} - \pi \Vert_2 $ 를 만족하는 상수 $ L &gt; 0 $ 가 존재할 수 있게 하는 $F$는 충분히 smooth하다.<br />
알고리즘 1에서 step-size를 $ \eta=\delta / L$ 라 할 때,</p>

<script type="math/tex; mode=display">J(\pi^t) \le \Bigl( 1 - {\lambda^2 \over 2L} \Bigr)^t J(\pi^0)</script>

<p>를 얻을 수 있다.<br />
또 gradient $ \nabla F(x) $ 의 2차 norm은 기하적으로 감소한다.</p>

<h3 id="practical-considerations">Practical Considerations</h3>

<p>Goodfellow가 GAN 논문 당시 제안한 팁 중에<br />
<script type="math/tex">\mathbb{E}_{x \sim Q_{\theta}} [log(1-D_\omega(x))]</script> 
를 최소화하는 대신 
<script type="math/tex">\mathbb{E}_{x \sim Q_{\theta}} [log D_\omega(x)]</script>
 를 최대화하는 것으로 속도를 빠르게 하는 것이 있었다.<br />
이를 더 일반적인 <em>f</em>-GAN에 적용하면</p>

<script type="math/tex; mode=display">\theta^{t+1} = \theta^t + \eta \nabla_\theta \mathbb{E}_{x \sim Q_{\theta^t}} [g_f(V_{\omega^t}(x))]</script>

<p>그렇게 함으로써 generator 출력을 최대화할 수 있다.</p>

<p>실험적으로, 우리는 Adam과 gradient clipping이 LSUN 데이터셋의 대규모 실험에서는 특히 유용함을 발견하였다.</p>

<hr />

<h2 id="실험experiments">실험(Experiments)</h2>

<p>이제 VDM에 기초하여 MNIST와 LSUN에 대해 학습시킨 결과는 다음과 같다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/05.png" width="100%" /></center>

<center><img src="/public/img/2019-03-19-f-GAN/06.png" width="100%" /></center>

<p>결과 요약을 하면… 약간 예상 외로 divergence 함수가 달라져도 결과의 품질은 큰 차이가 없었다고 한다.</p>

<hr />

<h2 id="관련-연구related-work">관련 연구(Related Work)</h2>

<p>오직 신경망에 적용할 수 있는 것에 대해서만 논하겠다.</p>

<ul>
  <li>Mixture density networks: 유한한 mixture 모델의 parameter를 직접 회귀시키는 데 쓸 수 있다.</li>
  <li>NADE and RNADE: 사전에 정의되었고 어느 정도 임의의 출력 차원을 가진 출력의 factorization을 수행한다.</li>
  <li>Diffusion probabilistic models: 자명하고 알려진 분포에서 출발하는 학습된 발산과정의 결과로 목표 분포를 정의한다.</li>
  <li>Noise contrasive estimation(NCE): 임의로 생성된 noise로부터 데이터를 식별하는 비선형 logistic 회귀를 수행하여 비정규화된 확률모델의 parameter를 추정하는 방법이다.</li>
  <li>Variational auto-encoders(VAE): 변분법적 베이지안 학습 목표함수를 갖고 sample을 잠재표현식으로 매핑하는 확률적 encoder와 decoder 모델의 쌍이다.</li>
</ul>

<hr />

<h2 id="토의discussion">토의(Discussion)</h2>

<p>Generative neural samplers는 factorizing 가정 없이도 복잡한 분포를 표현하는 강력한 방법을 제공한다. 그러나 이 논문에서 사용된 순수 generative neural samplers는 관측된 데이터에 대한 조건부로 적용할 수 없고 따라서 그로부터 추론할 것이 없다는 한계를 갖고 있다.</p>

<p>우리는 미래에는 표현의 불확실성을 위한 neural samplers의 진면목이 식별 모델에서 발견될 것이며 생성자와 조건부 GAN 모델에 추가적인 input을 넣음으로써 쉽게 이 경우에 대해 확장할 수 있을 것이라 믿는다.</p>

<hr />

<h2 id="참고문헌references">참고문헌(References)</h2>

<p>논문 참조!</p>

<hr />

<h2 id="부록">부록</h2>

<ul>
  <li>Section A: 이 부분이다.</li>
  <li>Section B: <em>f</em>-divergence의 확장된 리스트(생성함수와 볼록 켤레함수)를 나열하였다.</li>
  <li>Section C: Theorem 1를 증명한다. (논문에는 Theorem 2라 되어 있는데 같은 것이다)</li>
  <li>Section D: 현재 GAN 최적화 알고리즘과 차이를 논한다.</li>
  <li>Section E: 다양한 divergence 측정방법을 써서 Gaussian을 혼합 Gaussian 분포에 맞춤으로써 우리의 접근법을 증명한다.</li>
  <li>Section F: 본문에서 사용한 모델의 세부 구조를 보여준다.</li>
</ul>

<p>증명의 자세한 부분은 논문을 보는 것이 빠르므로 생략하겠다.</p>

<center><img src="/public/img/2019-03-19-f-GAN/07.png" width="100%" /></center>

<center><img src="/public/img/2019-03-19-f-GAN/08.png" width="100%" /></center>

<center><img src="/public/img/2019-03-19-f-GAN/09.png" width="100%" /></center>

<p>MNIST 생성자:<br />
$z \rightarrow Linear(100, 1200) \rightarrow BN \rightarrow ReLU \rightarrow Linear(1200, 1200) \rightarrow BN \rightarrow ReLU \rightarrow Linear(1200, 784) \rightarrow Sigmoid $</p>

<p>모든 weights는 0.05 scale로 초기화되었다.</p>

<p>MNIST Variational Function:<br />
$ x \rightarrow Linear(784, 240) \rightarrow ELU \rightarrow Linear(240, 240) \rightarrow ELU \rightarrow Linear(240, 1) $</p>

<p>ELU는 exponential linear unit이다. 모든 weights는 0.005 scale로 초기화되었다.</p>

<p>LSUN Natural Images:<br />
$ z \rightarrow Linear(100, 6\ast6\ast512)  \rightarrow BN \rightarrow ReLU \rightarrow Reshape(512, 6, 6) \rightarrow Deconv(512, 256) \rightarrow BN \rightarrow ReLU \rightarrow Deconv(256, 128) \rightarrow BN \rightarrow ReLU \rightarrow Deconv(128, 64) \rightarrow BN \rightarrow ReLU \rightarrow Deconv(64, 3) $</p>

<p>deconv는 kernel size 4, stride 2를 사용하였다.</p>

<p>Deconv는 Deconvolution을 의미하는데, <a href="https://greeksharifa.github.io/generative%20model/2019/03/17/DCGAN/">DCGAN 글</a>에서도 설명하였듯 잘못된 표현이다.</p>

<hr />

<h1 id="이후-연구들">이후 연구들</h1>

<p>GAN 이후로 수많은 발전된 GAN이 연구되어 발표되었다.</p>

<p>많은 GAN들(catGAN, Semi-supervised GAN, LSGAN, WGAN, WGAN_GP, DRAGAN, EBGAN, BEGAN, ACGAN, infoGAN 등)에 대한 설명은 <a href="https://greeksharifa.github.io/generative%20model/2019/03/20/advanced-GANs/">다음 글</a>에서 진행하도록 하겠다.</p>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/f-GAN/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/f-GAN/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/CGAN/">
        CGAN(Conditional GAN)
      </a>
    </h1>

    <span class="post-date">19 Mar 2019</span>
     |
    
    <a href="/blog/tags/#gan" class="post-tag">GAN</a>
    
    <a href="/blog/tags/#machine-learning" class="post-tag">Machine Learning</a>
    
    <a href="/blog/tags/#cnn" class="post-tag">CNN</a>
    
    <a href="/blog/tags/#generative-model" class="post-tag">Generative Model</a>
    
    <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
    
    

    <article>
      <hr />

<p>이 글에서는 2014년 11월 <em>Mehdi Mirza</em> 등이 발표한 Conditional Generative Adversarial Nets(CGAN)를 살펴보도록 한다.</p>

<p>CGAN은 GAN의 변형 모델이다.</p>

<p><code class="highlighter-rouge">(즉 DCGAN보다는 먼저 나왔다. 하지만 DCGAN이 GAN의 역사에서 제일 중요한 것 중 하나이기 때문에 CGAN을 나중으로 미뤘다.)</code></p>

<p>CGAN은 GAN과 학습 방법 자체는 별로 다를 것이 없다(D 학습 후 G 학습시키는 것).<br />
GAN의 변형 모델들은 대부분 그 모델 구조를 바꾼 것이다.</p>

<p>CGAN을 도식화한 구조는 다음과 같다. <a href="https://github.com/hwalsuklee/tensorflow-generative-model-collections">출처</a></p>

<center><img src="/public/img/2019-03-19-CGAN/04.png" width="50%" /></center>

<p>중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.</p>

<hr />

<h1 id="conditional-gancgan">Conditional GAN(CGAN)</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1411.1784">Conditional GAN</a></strong></p>

<h2 id="초록abstract">초록(Abstract)</h2>

<p>2014년에 나온 논문임을 생각하라.</p>

<p>최근 GAN이 생성모델을 학습시키는 근사한 방법으로 소개되었다. 우리는 이 GAN의 조건부(conditional) 버전, 간단히 $y$ 데이터를 추가하여 만든 적대적 망을 소개하려 한다. 이 CGAN이 class label(숫자 0~9)에 맞는 MNIST 이미지를 생성할 수 있음을 보일 것이다. 또한 이 모델이 multi-modal 모델에 어떻게 사용될지, 또 이미지 태깅에 어떻게 응용 가능할지도 또한 설명할 것이다.</p>

<hr />

<h2 id="서론introduction">서론(Introduction)</h2>

<p>생성 모델을 학습하기 위해, 다루기 힘든 엄청난 확률적 계산의 어려움을 대체하는 GAN이 최근 소개되었다. 적대신경망은 Markov chain이 필요없이 오직 back-propagation만으로 학습이 가능하고, 별다른 추측도 할 필요가 없다.</p>

<p>Unconditional 생성모델에서, 데이터가 생성되는 종류(mode)를 제어할 방법은 없다. 그러나, 추가 정보를 통해 데이터 생성 과정을 제어할 수 있다. 이러한 조건 설정(conditioning)은 class label 등에 기반할 수 있다.</p>

<p>이 논문에서 우리는 conditional 적대신경망을 구현할 것이다. 또 이를 MNIST와 MIR Flickr 데이터셋에 대해 테스트한다.</p>

<hr />

<h2 id="관련-연구related-works">관련 연구(Related Works)</h2>

<p>궁금하면 읽어보자.</p>

<h3 id="multi-modal-learning-for-image-labelling">Multi-modal Learning for Image Labelling</h3>

<p>굉장히 많은 카테고리를 다룰 수 있는 모델에 관한 문제는 추가 modality에 대한 정보를 다루는 것으로 일부 해결 가능하다. 단어를 vector representation으로 변형하는 것 등이 있다.</p>

<p>input-output 1-1 매칭에만 치중한 문제는 conditional 확률적 생성모델을 사용하는 것이 한 방법이 될 수 있다.</p>

<p>자세한 내용은 원문을 보고 각 논문을 찾아보라. 이미 요약된 부분이라 그냥 건너뛰거나 본문을 보는 것이 더 낫다.</p>

<hr />

<h2 id="조건부-적대신경망conditional-adversarial-nets">조건부 적대신경망(Conditional Adversarial Nets)</h2>

<h3 id="gangenearative-adversarial-nets">GAN(Genearative Adversarial Nets)</h3>

<p>최근 소개된 GAN은 다음 두 부분으로 이루어졌다. 둘 다 non-linear하게 매핑하는 함수일 수 있다.</p>
<ul>
  <li>데이터분포를 입력받아 실제에 가깝게 데이터를 생성하는 생성모델 G</li>
  <li>입력받은 데이터가 진짜 데이터인지 G가 만들어낸 것인지를 판별하는 D</li>
</ul>

<p>다음 식으로 표현되는 minimax 게임을 G와 D가 진행하게 된다:</p>

<script type="math/tex; mode=display">min_G max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[log D(x)] + \mathbb{E}_{x \sim p_{z}(z)}[log (1-D(G(z)))]</script>

<p>수식에 대한 자세한 설명은 <a href="https://greeksharifa.github.io/generative%20model/2019/03/03/GAN/#%EC%A0%81%EB%8C%80%EC%A0%81-%EB%A7%9Dadversarial-nets">GAN</a>을 참고하라.</p>

<h3 id="cganconditional-adversarial-nets">CGAN(Conditional Adversarial Nets)</h3>

<p>G와 D가 추가 정보 $y$라는 조건이 붙는다면 조건부 생성모델을 만들 수 있다. $y$는 어떤 보조 정보라도 될 수 있는데, class label이나 다른 modality의 데이터 등이다. 우리는 $y$를 G와 D의 input layer에 추가로 같이 집어넣음으로써 이를 수행할 수 있다.</p>

<p>G에서는 input noise $p_z(z)$와 $y$가 합쳐진 형태가 된다. 이 적대적 학습 framework는 이 hidden representation이 어떻게 생겼는지에 별 영향을 받지 않는다.<br />
D에서는 $x$와 $y$가 input으로써 들어가게 된다.</p>

<p>좀 전 수식을 conditional 버전으로 바꿔보면,</p>

<script type="math/tex; mode=display">min_G max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[log D(x|y)] + \mathbb{E}_{x \sim p_{z}(z)}[log (1-D(G(z|y)))]</script>

<p><em>참고</em>: D와 G에 들어가는 input이 단지 조건부로 바뀌었다. 실제 들어가는 형태는 합쳐진 형태이다.</p>

<center><img src="/public/img/2019-03-19-CGAN/01.png" width="80%" /></center>

<hr />

<h2 id="실험-결과experimental-results">실험 결과(Experimental Results)</h2>

<p>이미 좋다는 게 알려진 논문의 경우에는 굳이 실험 조건 등을 자세히 볼 필요는 없다. 여기서는 결과만 소개한다.</p>

<h3 id="unimodal">Unimodal</h3>

<p>모델 구조는 다음과 갈다.</p>
<ul>
  <li>G
    <ul>
      <li>uniform distribution $z$. size=100</li>
      <li>$z$와 $y$는 각각 size 200, 1000짜리 hidden layer(ReLU)로 매핑됨</li>
      <li>1200짜리 hidden layer로 합쳐짐(ReLU)</li>
      <li>마지막으로 784차원으로 변환됨(MNIST 이미지는 $28^2$이다)</li>
    </ul>
  </li>
  <li>D
    <ul>
      <li>$x$는 240 unit과 5 piece짜리 maxout layer, $y$는 50 unit과 5 piece짜리 maxout layer로 매핑됨</li>
      <li>240 unit, 5 piece짜리 maxout layer로 합쳐진 후 Sigmoid</li>
    </ul>
  </li>
</ul>

<p>MNIST로 실험한 결과이다. Log-likelihood 값이 잘 나왔음을 확인할 수 있다.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>MNIST</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DBN</td>
      <td>138 $\pm $ 2</td>
    </tr>
    <tr>
      <td>Stacked CAE</td>
      <td>121 $\pm $ 1.6</td>
    </tr>
    <tr>
      <td>Deep GSN</td>
      <td>214 $\pm $ 1.1</td>
    </tr>
    <tr>
      <td>Adversarial nets</td>
      <td>225 $\pm $ 2</td>
    </tr>
    <tr>
      <td>Conditional adversarial nets</td>
      <td>132 $\pm $ 1.8</td>
    </tr>
  </tbody>
</table>

<p>$y$ 데이터는 각 row별로 0~9까지 들어갔다. 아래는 CGAN을 통해 생성된 이미지이다.</p>

<center><img src="/public/img/2019-03-19-CGAN/02.png" width="100%" /></center>

<p>주어지는 조건($y$)에 따라 class가 잘 나뉘는 것은 확인할 수 있다(이미지 품질은 original GAN과 비슷하다).</p>

<h3 id="multimodal">Multimodal</h3>

<p>여러 이미지들에 대해 사람이 직접 넣은 태그와 CGAN이 생성해낸 태그를 비교한 테이블을 가져왔다.</p>

<center><img src="/public/img/2019-03-19-CGAN/03.png" width="100%" /></center>

<p>가장 오른쪽 열이 생성된 태그 중 제일 나은 것 10개를 나열한 것인데, 꽤 잘 된 것으로 보인다.</p>

<hr />

<h2 id="추후-연구future-work">추후 연구(Future work)</h2>

<p>이 논문에서 소개된 결과는 서론 정도의 내용이지만, 각각은 조건부 생성모델의 잠재력과 다른 많은 분야로의 응용에 대한 가능성을 보여 준다.</p>

<p>이번 실험에서는 태그를 독립적으로 사용했지만, 한번에 여러 태그를 사용한다면 더 나은 결과를 얻을 수 있을 것이다.</p>

<p>추후 연구의 또 다른 방향은 언어 모델을 배우는 학습계획을 구현하는 것이 있겠다.</p>

<h3 id="acknowledgments">Acknowledgments</h3>

<p>이 프로젝트는 Pylearn2 framework로 개발되었다.</p>

<h2 id="참고문헌references">참고문헌(References)</h2>

<p>논문 참조!</p>

<hr />

<h1 id="튜토리얼">튜토리얼</h1>

<p>GAN의 핵심 부분을 제외한 부분은 <a href="https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/">여기</a>를 참고하면 된다.</p>

<p><a href="https://github.com/znxlwm/pytorch-generative-model-collections/blob/master/CGAN.py">여기</a>에서 CGAN을 학습시켜볼 수 있다. 해당 repository에는 CGAN뿐 아니라 많은 종류의 GAN이 Pytorch로 구현되어 있으므로 참고하면 좋다.</p>

<hr />

<h1 id="이후-연구들">이후 연구들</h1>

<p>GAN 이후로 수많은 발전된 GAN이 연구되어 발표되었다.</p>

<p>많은 GAN들(catGAN, Semi-supervised GAN, LSGAN, WGAN, WGAN_GP, DRAGAN, EBGAN, BEGAN, ACGAN, infoGAN 등)에 대한 설명은 <a href="https://greeksharifa.github.io/generative%20model/2019/03/20/advanced-GANs/">여기</a>, f-GAN에 대한 설명은 <a href="https://greeksharifa.github.io/generative%20model/2019/03/19/f-GAN/">여기</a>에서 진행하도록 하겠다.</p>

<hr />


    </article>
    <div class="post-more">
      
      <a href="/CGAN/#disqus_thread"> <i class="fa fa-comments" aria-hidden="true"></i>Comment</a>&nbsp;
      
      <a href="/CGAN/"><i class="fa fa-plus-circle" aria-hidden="true"></i>Read more</a>
    </div>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/blog/page8">Older</a>
  
  
    
      <a class="pagination-item newer" href="/blog/page6">Newer</a>
    
  
</div>


  </div>
</div>

<label for="sidebar-checkbox" class="sidebar-toggle"></label>

<script>
  (function (document) {
    let toggle = document.querySelector('.sidebar-toggle');
    let sidebar = document.querySelector('#sidebar');
    let checkbox = document.querySelector('#sidebar-checkbox');

    document.addEventListener('click', function (e) {
      let target = e.target;

      if (target === toggle) {
        checkbox.checked = !checkbox.checked;
        e.preventDefault();
      } else if (checkbox.checked && !sidebar.contains(target)) {
        /* click outside the sidebar when sidebar is open */
        checkbox.checked = false;
      }
    }, false);
  })(document);
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    };
    i[r].l = 1 * new Date();
    a = s.createElement(o);
    m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-00000000-1', 'auto');
  ga('send', 'pageview');
</script>


<!-- Naver Analytics -->	
<script type="text/javascript" src="//wcs.naver.net/wcslog.js"></script>
<script type="text/javascript">
  if(!wcs_add) var wcs_add = {};
    wcs_add["wa"] = "18cbce78e94161";
  wcs_do();
</script>

</body>

<script id="dsq-count-scr" src="//greeksharifa-github-io.disqus.com/count.js" async></script>

</html>
