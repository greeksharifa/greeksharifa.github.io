<!DOCTYPE html>
<html lang="en-us">
<head>
  <head>
  <!-- Description of Blog -->
  <meta name="description" content="Python, Machine & Deep Learning">
  <link rel="canonical" href="https://greeksharifa.github.io/">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Python, Machine & Deep Learning">
  <meta property="og:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta property="og:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta property="og:url" content="https://greeksharifa.github.io/">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Python, Machine & Deep Learning">
  <meta name="twitter:description" content="Python, Machine Learning & Deep Learning 설명서">
  <meta name="twitter:image" content="https://greeksharifa.github.io/public/img/icon-144x144.png">
  <meta name="twitter:domain" content="https://greeksharifa.github.io/">

  <!-- link -->
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      DANs(Dual Attention Networks for Multimodal Reasoning and Matching)
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="icon-144x144" sizes="144x144" href="/public/img/icon-144x144.png">
  <link rel="shortcut icon" href="/public/img/icon_32x32.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  

  <!-- Ads -->
  <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
  </script>
</head>

  <!-- for Google AdSense-->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-9951774327887666",
    enable_page_level_ads: true
  });
</script>

  <style>blockquote {
    font-size: 1em;
    line-height: 1.4
  }</style>
  <link href='http://fonts.googleapis.com/css?family=Gill+Sans' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Consolas' rel='stylesheet' type='text/css'>
</head>
<body>

<!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/3c2986ad7ac1f2230ea3596f44563328">
          <img src="/public/img/maple_tree.jpg" title="Cover Photo" alt="Maple tree" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p><strong>Developer and Analyst</strong>, YW & YY.</p>
      </div>
      
      
      
      <div class="sidebar-personal-info-section">
        <p> Follow me:
        
        
        
        <a href="https://github.com/greeksharifa">
          <i class="fa fa-github" aria-hidden="true"></i>
        </a>
        
        |
        
        
        
        <a href="mailto:greeksharifa@gmail.com">
          <i class="fa fa-envelope" aria-hidden="true"></i>
        </a>
        
        
        
        </p>
      </div>
      
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/blog/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="/about/">
          About
        </a>

        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://greeksharifa.github.io/">
          Github Project
        </a>

        
      </span>

    

  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2020 YW & YY. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> and <a href="http://greeksharifa.github.io">YW & YY</a>
    </p>
  </div>
</div>


<!-- Wrap is the content to shift when toggling the sidebar. We wrap the
     content to avoid any CSS collisions with our real content. -->
<div class="wrap">
  <div class="masthead">
    <div class="container">
      <h3 class="masthead-title" align="center">
        <a href="/" title="Home" title="YW & YY">
          <img class="masthead-logo" src="/public/img/logo.png"/>
        </a>
        <small>YW & YY's Python, Machine & Deep Learning</small>
        <!-- HTML elements for search -->
        <a href="/search/" id="search_icon">
          <img src="/public/img/search.png" width="25" height="25"
               align="right" style="margin-top:5px; margin-bottom:0;"
               onmouseover="this.style.opacity=0.7" onmouseout="this.style.opacity=0.5"
               alt="search">
        </a>
      </h3>
    </div>
  </div>

  <div class="container content">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.9/styles/github.min.css"> 
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.9/highlight.min.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 수직형 디스플레이 광고1 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-9951774327887666"
     data-ad-slot="7237421728"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<div class="post">
  <h1 class="post-title">DANs(Dual Attention Networks for Multimodal Reasoning and Matching)</h1>
  <span class="post-date">17 Apr 2019</span>
   |
  
  <a href="/blog/tags/#attention-mechanism" class="post-tag">Attention Mechanism</a>
  
  <a href="/blog/tags/#paper-review" class="post-tag">Paper_Review</a>
  
  <a href="/blog/tags/#vqa" class="post-tag">VQA</a>
  
  
  <article>
    <p><strong>목차</strong></p>
    <ul>
  <li><a href="#dansdual-attention-networks-for-multimodal-reasoning-and-matching">DANs(Dual Attention Networks for Multimodal Reasoning and Matching)</a>
    <ul>
      <li><a href="#초록abstract">초록(Abstract)</a></li>
      <li><a href="#서론introduction">서론(Introduction)</a></li>
      <li><a href="#관련-연구related-works">관련 연구(Related Works)</a></li>
      <li><a href="#dual-attention-networksdans">Dual Attention Networks(DANs)</a>
        <ul>
          <li><a href="#input-representation">Input Representation</a>
            <ul>
              <li><a href="#image-representation">Image representation</a></li>
              <li><a href="#text-representation">Text representation</a></li>
            </ul>
          </li>
          <li><a href="#attention-mechanisms">Attention Mechanisms</a>
            <ul>
              <li><a href="#visual-attention">Visual Attention</a></li>
              <li><a href="#textual-attention">Textual Attention</a></li>
            </ul>
          </li>
          <li><a href="#r-dan-for-visual-question-answering">r-DAN for Visual Question Answering</a></li>
          <li><a href="#m-dan-for-image-text-matching">m-DAN for Image-Text Matching</a></li>
        </ul>
      </li>
      <li><a href="#실험experiments">실험(Experiments)</a>
        <ul>
          <li><a href="#experimental-setup">Experimental Setup</a></li>
          <li><a href="#evaluation-on-visual-question-answering">Evaluation on Visual Question Answering</a>
            <ul>
              <li><a href="#dataset-and-evaluation-metric">Dataset and Evaluation Metric</a></li>
              <li><a href="#results-and-analysis">Results and Analysis</a></li>
            </ul>
          </li>
          <li><a href="#evaluation-on-image-text-matching">Evaluation on Image-Text Matching</a></li>
        </ul>
      </li>
      <li><a href="#결론conclusion">결론(Conclusion)</a></li>
      <li><a href="#참고문헌references">참고문헌(References)</a></li>
    </ul>
  </li>
</ul>

    <hr />

<p>이 글에서는 네이버랩스(Naver Corp.)에서 2017년 발표한 논문인 Dual Attention Networks for Multimodal Reasoning and Matching에 대해 알아보고자 한다.<br />
네이버랩스는 인공지능 국제대회 ‘CVPR 2016: VQA Challenge’에서 2위를 차지하였고, 해당 챌린지에서 DAN(Dual Attention Networks)라는 알고리즘을 개발하였다. 이어 이 알고리즘을 조금 더 일반화하여 2017년 발표한 논문이 이 논문이다.</p>

<p>VQA가 무엇인지는 <a href="https://greeksharifa.github.io/computer%20vision/2019/04/17/Visual-Question-Answering/">여기</a>를 참조하면 된다.</p>

<p>간단히, DANs은 따로 존재하던 Visual 모델과 Textual 모델을 잘 합쳐 하나의 framework로 만든 모델이라고 할 수 있겠다.</p>

<p>중요한 부분만 적을 예정이므로 전체가 궁금하면 원 논문을 찾아 읽어보면 된다.</p>

<hr />

<h1 id="dansdual-attention-networks-for-multimodal-reasoning-and-matching">DANs(Dual Attention Networks for Multimodal Reasoning and Matching)</h1>

<p>논문 링크: <strong><a href="https://arxiv.org/abs/1611.00471">DANs(Dual Attention Networks for Multimodal Reasoning and Matching)</a></strong></p>

<h2 id="초록abstract">초록(Abstract)</h2>

<p>vision과 language 사이의 세밀한 상호작용을 포착하기 위해 우리는 visual 및 textual attention을 잘 조정한 Dual Attention Networks(DANs)를 제안하고자 한다. DANs는 이미지와 텍스트 모두로부터 각각의 중요한 부분에 여러 단계에 걸쳐 집중(attend / attention)하고 중요한 정보를 모아 이미지/텍스트의 특정 부분에만 집중하고자 한다. 이 framework에 기반해서, 우리는 multimodal reasoning(추론)과 matching(매칭)을 위한 두 종류의 DANs를 소개한다. 각각의 모델은 VQA(Visual Question Answering), 이미지-텍스트 매칭에 특화된 것이고 state-of-the-art 성능을 얻을 수 있었다.</p>

<hr />

<h2 id="서론introduction">서론(Introduction)</h2>

<p>Vision과 language는 실제 세계를 이해하기 위한 인간 지능의 중요한 두 부분이다. 이는 AI에도 마찬가지이며, 최근 딥러닝의 발전으로 인해 이 두 분야의 경계조차 허물어지고 있다. VQA, Image Captioning, image-text matching, visual grounding 등등.</p>

<p>최근 기술 발전 중 하나는 attention mechanism인데, 이는 이미지 등 전체 데이터 중에서 중요한 부분에만 ‘집중’한다는 것을 구현한 것으로 많은 신경망의 성능을 향상시키는 데 기여했다. <br />
시각 데이터와 텍스트 데이터 각각에서는 attention이 많은 발전을 가져다 주었지만, 이 두 모델을 결합시키는 것은 연구가 별로 진행되지 못했다.</p>

<p>VQA같은 경우 “(이미지 속) 저 우산의 색깔은 무엇인가?” 와 같은 질문에 대한 답은 ‘우산’과 ‘색깔’에 집중함으로써 얻을 수 있고, 이미지와 텍스트를 매칭하는 task에서는 이미지 속 ‘girl’과 ‘pool’에 집중함으로써 해답을 얻을 수 있다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/01.png" width="80%" /></center>

<p>이 논문에서 우리는 vision과 language의 fine-grained 상호작용을 위한 visual 모델과 textual 모델 두 가지를 잘 결합한 Dual Attention Networks(DANs)를 소개한다. DANs의 두 가지 변형 버전이 있는데, reasoning-DAN(r-DAN, 추론용 모델)과 matching-DAN(m-DAN, 매칭용 모델)이다.</p>

<p>r-DAN은 이전 attention 결과와 다음 attention을 모은 결합 메모리를 사용하여 시각적 그리고 언어적 attention을 협동 수행한다. 이는 VQA같은 multimodal 추론에 적합하다.<br />
m-DAN은 시각 집중 모델과 언어 집중 모델을 분리하여 각각 다른 메모리에 넣지만 이미지와 문장 사이의 의미를 찾기 위해 학습은 동시에 진행하는 모델이다. 이 접근법은 최종적으로 효율적인 cross-modal 매칭을 용이하게 해 준다.<br />
두 알고리즘 모두 시각적 그리고 언어적(문자적, textual) 집중 mechanism을 하나의 framework 안에 긴밀히 연결한 것이다.</p>

<p>이제 우리가 기여한 바는 다음과 같다:</p>

<ul>
  <li>시각적 그리고 언어적 attention을 위한 통합된 framework를 제안하였다. 이미지 내 중요한 부분과 단어들은 여러 단계에서 합쳐진 곳에 위치한다.</li>
  <li>이 framework의 변형 버전 두 가지는 실제로 추론 및 매칭을 위한 모델로 구현되어 VQA와 image-text 매칭에 적용되었다.</li>
  <li>attention 결과의 상세한 시각화는 우리의 모델이 task에 핵심적인 이미지 및 문장 부분에 잘 집중하고 있음을 보여주는 것을 가능하게 한다.</li>
  <li>이 framework는 VQA와 Flickr30K 데이터셋에서 SOTA(state-of-the-art) 결과를 보여주었다.</li>
</ul>

<hr />

<h2 id="관련-연구related-works">관련 연구(Related Works)</h2>

<ul>
  <li><strong>Attention Mechanisms:</strong> 간단히 말해 시각적 또는 언어적 입력에서 task를 해결하는 데 중요한 일부분에만 집중하도록 해 문제를 잘 풀 수 있게 하는 방법이다.</li>
  <li><strong>Visual Question Answering(VQA):</strong> 이미지와 그 이미지와 연관된 질문이 주어지면 적절한 답을 찾는 task이다. 자세한 내용은 <a href="https://greeksharifa.github.io/computer%20vision/2019/04/17/Visual-Question-Answering/">여기</a>를 참조하라.</li>
  <li><strong>Image-Text Matching:</strong> 시각자료(이미지)와 글자자료(=문장, 언어적 부분) 사이의 의미적 유사도를 찾는 것이 가장 중요하다. 많은 경우 이미지 특징벡터(feature vector)와 문장 특징벡터를 직접 비교할 수 있도록 변형해 비교하는 방법이 자주 쓰인다. 이 비교방법은 양방향 손실함수 또는 CNN으로 결합하는 방법 등이 쓰인다. 그러나 multimodal attention 모델을 개발하려는 시도는 없었다.</li>
</ul>

<hr />

<h2 id="dual-attention-networksdans">Dual Attention Networks(DANs)</h2>

<h3 id="input-representation">Input Representation</h3>

<h4 id="image-representation">Image representation</h4>

<ul>
  <li>이미지 특징은 19-layer VGGNet 또는 152-layer ResNet으로 추출했다.</li>
  <li>448 $\times$ 448 으로 바꿔 CNN에 집어넣는다.</li>
  <li>다른 ‘지역’(region)으로부터 특징벡터를 얻기 위해 VGGNet 및 ResNet의 마지막 pooling layer를 취했다.</li>
  <li>이제 이미지는 ${v_1, …, v_N}$으로 표현된다. $N$은 이미지 지역의 개수, $v_n$은 512(VGGNet) 또는 2048(ResNet)이다.</li>
</ul>

<h4 id="text-representation">Text representation</h4>

<p>one-hot 인코딩으로 주어진 $T$개의 입력 단어들 ${w_1, …, w_T}$을 임베딩시킨 후 양방향 LSTM에 집어넣는다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/02.png" width="80%" /></center>

<p>임베딩 행렬(embedding matrix)와 LSTM은 end-to-end로 학습된다.</p>

<h3 id="attention-mechanisms">Attention Mechanisms</h3>

<p>bias $b$는 생략되어 있다.</p>

<h4 id="visual-attention">Visual Attention</h4>

<p>이미지의 특정 부분에 집중하게 하는 context vector를 생성하는 것을 주목적으로 한다.</p>

<p>step $k$에서, 시각문맥벡터(visual context vector) $v^{(k)}$는</p>

<script type="math/tex; mode=display">v^{(k)} = \text{V\_Att} (\{v_n\}^N_{n=1}, \ m_v^{(k-1)}</script>

<p>$m_v^{(k-1)}$는 step $k-1$까지 집중했었던 정보를 인코딩하는 메모리 벡터이다.<br />
여기에다가 soft attention mechanism을 적용하게 된다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/03.png" width="80%" /></center>

<p>attention weights $\alpha$는 2-layer FNN과 softmax로 구해진다. $W$들은 네트워크 parameter이다.</p>

<h4 id="textual-attention">Textual Attention</h4>

<p>마찬가지로 문장의 특정 부분에 집중할 수 있도록 문맥벡터 $u^{(k)}$를 매 step마다 생성하는 것이다.</p>

<script type="math/tex; mode=display">u^{(k)} = \text{T\_Att} (\{u_t\}^T_{t=1}, \ m_u^{(k-1)}</script>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/04.png" width="80%" /></center>

<h3 id="r-dan-for-visual-question-answering">r-DAN for Visual Question Answering</h3>

<p>VQA는 multimodal 데이터를 결합 추론하는 것을 필요로 하는 문제이다. 이를 위해 r-DAN은 step $k$에서 시각 및 언어적 정보를 축적하는 메모리 벡터 $m^{(k)}$를 유지한다. 이는 재귀적으로 다음 식을 통해 업데이트된다.</p>

<script type="math/tex; mode=display">m^{(k)} = m^{(k-1)} + v^{(k)} \  (\cdot) \ u^{(k)}</script>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/05.png" width="60%" /></center>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/06.png" width="100%" /></center>

<p>최종 답은 다음과 같이 계산된다. $ \text{p}_{\text{ans}}$는 정답 후보들의 확률을 나타낸다.</p>

<script type="math/tex; mode=display">\bold{\text{p}}_{\text{ans}} = \text{softmax} \bigr( W_{\text{ans}} \ m^{(K)} \bigl)</script>

<h3 id="m-dan-for-image-text-matching">m-DAN for Image-Text Matching</h3>

<p>수식의 형태는 꽤 비슷하다.</p>

<script type="math/tex; mode=display">m_v^{(k)} = m_v^{(k-1)} + v^{(k)}</script>

<script type="math/tex; mode=display">m_u^{(k)} = m_u^{(k-1)} + u^{(k)}</script>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/07.png" width="100%" /></center>

<p><script type="math/tex">s^{(k)} = v^{(k)} \cdot u^{(k)}, \ S = \sum_{k=0}^K s^{(k)}</script>
Loss function은 다음과 같이 정의된다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/08.png" width="60%" /></center>

<p>추론할 시점에는 어떤 이미지나 문장이든 결합공간 안에 임베딩된다.</p>

<script type="math/tex; mode=display">z_v = [v^{(0)}; ... ; v^{(K)}],</script>

<script type="math/tex; mode=display">z_u = [u^{(0)}; ... ; u^{(K)}],</script>

<hr />

<h2 id="실험experiments">실험(Experiments)</h2>

<h3 id="experimental-setup">Experimental Setup</h3>

<p>r-DAN과 m-DAN 모두에 대해 모든 hyper-parameters들은 전부 고정되었다.</p>

<p>$K$=2, LSTM을 포함한 모든 네트워크의 hidden layer의 dimension=512,<br />
lr=0.1, momentum=0.9, weight decay=0.0005, dropout rate=0.5, gradient clipping=0.1,<br />
epochs=60, 30epoch 이후 lr=0.01,<br />
minibatch=128 $\times$ 128 quadruplets(긍정 이미지, 긍정 문장, 부정 이미지, 부정 문장),<br />
가능한 답변의 수 C=2000, margin $m$=100이다.</p>

<h3 id="evaluation-on-visual-question-answering">Evaluation on Visual Question Answering</h3>

<h4 id="dataset-and-evaluation-metric">Dataset and Evaluation Metric</h4>

<p>VQA 데이터셋을 사용하였고, train(이미지 8만 장), val(이미지 4만 장), test-dev(이미지 2만 장), test-std(이미지 2만 장)이다. 측정방법은</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/09.png" width="60%" /></center>

<p>$\hat{a}$는 예측된 답이다.</p>

<h4 id="results-and-analysis">Results and Analysis</h4>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/10.png" width="100%" /></center>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/11.png" width="100%" /></center>

<p>결과를 보면 대부분의 상황에서 SOTA 결과를 얻었으며, 이미지와 문장에서 집중해야 할 부분을 잘 찾았음을 확인할 수 있다.</p>

<h3 id="evaluation-on-image-text-matching">Evaluation on Image-Text Matching</h3>

<p>분석결과는 비슷하므로 생략한다.</p>

<center><img src="/public/img/2019-04-17-Dual-Attention-Networks/12.png" width="100%" /></center>

<hr />

<h2 id="결론conclusion">결론(Conclusion)</h2>

<p>우리는 시각 및 언어적 attention mechanism을 연결하기 위한 Dual Attention Networks (DANs)를 제안하였다. 추론과 매칭을 위한 모델을 하나씩 만들었고, 각각의 모델은 이미지와 문장으로부터 공통 의미를 찾아낸다.<br />
이 모델들은 VQA와 image-text 매칭 task에서 SOTA 결과를 얻어냄으로써 DANs의 효과를 입증하였다. 제안된 이 framework는 image captioning, visual grounding, video question answering 등등 많은 시각 및 언어 task들로 확장될 수 있다.</p>

<hr />

<h2 id="참고문헌references">참고문헌(References)</h2>

<p>논문 참조! 부록은 없다. <del>읽기 편하다</del></p>

<hr />

  </article>
  <script type="text/javascript" async
          src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  <script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

</div>

<amp-auto-ads type="adsense"
              data-ad-client="ca-pub-9951774327887666">
</amp-auto-ads>

<script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-9951774327887666"
     data-ad-slot="6606866336"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<script data-ad-client="ca-pub-9951774327887666" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
    <li>
      <h3>
        <a href="/github-usage-09-overall/">
          GitHub 사용법 - 09. Overall
          <small>27 May 2020</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/VAE/">
          Variational AutoEncoder 설명
          <small>25 May 2020</small>
        </a>
      </h3>
    </li>
    
    <li>
      <h3>
        <a href="/AFM/">
          추천 시스템의 기본 - 06. AFM 논문 리뷰 및 Tensorflow 구현
          <small>01 May 2020</small>
        </a>
      </h3>
    </li>
    
  </ul>
</div>

<div id="disqus_thread"></div>
<script>

  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

  var disqus_config = function () {
    this.page.url = 'http://localhost:4000/Dual-Attention-Networks/';
    this.page.identifier = 'http://localhost:4000/Dual-Attention-Networks/';
    //this.page.url = 'https://greeksharifa.github.com/';  // Replace PAGE_URL with your page's canonical URL variable
    //this.page.identifier = 'greeksharifa'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };

  (function () { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://greeksharifa.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by
  Disqus.</a></noscript>

  </div>
</div>

<label for="sidebar-checkbox" class="sidebar-toggle"></label>

<script>
  (function (document) {
    let toggle = document.querySelector('.sidebar-toggle');
    let sidebar = document.querySelector('#sidebar');
    let checkbox = document.querySelector('#sidebar-checkbox');

    document.addEventListener('click', function (e) {
      let target = e.target;

      if (target === toggle) {
        checkbox.checked = !checkbox.checked;
        e.preventDefault();
      } else if (checkbox.checked && !sidebar.contains(target)) {
        /* click outside the sidebar when sidebar is open */
        checkbox.checked = false;
      }
    }, false);
  })(document);
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    };
    i[r].l = 1 * new Date();
    a = s.createElement(o);
    m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-00000000-1', 'auto');
  ga('send', 'pageview');
</script>


<!-- Naver Analytics -->	
<script type="text/javascript" src="//wcs.naver.net/wcslog.js"></script>
<script type="text/javascript">
  if(!wcs_add) var wcs_add = {};
    wcs_add["wa"] = "18cbce78e94161";
  wcs_do();
</script>

</body>

<script id="dsq-count-scr" src="//greeksharifa-github-io.disqus.com/count.js" async></script>

</html>
